--- Page 15 ---
Preface
This book (known as CS:APP) is for computer scientists, computer engineers, and
others who want to be able to write better programs by learning what is going on
“under the hood” of a computer system.
Our aim is to explain the enduring concepts underlying all computer systems,
and to show you the concrete ways that these ideas affect the correctness, perfor-
mance, and utility of your application programs. Many systems books are written
from a builder’s perspective, describing how to implement the hardware or the sys-
tems software, including the operating system, compiler, and network interface.
This book is written from a programmer’s perspective, describing how application
programmers can use their knowledge of a system to write better programs. Of
course, learning what a system is supposed to do provides a good ﬁrst step in learn-
ing how to build one, so this book also serves as a valuable introduction to those
who go on to implement systems hardware and software. Most systems books also
tend to focus on just one aspect of the system, for example, the hardware archi-
tecture, the operating system, the compiler, or the network. This book spans all
of these aspects, with the unifying theme of a programmer’s perspective.
If you study and learn the concepts in this book, you will be on your way to
becoming the rare power programmer who knows how things work and how to
ﬁx them when they break. You will be able to write programs that make better
use of the capabilities provided by the operating system and systems software,
that operate correctly across a wide range of operating conditions and run-time
parameters, that run faster, and that avoid the ﬂaws that make programs vulner-
able to cyberattack. You will be prepared to delve deeper into advanced topics
such as compilers, computer architecture, operating systems, embedded systems,
networking, and cybersecurity.
Assumptions about the Reader’s Background
This book focuses on systems that execute x86-64 machine code. x86-64 is the latest
in an evolutionary path followed by Intel and its competitors that started with the
8086 microprocessor in 1978. Due to the naming conventions used by Intel for
its microprocessor line, this class of microprocessors is referred to colloquially as
“x86.” As semiconductor technology has evolved to allow more transistors to be
integrated onto a single chip, these processors have progressed greatly in their
computing power and their memory capacity. As part of this progression, they
have gone from operating on 16-bit words, to 32-bit words with the introduction
of IA32 processors, and most recently to 64-bit words with x86-64.
We consider how these machines execute C programs on Linux. Linux is one
of a number of operating systems having their heritage in the Unix operating
system developed originally by Bell Laboratories. Other members of this class


--- Page 16 ---
New to C?
Advice on the C programming language
To help readers whose background in C programming is weak (or nonexistent), we have also included
these special notes to highlight features that are especially important in C. We assume you are familiar
with C++ or Java.
of operating systems include Solaris, FreeBSD, and MacOS X. In recent years,
these operating systems have maintained a high level of compatibility through the
efforts of the Posix and Standard Unix Speciﬁcation standardization efforts. Thus,
the material in this book applies almost directly to these “Unix-like” operating
systems.
The text contains numerous programming examples that have been compiled
and run on Linux systems. We assume that you have access to such a machine, and
are able to log in and do simple things such as listing ﬁles and changing directo-
ries. If your computer runs Microsoft Windows, we recommend that you install
one of the many different virtual machine environments (such as VirtualBox or
VMWare) that allow programs written for one operating system (the guest OS)
to run under another (the host OS).
We also assume that you have some familiarity with C or C++. If your only
prior experience is with Java, the transition will require more effort on your part,
but we will help you. Java and C share similar syntax and control statements.
However, there are aspects of C (particularly pointers, explicit dynamic memory
allocation, and formatted I/O) that do not exist in Java. Fortunately, C is a small
language, and it is clearly and beautifully described in the classic “K&R” text
by Brian Kernighan and Dennis Ritchie [61]. Regardless of your programming
background, consider K&R an essential part of your personal systems library. If
your prior experience is with an interpreted language, such as Python, Ruby, or
Perl, you will deﬁnitely want to devote some time to learning C before you attempt
to use this book.
Several of the early chapters in the book explore the interactions between C
programs and their machine-language counterparts. The machine-language exam-
ples were all generated by the GNU gcc compiler running on x86-64 processors.
We do not assume any prior experience with hardware, machine language, or
assembly-language programming.
How to Read the Book
Learning how computer systems work from a programmer’s perspective is great
fun, mainly because you can do it actively. Whenever you learn something new,
you can try it out right away and see the result ﬁrsthand. In fact, we believe that
the only way to learn systems is to do systems, either working concrete problems
or writing and running programs on real systems.
This theme pervades the entire book. When a new concept is introduced, it
is followed in the text by one or more practice problems that you should work


--- Page 17 ---
code/intro/hello.c
1
#include <stdio.h>
2
3
int main()
4
{
5
printf("hello, world\n");
6
return 0;
7
}
code/intro/hello.c
Figure 1
A typical code example.
immediately to test your understanding. Solutions to the practice problems are
at the end of each chapter. As you read, try to solve each problem on your own
and then check the solution to make sure you are on the right track. Each chapter
is followed by a set of homework problems of varying difﬁculty. Your instructor
has the solutions to the homework problems in an instructor’s manual. For each
homework problem, we show a rating of the amount of effort we feel it will require:
◆Should require just a few minutes. Little or no programming required.
◆◆Might require up to 20 minutes. Often involves writing and testing some
code. (Many of these are derived from problems we have given on exams.)
◆◆◆Requires a signiﬁcant effort, perhaps 1–2 hours. Generally involves writ-
ing and testing a signiﬁcant amount of code.
◆◆◆◆A lab assignment, requiring up to 10 hours of effort.
Each code example in the text was formatted directly, without any manual
intervention, from a C program compiled with gcc and tested on a Linux system.
Of course, your system may have a different version of gcc, or a different compiler
altogether, so your compiler might generate different machine code; but the
overall behavior should be the same. All of the source code is available from the
CS:APP Web page (“CS:APP” being our shorthand for the book’s title) at csapp
.cs.cmu.edu. In the text, the ﬁlenames of the source programs are documented
in horizontal bars that surround the formatted code. For example, the program in
Figure 1 can be found in the ﬁle hello.c in directory code/intro/. We encourage
you to try running the example programs on your system as you encounter them.
To avoid having a book that is overwhelming, both in bulk and in content, we
have created a number of Web asides containing material that supplements the
main presentation of the book. These asides are referenced within the book with
a notation of the form chap:top, where chap is a short encoding of the chapter sub-
ject, and top is a short code for the topic that is covered. For example, Web Aside
data:bool contains supplementary material on Boolean algebra for the presenta-
tion on data representations in Chapter 2, while Web Aside arch:vlog contains


--- Page 18 ---
material describing processor designs using the Verilog hardware description lan-
guage, supplementing the presentation of processor design in Chapter 4. All of
these Web asides are available from the CS:APP Web page.
Book Overview
The CS:APP book consists of 12 chapters designed to capture the core ideas in
computer systems. Here is an overview.
Chapter 1: A Tour of Computer Systems. This chapter introduces the major ideas
and themes in computer systems by tracing the life cycle of a simple “hello,
world” program.
Chapter 2: Representing and Manipulating Information. We cover computer arith-
metic, emphasizing the properties of unsigned and two’s-complement num-
ber representations that affect programmers. We consider how numbers
are represented and therefore what range of values can be encoded for
a given word size. We consider the effect of casting between signed and
unsigned numbers. We cover the mathematical properties of arithmetic op-
erations. Novice programmers are often surprised to learn that the (two’s-
complement) sum or product of two positive numbers can be negative. On
the other hand, two’s-complement arithmetic satisﬁes many of the algebraic
properties of integer arithmetic, and hence a compiler can safely transform
multiplication by a constant into a sequence of shifts and adds. We use the
bit-level operations of C to demonstrate the principles and applications of
Boolean algebra. We cover the IEEE ﬂoating-point format in terms of how
it represents values and the mathematical properties of ﬂoating-point oper-
ations.
Having a solid understanding of computer arithmetic is critical to writ-
ing reliable programs. For example, programmers and compilers cannot re-
place the expression (x<y) with (x-y < 0), due to the possibility of overﬂow.
They cannot even replace it with the expression (-y < -x), due to the asym-
metric range of negative and positive numbers in the two’s-complement
representation. Arithmetic overﬂow is a common source of programming
errors and security vulnerabilities, yet few other books cover the properties
of computer arithmetic from a programmer’s perspective.
Chapter 3: Machine-Level Representation of Programs. We teach you how to read
the x86-64 machine code generated by a C compiler. We cover the ba-
sic instruction patterns generated for different control constructs, such as
conditionals, loops, and switch statements. We cover the implementation
of procedures, including stack allocation, register usage conventions, and
parameter passing. We cover the way different data structures such as struc-
tures, unions, and arrays are allocated and accessed. We cover the instruc-
tions that implement both integer and ﬂoating-point arithmetic. We also
use the machine-level view of programs as a way to understand common
code security vulnerabilities, such as buffer overﬂow, and steps that the pro-


--- Page 19 ---
Aside
What is an aside?
You will encounter asides of this form throughout the text. Asides are parenthetical remarks that give
you some additional insight into the current topic. Asides serve a number of purposes. Some are little
history lessons. For example, where did C, Linux, and the Internet come from? Other asides are meant
to clarify ideas that students often ﬁnd confusing. For example, what is the difference between a cache
line, set, and block? Other asides give real-world examples, such as how a ﬂoating-point error crashed
a French rocket or the geometric and operational parameters of a commercial disk drive. Finally, some
asides are just fun stuff. For example, what is a “hoinky”?
grammer, the compiler, and the operating system can take to reduce these
threats. Learning the concepts in this chapter helps you become a better
programmer, because you will understand how programs are represented
on a machine. One certain beneﬁt is that you will develop a thorough and
concrete understanding of pointers.
Chapter 4: Processor Architecture. This chapter covers basic combinational and
sequential logic elements, and then shows how these elements can be com-
bined in a datapath that executes a simpliﬁed subset of the x86-64 instruction
set called “Y86-64.” We begin with the design of a single-cycle datapath.
This design is conceptually very simple, but it would not be very fast. We
then introduce pipelining, where the different steps required to process an
instruction are implemented as separate stages. At any given time, each
stage can work on a different instruction. Our ﬁve-stage processor pipeline is
much more realistic. The control logic for the processor designs is described
using a simple hardware description language called HCL. Hardware de-
signs written in HCL can be compiled and linked into simulators provided
with the textbook, and they can be used to generate Verilog descriptions
suitable for synthesis into working hardware.
Chapter 5: Optimizing Program Performance. This chapter introduces a number
of techniques for improving code performance, with the idea being that pro-
grammers learn to write their C code in such a way that a compiler can then
generate efﬁcient machine code. We start with transformations that reduce
the work to be done by a program and hence should be standard practice
when writing any program for any machine. We then progress to trans-
formations that enhance the degree of instruction-level parallelism in the
generated machine code, thereby improving their performance on modern
“superscalar” processors. To motivate these transformations, we introduce
a simple operational model of how modern out-of-order processors work,
and show how to measure the potential performance of a program in terms
of the critical paths through a graphical representation of a program. You
will be surprised how much you can speed up a program by simple transfor-
mations of the C code.


--- Page 20 ---
Chapter 6: The Memory Hierarchy. The memory system is one of the most visible
parts of a computer system to application programmers. To this point, you
have relied on a conceptual model of the memory system as a linear array
with uniform access times. In practice, a memory system is a hierarchy of
storage devices with different capacities, costs, and access times. We cover
the different types of RAM and ROM memories and the geometry and
organization of magnetic-disk and solid state drives. We describe how these
storage devices are arranged in a hierarchy. We show how this hierarchy is
made possible by locality of reference. We make these ideas concrete by
introducing a unique view of a memory system as a “memory mountain”
with ridges of temporal locality and slopes of spatial locality. Finally, we
show you how to improve the performance of application programs by
improving their temporal and spatial locality.
Chapter 7: Linking. This chapter covers both static and dynamic linking, including
the ideas of relocatable and executable object ﬁles, symbol resolution, re-
location, static libraries, shared object libraries, position-independent code,
and library interpositioning. Linking is not covered in most systems texts,
but we cover it for two reasons. First, some of the most confusing errors that
programmers can encounter are related to glitches during linking, especially
for large software packages. Second, the object ﬁles produced by linkers are
tied to concepts such as loading, virtual memory, and memory mapping.
Chapter 8: Exceptional Control Flow. In this part of the presentation, we step
beyond the single-program model by introducing the general concept of
exceptional control ﬂow (i.e., changes in control ﬂow that are outside the
normal branches and procedure calls). We cover examples of exceptional
control ﬂow that exist at all levels of the system, from low-level hardware ex-
ceptions and interrupts, to context switches between concurrent processes,
to abrupt changes in control ﬂow caused by the receipt of Linux signals, to
the nonlocal jumps in C that break the stack discipline.
This is the part of the book where we introduce the fundamental idea
of a process, an abstraction of an executing program. You will learn how
processes work and how they can be created and manipulated from appli-
cation programs. We show how application programmers can make use of
multiple processes via Linux system calls. When you ﬁnish this chapter, you
will be able to write a simple Linux shell with job control. It is also your ﬁrst
introduction to the nondeterministic behavior that arises with concurrent
program execution.
Chapter 9: Virtual Memory. Our presentation of the virtual memory system seeks
to give some understanding of how it works and its characteristics. We want
you to know how it is that the different simultaneous processes can each use
an identical range of addresses, sharing some pages but having individual
copies of others. We also cover issues involved in managing and manip-
ulating virtual memory. In particular, we cover the operation of storage
allocators such as the standard-library malloc and free operations. Cov-


--- Page 21 ---
ering this material serves several purposes. It reinforces the concept that
the virtual memory space is just an array of bytes that the program can
subdivide into different storage units. It helps you understand the effects
of programs containing memory referencing errors such as storage leaks
and invalid pointer references. Finally, many application programmers write
their own storage allocators optimized toward the needs and characteris-
tics of the application. This chapter, more than any other, demonstrates the
beneﬁt of covering both the hardware and the software aspects of computer
systems in a uniﬁed way. Traditional computer architecture and operating
systems texts present only part of the virtual memory story.
Chapter 10: System-Level I/O. We cover the basic concepts of Unix I/O such as
ﬁles and descriptors. We describe how ﬁles are shared, how I/O redirection
works, and how to access ﬁle metadata. We also develop a robust buffered
I/O package that deals correctly with a curious behavior known as short
counts, where the library function reads only part of the input data. We
cover the C standard I/O library and its relationship to Linux I/O, focusing
on limitations of standard I/O that make it unsuitable for network program-
ming. In general, the topics covered in this chapter are building blocks for
the next two chapters on network and concurrent programming.
Chapter 11: Network Programming. Networks are interesting I/O devices to pro-
gram, tying together many of the ideas that we study earlier in the text, such
as processes, signals, byte ordering, memory mapping, and dynamic storage
allocation. Network programs also provide a compelling context for con-
currency, which is the topic of the next chapter. This chapter is a thin slice
through network programming that gets you to the point where you can
write a simple Web server. We cover the client-server model that underlies
all network applications. We present a programmer’s view of the Internet
and show how to write Internet clients and servers using the sockets inter-
face. Finally, we introduce HTTP and develop a simple iterative Web server.
Chapter 12: Concurrent Programming. This chapter introduces concurrent pro-
gramming using Internet server design as the running motivational example.
We compare and contrast the three basic mechanisms for writing concur-
rent programs—processes, I/O multiplexing, and threads—and show how
to use them to build concurrent Internet servers. We cover basic principles
of synchronization using P and V semaphore operations, thread safety and
reentrancy, race conditions, and deadlocks. Writing concurrent code is es-
sential for most server applications. We also describe the use of thread-level
programming to express parallelism in an application program, enabling
faster execution on multi-core processors. Getting all of the cores working
on a single computational problem requires a careful coordination of the
concurrent threads, both for correctness and to achieve high performance.


--- Page 22 ---
New to This Edition
The ﬁrst edition of this book was published with a copyright of 2003, while the
second had a copyright of 2011. Considering the rapid evolution of computer
technology, the book content has held up surprisingly well. Intel x86 machines
running C programs under Linux (and related operating systems) has proved to
be a combination that continues to encompass many systems today. However,
changes in hardware technology, compilers, program library interfaces, and the
experience of many instructors teaching the material have prompted a substantial
revision.
The biggest overall change from the second edition is that we have switched
our presentation from one based on a mix of IA32 and x86-64 to one based
exclusively on x86-64. This shift in focus affected the contents of many of the
chapters. Here is a summary of the signiﬁcant changes.
Chapter 1: A Tour of Computer Systems We have moved the discussion of Am-
dahl’s Law from Chapter 5 into this chapter.
Chapter 2: Representing and Manipulating Information. A consistent bit of feed-
back from readers and reviewers is that some of the material in this chapter
can be a bit overwhelming. So we have tried to make the material more ac-
cessible by clarifying the points at which we delve into a more mathematical
style of presentation. This enables readers to ﬁrst skim over mathematical
details to get a high-level overview and then return for a more thorough
reading.
Chapter 3: Machine-Level Representation of Programs. We have converted from
the earlier presentation based on a mix of IA32 and x86-64 to one based
entirely on x86-64. We have also updated for the style of code generated by
more recent versions of gcc. The result is a substantial rewriting, including
changing the order in which some of the concepts are presented. We also
have included, for the ﬁrst time, a presentation of the machine-level support
for programs operating on ﬂoating-point data. We have created a Web aside
describing IA32 machine code for legacy reasons.
Chapter 4: Processor Architecture. We have revised the earlier processor design,
based on a 32-bit architecture, to one that supports 64-bit words and oper-
ations.
Chapter 5: Optimizing Program Performance. We have updated the material to
reﬂect the performance capabilities of recent generations of x86-64 proces-
sors. With the introduction of more functional units and more sophisticated
control logic, the model of program performance we developed based on a
data-ﬂow representation of programs has become a more reliable predictor
of performance than it was before.
Chapter 6: The Memory Hierarchy. We have updated the material to reﬂect more
recent technology.


--- Page 23 ---
Chapter 7: Linking. We have rewritten this chapter for x86-64, expanded the
discussion of using the GOT and PLT to create position-independent code,
and added a new section on a powerful linking technique known as library
interpositioning.
Chapter 8: Exceptional Control Flow. We have added a more rigorous treatment
of signal handlers, including async-signal-safe functions, speciﬁc guidelines
for writing signal handlers, and using sigsuspend to wait for handlers.
Chapter 9: Virtual Memory. This chapter has changed only slightly.
Chapter 10: System-Level I/O. We have added a new section on ﬁles and the ﬁle
hierarchy, but otherwise, this chapter has changed only slightly.
Chapter 11: Network Programming. We have introduced techniques for protocol-
independent and thread-safe network programming using the modern
getaddrinfo and getnameinfo functions, which replace the obsolete and
non-reentrant gethostbyname and gethostbyaddr functions.
Chapter 12: Concurrent Programming. We have increased our coverage of using
thread-level parallelism to make programs run faster on multi-core ma-
chines.
In addition, we have added and revised a number of practice and homework
problems throughout the text.
Origins of the Book
This book stems from an introductory course that we developed at Carnegie Mel-
lon University in the fall of 1998, called 15-213: Introduction to Computer Systems
(ICS) [14]. The ICS course has been taught every semester since then. Over 400
students take the course each semester. The students range from sophomores to
graduate students in a wide variety of majors. It is a required core course for all
undergraduates in the CS and ECE departments at Carnegie Mellon, and it has
become a prerequisite for most upper-level systems courses in CS and ECE.
The idea with ICS was to introduce students to computers in a different way.
Few of our students would have the opportunity to build a computer system. On
the other hand, most students, including all computer scientists and computer
engineers, would be required to use and program computers on a daily basis. So we
decided to teach about systems from the point of view of the programmer, using
the following ﬁlter: we would cover a topic only if it affected the performance,
correctness, or utility of user-level C programs.
For example, topics such as hardware adder and bus designs were out. Top-
ics such as machine language were in; but instead of focusing on how to write
assembly language by hand, we would look at how a C compiler translates C con-
structs into machine code, including pointers, loops, procedure calls, and switch
statements. Further, we would take a broader and more holistic view of the system
as both hardware and systems software, covering such topics as linking, loading,


--- Page 24 ---
processes, signals, performance optimization, virtual memory, I/O, and network
and concurrent programming.
This approach allowed us to teach the ICS course in a way that is practical,
concrete, hands-on, and exciting for the students. The response from our students
and faculty colleagues was immediate and overwhelmingly positive, and we real-
ized that others outside of CMU might beneﬁt from using our approach. Hence
this book, which we developed from the ICS lecture notes, and which we have
now revised to reﬂect changes in technology and in how computer systems are
implemented.
Via the multiple editions and multiple translations of this book, ICS and many
variants have become part of the computer science and computer engineering
curricula at hundreds of colleges and universities worldwide.
For Instructors: Courses Based on the Book
Instructors can use the CS:APP book to teach a number of different types of
systems courses. Five categories of these courses are illustrated in Figure 2. The
particular course depends on curriculum requirements, personal taste, and
the backgrounds and abilities of the students. From left to right in the ﬁgure,
the courses are characterized by an increasing emphasis on the programmer’s
perspective of a system. Here is a brief description.
ORG. A computer organization course with traditional topics covered in an un-
traditional style. Traditional topics such as logic design, processor architec-
ture, assembly language, and memory systems are covered. However, there
is more emphasis on the impact for the programmer. For example, data rep-
resentations are related back to the data types and operations of C programs,
and the presentation on assembly code is based on machine code generated
by a C compiler rather than handwritten assembly code.
ORG+. The ORG course with additional emphasis on the impact of hardware
on the performance of application programs. Compared to ORG, students
learn more about code optimization and about improving the memory per-
formance of their C programs.
ICS. The baseline ICS course, designed to produce enlightened programmers who
understand the impact of the hardware, operating system, and compilation
system on the performance and correctness of their application programs.
A signiﬁcant difference from ORG+ is that low-level processor architecture
is not covered. Instead, programmers work with a higher-level model of a
modern out-of-order processor. The ICS course ﬁts nicely into a 10-week
quarter, and can also be stretched to a 15-week semester if covered at a
more leisurely pace.
ICS+. The baseline ICS course with additional coverage of systems programming
topics such as system-level I/O, network programming, and concurrent pro-
gramming. This is the semester-long Carnegie Mellon course, which covers
every chapter in CS:APP except low-level processor architecture.


--- Page 25 ---
Course
Chapter
Topic
ORG
ORG+
ICS
ICS+
SP
1
Tour of systems
•
•
•
•
•
2
Data representation
•
•
•
•
⊙(d)
3
Machine language
•
•
•
•
•
4
Processor architecture
•
•
5
Code optimization
•
•
•
6
Memory hierarchy
⊙(a)
•
•
•
⊙(a)
7
Linking
⊙(c)
⊙(c)
•
8
Exceptional control ﬂow
•
•
•
9
Virtual memory
⊙(b)
•
•
•
•
10
System-level I/O
•
•
11
Network programming
•
•
12
Concurrent programming
•
•
Figure 2
Five systems courses based on the CS:APP book. ICS+ is the 15-213 course
from Carnegie Mellon. Notes: The ⊙symbol denotes partial coverage of a chapter, as
follows: (a) hardware only; (b) no dynamic storage allocation; (c) no dynamic linking;
(d) no ﬂoating point.
SP. A systems programming course. This course is similar to ICS+, but it drops
ﬂoating point and performance optimization, and it places more empha-
sis on systems programming, including process control, dynamic linking,
system-level I/O, network programming, and concurrent programming. In-
structors might want to supplement from other sources for advanced topics
such as daemons, terminal control, and Unix IPC.
The main message of Figure 2 is that the CS:APP book gives a lot of options
to students and instructors. If you want your students to be exposed to lower-
level processor architecture, then that option is available via the ORG and ORG+
courses. On the other hand, if you want to switch from your current computer
organization course to an ICS or ICS+ course, but are wary of making such a
drastic change all at once, then you can move toward ICS incrementally. You
can start with ORG, which teaches the traditional topics in a nontraditional way.
Once you are comfortable with that material, then you can move to ORG+,
and eventually to ICS. If students have no experience in C (e.g., they have only
programmed in Java), you could spend several weeks on C and then cover the
material of ORG or ICS.
Finally, we note that the ORG+ and SP courses would make a nice two-term
sequence (either quarters or semesters). Or you might consider offering ICS+ as
one term of ICS and one term of SP.


--- Page 26 ---
For Instructors: Classroom-Tested Laboratory Exercises
The ICS+ course at Carnegie Mellon receives very high evaluations from students.
Median scores of 5.0/5.0 and means of 4.6/5.0 are typical for the student course
evaluations. Students cite the fun, exciting, and relevant laboratory exercises as
the primary reason. The labs are available from the CS:APP Web page. Here are
examples of the labs that are provided with the book.
Data Lab. This lab requires students to implement simple logical and arithmetic
functions, but using a highly restricted subset of C. For example, they must
compute the absolute value of a number using only bit-level operations. This
lab helps students understand the bit-level representations of C data types
and the bit-level behavior of the operations on data.
Binary Bomb Lab. A binary bomb is a program provided to students as an object-
code ﬁle. When run, it prompts the user to type in six different strings. If
any of these are incorrect, the bomb “explodes,” printing an error message
and logging the event on a grading server. Students must “defuse” their
own unique bombs by disassembling and reverse engineering the programs
to determine what the six strings should be. The lab teaches students to
understand assembly language and also forces them to learn how to use a
debugger.
Buffer Overﬂow Lab. Students are required to modify the run-time behavior of
a binary executable by exploiting a buffer overﬂow vulnerability. This lab
teaches the students about the stack discipline and about the danger of
writing code that is vulnerable to buffer overﬂow attacks.
Architecture Lab. Several of the homework problems of Chapter 4 can be com-
bined into a lab assignment, where students modify the HCL description of
a processor to add new instructions, change the branch prediction policy, or
add or remove bypassing paths and register ports. The resulting processors
can be simulated and run through automated tests that will detect most of
the possible bugs. This lab lets students experience the exciting parts of pro-
cessor design without requiring a complete background in logic design and
hardware description languages.
Performance Lab. Students must optimize the performance of an application ker-
nel function such as convolution or matrix transposition. This lab provides
a very clear demonstration of the properties of cache memories and gives
students experience with low-level program optimization.
Cache Lab. In this alternative to the performance lab, students write a general-
purpose cache simulator, and then optimize a small matrix transpose kernel
to minimize the number of misses on a simulated cache. We use the Valgrind
tool to generate real address traces for the matrix transpose kernel.
Shell Lab. Students implement their own Unix shell program with job control,
including the Ctrl+C and Ctrl+Z keystrokes and the fg, bg, and jobs com-


--- Page 27 ---
mands. This is the student’s ﬁrst introduction to concurrency, and it gives
them a clear idea of Unix process control, signals, and signal handling.
Malloc Lab. Students implement their own versions of malloc, free, and (op-
tionally) realloc. This lab gives students a clear understanding of data
layout and organization, and requires them to evaluate different trade-offs
between space and time efﬁciency.
Proxy Lab. Students implement a concurrent Web proxy that sits between their
browsers and the rest of the World Wide Web. This lab exposes the students
to such topics as Web clients and servers, and ties together many of the con-
cepts from the course, such as byte ordering, ﬁle I/O, process control, signals,
signal handling, memory mapping, sockets, and concurrency. Students like
being able to see their programs in action with real Web browsers and Web
servers.
The CS:APP instructor’s manual has a detailed discussion of the labs, as well
as directions for downloading the support software.
Acknowledgments for the Third Edition
It is a pleasure to acknowledge and thank those who have helped us produce this
third edition of the CS:APP text.
We would like to thank our Carnegie Mellon colleagues who have taught the
ICS course over the years and who have provided so much insightful feedback
and encouragement: Guy Blelloch, Roger Dannenberg, David Eckhardt, Franz
Franchetti, Greg Ganger, Seth Goldstein, Khaled Harras, Greg Kesden, Bruce
Maggs, Todd Mowry, Andreas Nowatzyk, Frank Pfenning, Markus Pueschel, and
Anthony Rowe. David Winters was very helpful in installing and conﬁguring the
reference Linux box.
Jason Fritts (St. Louis University) and Cindy Norris (Appalachian State)
provided us with detailed and thoughtful reviews of the second edition. Yili Gong
(Wuhan University) wrote the Chinese translation, maintained the errata page for
the Chinese version, and contributed many bug reports. Godmar Back (Virginia
Tech) helped us improve the text signiﬁcantly by introducing us to the notions of
async-signal safety and protocol-independent network programming.
Many thanks to our eagle-eyed readers who reported bugs in the second edi-
tion: Rami Ammari, Paul Anagnostopoulos, Lucas B¨arenf¨anger, Godmar Back,
Ji Bin, Sharbel Bousemaan, Richard Callahan, Seth Chaiken, Cheng Chen, Libo
Chen, Tao Du, Pascal Garcia, Yili Gong, Ronald Greenberg, Dorukhan G¨ul¨oz,
Dong Han, Dominik Helm, Ronald Jones, Mustafa Kazdagli, Gordon Kindlmann,
Sankar Krishnan, Kanak Kshetri, Junlin Lu, Qiangqiang Luo, Sebastian Luy,
Lei Ma, Ashwin Nanjappa, Gregoire Paradis, Jonas Pfenninger, Karl Pichotta,
David Ramsey, Kaustabh Roy, David Selvaraj, Sankar Shanmugam, Dominique
Smulkowska, Dag Sørbø, Michael Spear, Yu Tanaka, Steven Tricanowicz, Scott
Wright, Waiki Wright, Han Xu, Zhengshan Yan, Firo Yang, Shuang Yang, John
Ye, Taketo Yoshida, Yan Zhu, and Michael Zink.


--- Page 28 ---
Thanks also to our readers who have contributed to the labs, including God-
mar Back (Virginia Tech), Taymon Beal (Worcester Polytechnic Institute), Aran
Clauson (Western Washington University), Cary Gray (Wheaton College), Paul
Haiduk (West Texas A&M University), Len Hamey (Macquarie University), Ed-
die Kohler (Harvard), Hugh Lauer (Worcester Polytechnic Institute), Robert
Marmorstein (Longwood University), and James Riely (DePaul University).
Once again, Paul Anagnostopoulos of Windfall Software did a masterful job
of typesetting the book and leading the production process. Many thanks to Paul
and his stellar team: Richard Camp (copyediting), Jennifer McClain (proofread-
ing), Laurel Muller (art production), and Ted Laux (indexing). Paul even spotted
a bug in our description of the origins of the acronym BSS that had persisted
undetected since the ﬁrst edition!
Finally, we would like to thank our friends at Prentice Hall. Marcia Horton
and our editor, Matt Goldstein, have been unﬂagging in their support and encour-
agement, and we are deeply grateful to them.
Acknowledgments from the Second Edition
We are deeply grateful to the many people who have helped us produce this second
edition of the CS:APP text.
First and foremost, we would like to recognize our colleagues who have taught
the ICS course at Carnegie Mellon for their insightful feedback and encourage-
ment: Guy Blelloch, Roger Dannenberg, David Eckhardt, Greg Ganger, Seth
Goldstein, Greg Kesden, Bruce Maggs, Todd Mowry, Andreas Nowatzyk, Frank
Pfenning, and Markus Pueschel.
Thanks also to our sharp-eyed readers who contributed reports to the errata
page for the ﬁrst edition: Daniel Amelang, Rui Baptista, Quarup Barreirinhas,
Michael Bombyk, J¨org Brauer, Jordan Brough, Yixin Cao, James Caroll, Rui Car-
valho, Hyoung-Kee Choi, Al Davis, Grant Davis, Christian Dufour, Mao Fan,
Tim Freeman, Inge Frick, Max Gebhardt, Jeff Goldblat, Thomas Gross, Anita
Gupta, John Hampton, Hiep Hong, Greg Israelsen, Ronald Jones, Haudy Kazemi,
Brian Kell, Constantine Kousoulis, Sacha Krakowiak, Arun Krishnaswamy, Mar-
tin Kulas, Michael Li, Zeyang Li, Ricky Liu, Mario Lo Conte, Dirk Maas, Devon
Macey, Carl Marcinik, Will Marrero, Simone Martins, Tao Men, Mark Morris-
sey, Venkata Naidu, Bhas Nalabothula, Thomas Niemann, Eric Peskin, David Po,
Anne Rogers, John Ross, Michael Scott, Seiki, Ray Shih, Darren Shultz, Erik
Silkensen, Suryanto, Emil Tarazi, Nawanan Theera-Ampornpunt, Joe Trdinich,
Michael Trigoboff, James Troup, Martin Vopatek, Alan West, Betsy Wolff, Tim
Wong, James Woodruff, Scott Wright, Jackie Xiao, Guanpeng Xu, Qing Xu, Caren
Yang, Yin Yongsheng, Wang Yuanxuan, Steven Zhang, and Day Zhong. Special
thanks to Inge Frick, who identiﬁed a subtle deep copy bug in our lock-and-copy
example, and to Ricky Liu for his amazing proofreading skills.
Our Intel Labs colleagues Andrew Chien and Limor Fix were exceptionally
supportive throughout the writing of the text. Steve Schlosser graciously provided
some disk drive characterizations. Casey Helfrich and Michael Ryan installed


--- Page 29 ---
and maintained our new Core i7 box. Michael Kozuch, Babu Pillai, and Jason
Campbell provided valuable insight on memory system performance, multi-core
systems, and the power wall. Phil Gibbons and Shimin Chen shared their consid-
erable expertise on solid state disk designs.
We have been able to call on the talents of many, including Wen-Mei Hwu,
Markus Pueschel, and Jiri Simsa, to provide both detailed comments and high-
level advice. James Hoe helped us create a Verilog version of the Y86 processor
and did all of the work needed to synthesize working hardware.
Many thanks to our colleagues who provided reviews of the draft manu-
script: James Archibald (Brigham Young University), Richard Carver (George
Mason University), Mirela Damian (Villanova University), Peter Dinda (North-
western University), John Fiore (Temple University), Jason Fritts (St. Louis Uni-
versity), John Greiner (Rice University), Brian Harvey (University of California,
Berkeley), Don Heller (Penn State University), Wei Chung Hsu (University of
Minnesota), Michelle Hugue (University of Maryland), Jeremy Johnson (Drexel
University), Geoff Kuenning (Harvey Mudd College), Ricky Liu, Sam Mad-
den (MIT), Fred Martin (University of Massachusetts, Lowell), Abraham Matta
(Boston University), Markus Pueschel (Carnegie Mellon University), Norman
Ramsey (Tufts University), Glenn Reinmann (UCLA), Michela Taufer (Univer-
sity of Delaware), and Craig Zilles (UIUC).
Paul Anagnostopoulos of Windfall Software did an outstanding job of type-
setting the book and leading the production team. Many thanks to Paul and his
superb team: Rick Camp (copyeditor), Joe Snowden (compositor), MaryEllen N.
Oliver (proofreader), Laurel Muller (artist), and Ted Laux (indexer).
Finally, we would like to thank our friends at Prentice Hall. Marcia Horton has
always been there for us. Our editor, Matt Goldstein, provided stellar leadership
from beginning to end. We are profoundly grateful for their help, encouragement,
and insights.
Acknowledgments from the First Edition
We are deeply indebted to many friends and colleagues for their thoughtful crit-
icisms and encouragement. A special thanks to our 15-213 students, whose infec-
tious energy and enthusiasm spurred us on. Nick Carter and Vinny Furia gener-
ously provided their malloc package.
Guy Blelloch, Greg Kesden, Bruce Maggs, and Todd Mowry taught the course
over multiple semesters, gave us encouragement, and helped improve the course
material. Herb Derby provided early spiritual guidance and encouragement. Al-
lan Fisher, Garth Gibson, Thomas Gross, Satya, Peter Steenkiste, and Hui Zhang
encouraged us to develop the course from the start. A suggestion from Garth
early on got the whole ball rolling, and this was picked up and reﬁned with the
help of a group led by Allan Fisher. Mark Stehlik and Peter Lee have been very
supportive about building this material into the undergraduate curriculum. Greg
Kesden provided helpful feedback on the impact of ICS on the OS course. Greg
Ganger and Jiri Schindler graciously provided some disk drive characterizations


--- Page 30 ---
and answered our questions on modern disks. Tom Stricker showed us the mem-
ory mountain. James Hoe provided useful ideas and feedback on how to present
processor architecture.
A special group of students—Khalil Amiri, Angela Demke Brown, Chris
Colohan, Jason Crawford, Peter Dinda, Julio Lopez, Bruce Lowekamp, Jeff
Pierce, Sanjay Rao, Balaji Sarpeshkar, Blake Scholl, Sanjit Seshia, Greg Stef-
fan, Tiankai Tu, Kip Walker, and Yinglian Xie—were instrumental in helping
us develop the content of the course. In particular, Chris Colohan established a
fun (and funny) tone that persists to this day, and invented the legendary “binary
bomb” that has proven to be a great tool for teaching machine code and debugging
concepts.
Chris Bauer, Alan Cox, Peter Dinda, Sandhya Dwarkadas, John Greiner,
Don Heller, Bruce Jacob, Barry Johnson, Bruce Lowekamp, Greg Morrisett,
Brian Noble, Bobbie Othmer, Bill Pugh, Michael Scott, Mark Smotherman, Greg
Steffan, and Bob Wier took time that they did not have to read and advise us
on early drafts of the book. A very special thanks to Al Davis (University of
Utah), Peter Dinda (Northwestern University), John Greiner (Rice University),
Wei Hsu (University of Minnesota), Bruce Lowekamp (College of William &
Mary), Bobbie Othmer (University of Minnesota), Michael Scott (University of
Rochester), and Bob Wier (Rocky Mountain College) for class testing the beta
version. A special thanks to their students as well!
We would also like to thank our colleagues at Prentice Hall. Marcia Horton,
Eric Frank, and Harold Stone have been unﬂagging in their support and vision.
Harold also helped us present an accurate historical perspective on RISC and
CISC processor architectures. Jerry Ralya provided sharp insights and taught us
a lot about good writing.
Finally, we would like to acknowledge the great technical writers Brian
Kernighan and the late W. Richard Stevens, for showing us that technical books
can be beautiful.
Thank you all.
Randy Bryant
Dave O’Hallaron
Pittsburgh, Pennsylvania
Pearson would like to thank and acknowledge Chetan Venkatesh, MS Ramaiah
Institute of Technology, Desny Antony, Don Bosco College, and Chitra Dhawale,
SP College, for reviewing the Global Edition.

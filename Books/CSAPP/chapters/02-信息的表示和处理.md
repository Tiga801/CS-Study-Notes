# 第02章 信息的表示和处理

> 原书: Chapter 02 - Representing and Manipulating Information

## 本章概述

本章研究计算机如何表示和处理数字信息。主要内容包括：

- **2.1 信息存储** - 字节、字、数据大小、寻址和字节顺序
- **2.2 整数表示** - 无符号编码和补码编码
- **2.3 整数运算** - 加法、乘法、除法的位级实现
- **2.4 浮点数** - IEEE 浮点标准及其运算
- **2.5 小结** - 本章要点总结

---

## 章节目录

| 节号 | 标题 | 页码 |
|------|------|------|
| 2.1 | 信息存储 | 70 |
| 2.2 | 整数表示 | 95 |
| 2.3 | 整数运算 | 120 |
| 2.4 | 浮点数 | 144 |
| 2.5 | 小结 | 162 |
| - | 参考文献说明 | 163 |
| - | 家庭作业 | 164 |
| - | 练习题答案 | 179 |

现代计算机存储和处理以二值信号表示的信息。这些不起眼的二进制数字，即**位（bit）**，构成了数字革命的基础。我们熟悉的十进制（即以 10 为基数的）表示法已经使用了 1000 多年，它起源于印度，在 12 世纪由阿拉伯数学家改进，并在 13 世纪由意大利数学家莱昂纳多·皮萨诺（约 1170 年至约 1250 年，更广为人知的名字是斐波那契）传入西方。对于有十根手指的人类来说，使用十进制记数法是很自然的，但在构建存储和处理信息的机器时，二进制值更为有效。二值信号可以很容易地表示、存储和传输——例如，可以表示为穿孔卡片上孔的有无、导线上的高低电压，或者顺时针或逆时针定向的磁畴。用于存储和执行二值信号计算的电子电路非常简单可靠，使得制造商能够在单个硅芯片上集成数百万甚至数十亿个这样的电路。
单独来看，一个位并不是很有用。但是，当我们把位组合在一起，并对不同的可能位模式赋予某种解释意义时，就可以表示任何有限集合的元素。例如，使用二进制数系统，我们可以用一组位来编码非负数。通过使用标准字符编码，我们可以对文档中的字母和符号进行编码。本章将涵盖这两种编码方式，以及用于表示负数和近似实数的编码。
我们将讨论三种最重要的数字表示方法。**无符号编码**基于传统的二进制表示法，用于表示大于或等于 0 的数。**补码编码**是表示有符号整数（即可以是正数或负数的数）最常用的方式。**浮点编码**是用于表示实数的以 2 为基数的科学记数法。计算机使用这些不同的表示方法来实现算术运算，如加法和乘法，类似于对整数和实数进行的相应运算。
计算机表示使用有限数量的位来编码数字，因此当结果太大而无法表示时，某些运算可能会**溢出**。这可能导致一些令人惊讶的结果。例如，在当今大多数计算机上（那些使用 32 位表示 `int` 数据类型的计算机），计算表达式

```
200 * 300 * 400 * 500
```

会得到 −884,901,888。这与整数算术的性质相悖——计算一组正数的乘积却得到了负数结果。

另一方面，计算机整数算术满足真正整数算术的许多熟悉性质。例如，乘法是可结合和可交换的，因此计算以下任何 C 表达式都会得到 −884,901,888：

```c
(500 * 400) * (300 * 200)
((500 * 400) * 300) * 200
((200 * 500) * 300) * 400
400 * (200 * (300 * 500))
```

计算机可能不会生成预期的结果，但至少它是一致的！

浮点算术具有完全不同的数学性质。一组正数的乘积总是正的，尽管溢出会产生特殊值 +∞。由于表示的有限精度，浮点算术不满足结合律。例如，C 表达式 `(3.14+1e20)-1e20` 在大多数机器上会计算为 0.0，而 `3.14+(1e20-1e20)` 会计算为 3.14。整数算术与浮点算术之间不同的数学性质源于它们处理表示有限性方式的差异——整数表示可以编码相对较小范围的值，但精确地这样做；而浮点表示可以编码很大范围的值，但只能近似地表示。
通过研究实际的数字表示，我们可以理解可表示值的范围以及不同算术运算的性质。这种理解对于编写在完整数值范围内正确工作且可在不同机器、操作系统和编译器组合之间移植的程序至关重要。正如我们将要描述的，由于计算机算术的一些微妙之处，已经产生了许多计算机安全漏洞。在早期，程序错误只会在碰巧被触发时给人们带来不便，而现在有大量的攻击者试图利用他们能找到的任何漏洞来获取对他人系统的未授权访问。这给程序员提出了更高的要求，他们需要理解自己的程序如何工作，以及如何可能被迫以不期望的方式运行。
计算机使用几种不同的二进制表示来编码数值。当你在第 3 章进入机器级编程时，你将需要熟悉这些表示。我们在本章中描述这些编码，并向你展示如何推理数字表示。

我们推导出几种通过直接操纵数字的位级表示来执行算术运算的方法。理解这些技术对于理解编译器在优化算术表达式求值性能时生成的机器级代码非常重要。
我们对这些材料的处理基于一组核心数学原理。我们从编码的基本定义开始，然后推导出诸如可表示数的范围、它们的位级表示以及算术运算的性质等属性。我们认为从这种抽象的角度来审视这些材料是很重要的，因为程序员需要清楚地理解计算机算术与更熟悉的整数和实数算术之间的关系。

C++ 编程语言建立在 C 之上，使用完全相同的数值表示和运算。本章中关于 C 的所有内容也适用于 C++。另一方面，Java 语言定义创建了一套新的数值表示和运算标准。C 标准被设计为允许广泛的实现方式，而 Java 标准对数据的格式和编码非常具体。我们在本章的几个地方重点介绍了 Java 支持的表示和运算。

> **旁注：如何阅读本章**
>
> 在本章中，我们研究数字和其他形式的数据在计算机上如何表示的基本性质，以及计算机对这些数据执行的运算的性质。这需要我们深入数学语言，编写公式和方程，并展示重要性质的推导。
>
> 为了帮助你浏览这些内容，我们将讲解结构化为首先用数学符号陈述一个性质作为原理。然后我们用例子和非正式讨论来说明这个原理。我们建议你在原理陈述和例子讨论之间来回切换，直到你对所说的内容以及该性质的重要性有了扎实的直觉。对于更复杂的性质，我们还提供了推导，其结构很像数学证明。你最终应该尝试理解这些推导，但在第一次阅读时可以跳过它们。
>
> 我们还鼓励你在阅读过程中完成练习题。练习题让你参与主动学习，帮助你将想法付诸行动。有了这些作为背景，你会发现回头理解推导要容易得多。同时请放心，理解这些材料所需的数学技能对于掌握了高中代数的人来说是完全可以达到的。
## 2.1 信息存储

大多数计算机使用 8 位的块，即**字节（byte）**，作为最小的可寻址内存单位，而不是访问内存中的单个位。机器级程序将内存视为一个非常大的字节数组，称为**虚拟内存（virtual memory）**。内存的每个字节都由一个唯一的数字标识，称为其**地址（address）**，所有可能地址的集合称为**虚拟地址空间（virtual address space）**。

顾名思义，这个虚拟地址空间只是呈现给机器级程序的一个概念性映像。实际的实现（在第 9 章中介绍）使用动态随机访问存储器（DRAM）、闪存、磁盘存储、专用硬件和操作系统软件的组合，为程序提供看起来是一个整体字节数组的东西。

在后续章节中，我们将介绍编译器和运行时系统如何将这个内存空间划分为更易管理的单元，以存储不同的程序对象，即程序数据、指令和控制信息。各种机制用于为程序的不同部分分配和管理存储。这种管理都在虚拟地址空间内执行。例如，C 中指针的值——无论它指向整数、结构还是其他程序对象——都是某个存储块第一个字节的虚拟地址。C 编译器还将类型信息与每个指针关联，以便它可以根据该值的类型生成不同的机器级代码来访问指针指定位置存储的值。虽然 C 编译器维护这种类型信息，但它生成的实际机器级程序没有关于数据类型的信息。它只是将每个程序对象视为一个字节块，将程序本身视为一个字节序列。

> **旁注：C 编程语言的演变**
>
> 如第 40 页的旁注所述，C 编程语言最初由贝尔实验室的丹尼斯·里奇（Dennis Ritchie）开发，用于 Unix 操作系统（也在贝尔实验室开发）。当时，大多数系统程序（如操作系统）必须主要用汇编代码编写，才能访问不同数据类型的底层表示。例如，在那个时代的其他高级语言中编写内存分配器（如 malloc 库函数提供的那种）是不可行的。
>
> 贝尔实验室原版 C 记录在布莱恩·柯尼汉和丹尼斯·里奇的书的第一版中 [60]。随着时间的推移，C 通过几个标准化组织的努力不断演变。原始贝尔实验室 C 的第一次重大修订导致了 1989 年的 ANSI C 标准，由一个在美国国家标准学会主持下工作的小组制定。ANSI C 与贝尔实验室 C 有重大区别，特别是在函数声明方式上。ANSI C 在柯尼汉和里奇的书的第二版中描述 [61]，该书仍被认为是关于 C 的最佳参考书之一。
>
> 国际标准化组织接管了 C 语言标准化的责任，在 1990 年采用了与 ANSI C 基本相同的版本，因此被称为"ISO C90"。
>
> 同一组织在 1999 年赞助了语言的更新，产生了"ISO C99"。除其他外，这个版本引入了一些新的数据类型，并提供了对需要英语中没有的字符的文本字符串的支持。更近的标准在 2011 年获得批准，因此被命名为"ISO C11"，再次增加了更多的数据类型和功能。这些最近的大多数添加都是向后兼容的，这意味着根据早期标准（至少追溯到 ISO C90）编写的程序在根据较新标准编译时将具有相同的行为。
>
> GNU 编译器集合（gcc）可以根据不同的命令行选项，按照 C 语言几个不同版本的约定来编译程序，如图 2.1 所示。例如，要按照 ISO C11 编译程序 prog.c，我们可以给出命令行：
>
> ```bash
> linux> gcc -std=c11 prog.c
> ```
>
> 选项 `-ansi` 和 `-std=c89` 具有相同的效果——代码按照 ANSI 或 ISO C90 标准编译。（C90 有时被称为"C89"，因为其标准化工作始于 1989 年。）选项 `-std=c99` 使编译器遵循 ISO C99 约定。
>
> 在本书撰写时，当没有指定选项时，程序将按照基于 ISO C90 的 C 版本编译，但包括 C99 的一些功能、C11 的一些功能、C++ 的一些功能以及 gcc 特有的其他功能。GNU 项目正在开发一个结合 ISO C11 和其他功能的版本，可以用命令行选项 `-std=gnu11` 指定。（目前，这个实现是不完整的。）这将成为默认版本。

**图 2.1 向 gcc 指定不同版本的 C**

| C 版本 | gcc 命令行选项 |
|--------|----------------|
| GNU 89 | 无, `-std=gnu89` |
| ANSI, ISO C90 | `-ansi`, `-std=c89` |
| ISO C99 | `-std=c99` |
| ISO C11 | `-std=c11` |

> **C 语言新手？指针在 C 中的角色**
>
> 指针是 C 的核心特性。它们提供了引用数据结构（包括数组）元素的机制。就像变量一样，指针有两个方面：它的值和它的类型。值指示某个对象的位置，而类型指示在该位置存储的是什么类型的对象（例如，整数或浮点数）。
>
> 真正理解指针需要在机器级别检查它们的表示和实现。这将是第 3 章的一个主要重点，最终在 3.10.1 节中进行深入介绍。
### 2.1.1 十六进制表示法

一个字节由 8 位组成。在二进制表示法中，它的值范围从 00000000₂ 到 11111111₂。当作为十进制整数查看时，它的值范围从 0₁₀ 到 255₁₀。这两种表示法对于描述位模式都不太方便。二进制表示法太冗长，而十进制表示法在与位模式之间转换时很繁琐。相反，我们将位模式写成以 16 为基数的**十六进制**数。十六进制（或简称"hex"）使用数字 '0' 到 '9' 以及字符 'A' 到 'F' 来表示 16 个可能的值。图 2.2 显示了与 16 个十六进制数字相关联的十进制和二进制值。用十六进制表示时，单个字节的值可以从 00₁₆ 到 FF₁₆。
在 C 中，以 `0x` 或 `0X` 开头的数值常量被解释为十六进制。字符 'A' 到 'F' 可以用大写或小写书写。例如，我们可以将数字 FA1D37B₁₆ 写成 `0xFA1D37B`、`0xfa1d37b`，甚至混合使用大小写（例如 `0xFa1D37b`）。本书将使用 C 表示法来表示十六进制值。

在处理机器级程序时，一个常见的任务是在位模式的十进制、二进制和十六进制表示之间手动转换。二进制和十六进制之间的转换很简单，因为可以一次转换一个十六进制数字。可以通过参考图 2.2 所示的图表来转换数字。在脑中进行转换的一个简单技巧是记住十六进制数字 A、C 和 F 的十进制等价值。

**图 2.2 十六进制表示法。每个十六进制数字编码 16 个值中的一个。**

| 十六进制数字 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
|-------------|---|---|---|---|---|---|---|---|
| 十进制值 | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
| 二进制值 | 0000 | 0001 | 0010 | 0011 | 0100 | 0101 | 0110 | 0111 |

| 十六进制数字 | 8 | 9 | A | B | C | D | E | F |
|-------------|---|---|---|---|---|---|---|---|
| 十进制值 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 |
| 二进制值 | 1000 | 1001 | 1010 | 1011 | 1100 | 1101 | 1110 | 1111 |

十六进制值 B、D 和 E 可以通过计算它们相对于前三个值的值来转换为十进制。

例如，假设给你数字 `0x173A4C`。你可以通过展开每个十六进制数字将其转换为二进制格式，如下所示：

| 十六进制 | 1 | 7 | 3 | A | 4 | C |
|----------|------|------|------|------|------|------|
| 二进制 | 0001 | 0111 | 0011 | 1010 | 0100 | 1100 |

这给出二进制表示 000101110011101001001100。

相反，给定二进制数 1111001010110110110011，你首先将它分成每组 4 位来转换为十六进制。但是请注意，如果总位数不是 4 的倍数，你应该让最左边的组少于 4 位，实际上是用前导零填充该数。然后将每组位转换为相应的十六进制数字：

| 二进制 | 11 | 1100 | 1010 | 1101 | 1011 | 0011 |
|--------|------|------|------|------|------|------|
| 十六进制 | 3 | C | A | D | B | 3 |
**练习题 2.1**（答案见第 179 页）

执行以下数字转换：
- A. `0x25B9D2` 转换为二进制
- B. 二进制 `1010111001001001` 转换为十六进制
- C. `0xA8B3D` 转换为二进制
- D. 二进制 `1100100010110110010110` 转换为十六进制
当值 x 是 2 的幂时，即对于某个非负整数 n，x = 2ⁿ，我们可以通过记住 x 的二进制表示就是 1 后面跟着 n 个零来轻松地用十六进制形式表示 x。十六进制数字 0 表示 4 个二进制零。因此，对于写成 i + 4j 形式的 n，其中 0 ≤ i ≤ 3，我们可以用前导十六进制数字 1（i = 0）、2（i = 1）、4（i = 2）或 8（i = 3）后跟 j 个十六进制 0 来表示 x。例如，对于 x = 2,048 = 2¹¹，我们有 n = 11 = 3 + 4 × 2，给出十六进制表示 `0x800`。

**练习题 2.2**（答案见第 179 页）

填写下表中的空白条目，给出不同 2 的幂的十进制和十六进制表示：

| n | 2ⁿ（十进制）| 2ⁿ（十六进制）|
|---|------------|--------------|
| 5 | 32 | 0x20 |
| 23 | | |
| | 32,768 | |
| | | 0x2000 |
| 12 | | |
| | 64 | |
| | | 0x100 |
十进制和十六进制表示之间的转换需要使用乘法或除法来处理一般情况。要将十进制数 x 转换为十六进制，我们可以反复将 x 除以 16，得到商 q 和余数 r，使得 x = q × 16 + r。然后我们使用表示 r 的十六进制数字作为最低有效位，并通过对 q 重复该过程来生成剩余的数字。例如，考虑将十进制 314,156 转换：

```
314,156 = 19,634 × 16 + 12    (C)
 19,634 =  1,227 × 16 +  2    (2)
  1,227 =     76 × 16 + 11    (B)
     76 =      4 × 16 + 12    (C)
      4 =      0 × 16 +  4    (4)
```

由此我们可以读出十六进制表示为 `0x4CB2C`。

相反，要将十六进制数转换为十进制，我们可以将每个十六进制数字乘以适当的 16 的幂。例如，给定数字 `0x7AF`，我们计算其十进制等价值为 7 × 16² + 10 × 16 + 15 = 7 × 256 + 10 × 16 + 15 = 1,792 + 160 + 15 = 1,967。
**练习题 2.3**（答案见第 180 页）

单个字节可以用 2 个十六进制数字表示。填写下表中缺失的条目，给出不同字节模式的十进制、二进制和十六进制值：

| 十进制 | 二进制 | 十六进制 |
|--------|--------|----------|
| 0 | 0000 0000 | 0x00 |
| 158 | | |
| 76 | | |
| 145 | | |
| | 1010 1110 | |
| | 0011 1100 | |
| | 1111 0001 | |
| | | 0x75 |
| | | 0xBD |
| | | 0xF5 |

> **旁注：十进制和十六进制之间的转换**
>
> 对于较大值在十进制和十六进制之间的转换，最好让计算机或计算器来完成这项工作。有许多工具可以做到这一点。一个简单的方法是使用任何标准搜索引擎，查询如"Convert 0xabcd to decimal"或"123 in hex"。
**练习题 2.4**（答案见第 180 页）

不要将数字转换为十进制或二进制，尝试解决以下算术问题，用十六进制给出答案。提示：只需修改你用于执行十进制加法和减法的方法，改为使用基数 16。

- A. `0x605c + 0x5` =
- B. `0x605c - 0x20` =
- C. `0x605c + 32` =
- D. `0x60fa - 0x605c` =
### 2.1.2 数据大小

每台计算机都有一个**字长（word size）**，指示指针数据的标称大小。由于虚拟地址由这样的字编码，字长决定的最重要的系统参数是虚拟地址空间的最大大小。也就是说，对于具有 w 位字长的机器，虚拟地址的范围可以从 0 到 2^w - 1，使程序最多可以访问 2^w 字节。

近年来，机器从 32 位字长广泛转向 64 位字长。这首先发生在为大规模科学和数据库应用设计的高端机器上，然后是台式机和笔记本电脑，最近是智能手机中的处理器。32 位字长将虚拟地址空间限制为 4 吉字节（写作 4 GB），即略超过 4 × 10⁹ 字节。扩展到 64 位字长会导致 16 艾字节的虚拟地址空间，约为 1.84 × 10¹⁹ 字节。

大多数 64 位机器也可以运行为 32 位机器编译的程序，这是一种向后兼容性。因此，例如，当使用以下指令编译程序 prog.c 时：

```bash
linux> gcc -m32 prog.c
```

该程序将在 32 位或 64 位机器上正确运行。另一方面，使用以下指令编译的程序：

```bash
linux> gcc -m64 prog.c
```

将只能在 64 位机器上运行。因此，我们将程序称为"32 位程序"或"64 位程序"，因为区别在于程序是如何编译的，而不是它运行在什么类型的机器上。

计算机和编译器支持多种数据格式，使用不同的方式来编码数据，如整数和浮点数，以及不同的长度。例如，许多机器都有操纵单个字节的指令，以及用 2、4 和 8 字节数量表示的整数的指令。它们还支持用 4 和 8 字节数量表示的浮点数。

C 语言对整数和浮点数据都支持多种数据格式。图 2.3 显示了不同 C 数据类型通常分配的字节数。（我们在 2.2 节讨论 C 标准保证的与典型值之间的关系。）某些数据类型的确切字节数取决于程序是如何编译的。我们显示了典型 32 位和 64 位程序的大小。整数数据可以是有符号的，能够表示负数、零和正数，也可以是无符号的，只允许非负值。数据类型 `char` 表示单个字节。虽然名称 char 源于它用于存储文本字符串中的单个字符这一事实，但它也可以用于存储整数值。数据类型 `short`、`int` 和 `long` 旨在提供一系列

**图 2.3 基本 C 数据类型的典型大小（以字节为单位）。分配的字节数随程序编译方式而变化。此图表显示了 32 位和 64 位程序的典型值。**

| C 声明 | 有符号 | 无符号 | 32 位 | 64 位 |
|--------|--------|--------|-------|-------|
| `[signed] char` | `unsigned char` | | 1 | 1 |
| `short` | `unsigned short` | | 2 | 2 |
| `int` | `unsigned` | | 4 | 4 |
| `long` | `unsigned long` | | 4 | 8 |
| `int32_t` | `uint32_t` | | 4 | 4 |
| `int64_t` | `uint64_t` | | 8 | 8 |
| `char *` | | | 4 | 8 |
| `float` | | | 4 | 4 |
| `double` | | | 8 | 8 |

> **C 语言新手？声明指针**
>
> 对于任何数据类型 T，声明 `T *p;` 表示 p 是一个指针变量，指向类型为 T 的对象。例如，`char *p;` 是指向类型为 char 的对象的指针的声明。

大小范围。即使为 64 位系统编译，数据类型 `int` 通常也只有 4 字节。数据类型 `long` 在 32 位程序中通常有 4 字节，在 64 位程序中有 8 字节。

为了避免依赖"典型"大小和不同编译器设置的不确定性，ISO C99 引入了一类数据类型，其数据大小是固定的，与编译器和机器设置无关。其中包括数据类型 `int32_t` 和 `int64_t`，分别恰好有 4 和 8 字节。使用固定大小的整数类型是程序员严格控制数据表示的最佳方式。
大多数数据类型编码有符号值，除非以关键字 `unsigned` 为前缀或使用固定大小数据类型的特定无符号声明。例外是数据类型 `char`。虽然大多数编译器和机器将其视为有符号数据，但 C 标准并不保证这一点。相反，如方括号所示，程序员应该使用声明 `signed char` 来保证 1 字节的有符号值。然而，在许多情况下，程序的行为对数据类型 `char` 是有符号还是无符号不敏感。

C 语言允许以多种方式排列关键字，以及包含或省略可选关键字。例如，以下所有声明具有相同的含义：

```c
unsigned long
unsigned long int
long unsigned
long unsigned int
```

我们将一致使用图 2.3 中的形式。

图 2.3 还显示指针（例如，声明为类型 `char *` 的变量）使用程序的完整字长。大多数机器还支持两种不同的浮点格式：**单精度**，在 C 中声明为 `float`，以及**双精度**，在 C 中声明为 `double`。这些格式分别使用 4 和 8 字节。

程序员应努力使其程序可在不同的机器和编译器之间移植。可移植性的一个方面是使程序对不同数据类型的确切大小不敏感。C 标准设定了不同数据类型数值范围的下限，稍后将介绍，但没有上限（固定大小类型除外）。从大约 1980 年到大约 2010 年，32 位机器和 32 位程序是主要组合，许多程序都是假设图 2.3 中列出的 32 位程序的分配编写的。随着向 64 位机器的过渡，在将这些程序迁移到新机器时，许多隐藏的字长依赖性已经作为错误出现。例如，许多程序员历史上假设声明为类型 `int` 的对象可以用于存储指针。这对于大多数 32 位程序工作正常，但对于 64 位程序会导致问题。
### 2.1.3 寻址和字节顺序

对于跨越多个字节的程序对象，我们必须建立两个约定：对象的地址是什么，以及我们如何在内存中排列字节。在几乎所有机器中，多字节对象都作为连续的字节序列存储，对象的地址由所用字节的最小地址给出。例如，假设类型为 `int` 的变量 x 的地址为 `0x100`；也就是说，地址表达式 `&x` 的值是 `0x100`。那么（假设数据类型 `int` 有 32 位表示）x 的 4 个字节将存储在内存位置 `0x100`、`0x101`、`0x102` 和 `0x103`。

对于排列表示对象的字节，有两种常见的约定。考虑一个 w 位整数，其位表示为 [x_(w-1), x_(w-2), ..., x_1, x_0]，其中 x_(w-1) 是最高有效位，x_0 是最低有效位。假设 w 是 8 的倍数，这些位可以分组为字节，最高有效字节包含位
[x_(w-1), x_(w-2), ..., x_(w-8)]，最低有效字节包含位 [x_7, x_6, ..., x_0]，其他字节包含中间的位。有些机器选择在内存中按从最低有效字节到最高有效字节的顺序存储对象，而其他机器则按从最高到最低的顺序存储。前一种约定——最低有效字节在前——称为**小端法（little endian）**。后一种约定——最高有效字节在前——称为**大端法（big endian）**。

假设类型为 `int` 且地址为 `0x100` 的变量 x 的十六进制值为 `0x01234567`。地址范围 `0x100` 到 `0x103` 内字节的排列取决于机器类型：

**大端法（Big endian）**：
| 地址 | 0x100 | 0x101 | 0x102 | 0x103 |
|------|-------|-------|-------|-------|
| 值   | 01    | 23    | 45    | 67    |

**小端法（Little endian）**：
| 地址 | 0x100 | 0x101 | 0x102 | 0x103 |
|------|-------|-------|-------|-------|
| 值   | 67    | 45    | 23    | 01    |

注意在字 `0x01234567` 中，高位字节的十六进制值为 `0x01`，而低位字节的值为 `0x67`。
大多数 Intel 兼容机器专门在小端模式下运行。另一方面，大多数来自 IBM 和 Oracle（源于他们在 2010 年收购 Sun Microsystems）的机器在大端模式下运行。注意我们说的是"大多数"。这些约定并不严格按公司边界划分。例如，IBM 和 Oracle 都生产使用 Intel 兼容处理器的机器，因此是小端的。许多最近的微处理器芯片是**双端的（bi-endian）**，意味着它们可以配置为以小端或大端机器方式运行。然而，在实践中，一旦选择了特定的操作系统，字节顺序就固定了。例如，用于许多手机的 ARM 微处理器的硬件可以在小端或大端模式下运行，但这些芯片最常见的两个操作系统——Android（来自 Google）和 iOS（来自 Apple）——只在小端模式下运行。

> **旁注："endian"的起源**
>
> 以下是乔纳森·斯威夫特在 1726 年描述大端派和小端派之间争议历史的方式：
>
> "……利立浦特和不来夫斯古……正如我要告诉你的，在过去三十六个月里一直在进行一场最顽固的战争。它是在以下情况下开始的。众所周知，我们吃鸡蛋之前打破鸡蛋的原始方法是从大端打开；但当今陛下的祖父还是个孩子时，去吃一个鸡蛋，按照古老的做法打破它，碰巧划伤了一根手指。于是他父亲皇帝颁布了一道敕令，命令所有臣民在严厉惩罚下从小端打破他们的鸡蛋。人民对这条法律非常不满，我们的历史告诉我们，因此发生了六次叛乱；其中一位皇帝丢了性命，另一位丢了王冠……"
>
> （乔纳森·斯威夫特，《格列佛游记》，Benjamin Motte，1726 年。）
>
> 在他那个时代，斯威夫特是在讽刺英格兰（利立浦特）和法国（不来夫斯古）之间持续的冲突。网络协议的早期先驱丹尼·科恩首先将这些术语应用于指代字节顺序 [24]，该术语已被广泛采用。
人们对哪种字节顺序是正确的会出奇地情绪化。事实上，"小端"和"大端"这两个术语来自乔纳森·斯威夫特的《格列佛游记》，其中两个交战派系无法就如何打开一个半熟鸡蛋达成一致——从小端还是从大端。就像鸡蛋问题一样，没有技术理由选择一种字节顺序约定而不是另一种，因此争论退化为关于社会政治问题的争吵。只要选择并一致遵守其中一种约定，选择就是任意的。

对于大多数应用程序员来说，他们机器使用的字节顺序是完全不可见的；为任一类机器编译的程序都会给出相同的结果。然而，有时字节顺序会成为一个问题。

**第一种情况**是当二进制数据在不同机器之间通过网络通信时。一个常见的问题是小端机器产生的数据被发送到大端机器，或者反过来，导致字内的字节对于接收程序来说是逆序的。为了避免这类问题，为网络应用编写的代码必须遵循字节顺序的既定约定，确保发送机器将其内部表示转换为网络标准，而接收机器将网络标准转换为其内部表示。我们将在第 11 章看到这些转换的例子。
**第二种情况**是当查看表示整数数据的字节序列时，字节顺序变得重要。这在检查机器级程序时经常发生。例如，以下行出现在一个给出 Intel x86-64 处理器机器级代码文本表示的文件中：

```
4004d3:    01 05 43 0b 20 00    add    %eax,0x200b43(%rip)
```

这一行由**反汇编器**生成，反汇编器是一种确定可执行程序文件表示的指令序列的工具。我们将在第 3 章学习更多关于反汇编器以及如何解释这样的行。现在，我们只需注意这一行表示十六进制字节序列 `01 05 43 0b 20 00` 是一条指令的字节级表示，该指令将一个字的数据加到存储在某地址的值上，该地址是通过将 `0x200b43` 加到程序计数器（下一条要执行的指令的地址）的当前值来计算的。如果我们取序列的最后 4 个字节 `43 0b 20 00` 并以相反的顺序写出，我们得到 `00 20 0b 43`。去掉前导 0，我们得到值 `0x200b43`，即写在右边的数值。字节以相反顺序出现是阅读为小端机器（如这台机器）生成的机器级程序表示时的常见现象。写字节序列的自然方式是将最低编号的字节放在左边，最高的放在右边，但这与数字的正常书写方式相反，后者是最高有效位在左边，最低有效位在右边。
**第三种情况**是当编写绕过正常类型系统的程序时，字节顺序变得可见。在 C 语言中，这可以通过使用强制类型转换或联合体来完成，允许按照与创建时不同的数据类型来引用对象。这种编码技巧在大多数应用程序编程中是强烈不建议的，但它们对于系统级编程可能相当有用甚至是必要的。

图 2.4 显示了使用强制类型转换来访问和打印不同程序对象字节表示的 C 代码。我们使用 `typedef` 将数据类型 `byte_pointer` 定义为指向 `unsigned char` 类型对象的指针。这样的字节指针引用一个字节序列，其中每个字节被视为非负整数。第一个例程 `show_bytes` 被给定一个字节序列的地址（由字节指针表示）和一个字节计数。字节计数被指定为具有数据类型 `size_t`，这是表达数据结构大小的首选数据类型。它以十六进制打印各个字节。C 格式化指令 `%.2x` 指示整数应以十六进制打印，至少 2 位数字。

**图 2.4 打印程序对象字节表示的代码。此代码使用强制类型转换来绕过类型系统。类似的机器相关代码用于许多应用程序中。**

```c
#include <stdio.h>

typedef unsigned char *byte_pointer;

void show_bytes(byte_pointer start, size_t len) {
    int i;
    for (i = 0; i < len; i++)
        printf(" %.2x", start[i]);
    printf("\n");
}

```c
void show_int(int x) {
```

13
show_bytes((byte_pointer) &x, sizeof(int));
14
}
15
16

```c
void show_float(float x) {
```

17
show_bytes((byte_pointer) &x, sizeof(float));
18
}
19
20

```c
void show_pointer(void *x) {
```

21
show_bytes((byte_pointer) &x, sizeof(void *));
22
}
Figure 2.4
过程 `show_int`、`show_float` 和 `show_pointer` 演示了如何使用过程 `show_bytes` 来打印 C 程序对象（分别为 `int`、`float` 和 `void *` 类型）的字节表示。观察到它们只是将指针 `&x`（指向它们的参数 x）传递给 `show_bytes`，将指针强制转换为 `unsigned char *` 类型。这个强制转换向编译器指示程序应该将指针视为指向字节序列而不是原始数据类型的对象。然后这个指针将指向对象占用的最低字节地址。这些过程使用 C 的 `sizeof` 运算符来确定对象使用的字节数。通常，表达式 `sizeof(T)` 返回存储类型 T 的对象所需的字节数。使用 `sizeof` 而不是固定值是编写可在不同机器类型之间移植的代码的一步。

我们在几台不同的机器上运行了图 2.5 所示的代码，得到了图 2.6 所示的结果。使用了以下机器：

- **Linux 32**：运行 Linux 的 Intel IA32 处理器
- **Windows**：运行 Windows 的 Intel IA32 处理器
- **Sun**：运行 Solaris 的 Sun Microsystems SPARC 处理器（这些机器现在由 Oracle 生产）
- **Linux 64**：运行 Linux 的 Intel x86-64 处理器

**图 2.5 字节表示示例。此代码打印示例数据对象的字节表示。**

```c
void test_show_bytes(int val) {
    int ival = val;
    float fval = (float) ival;
    int *pval = &ival;
    show_int(ival);
    show_float(fval);
    show_pointer(pval);
}
```

**图 2.6 不同数据值的字节表示。int 和 float 的结果相同，除了字节顺序。指针值是机器相关的。**

| 机器 | 值 | 类型 | 字节（十六进制）|
|------|-------|------|-----------------|
| Linux 32 | 12,345 | int | 39 30 00 00 |
| Windows | 12,345 | int | 39 30 00 00 |
| Sun | 12,345 | int | 00 00 30 39 |
| Linux 64 | 12,345 | int | 39 30 00 00 |
| Linux 32 | 12,345.0 | float | 00 e4 40 46 |
| Windows | 12,345.0 | float | 00 e4 40 46 |
| Sun | 12,345.0 | float | 46 40 e4 00 |
| Linux 64 | 12,345.0 | float | 00 e4 40 46 |
| Linux 32 | &ival | int * | e4 f9 ff bf |
| Windows | &ival | int * | b4 cc 22 00 |
| Sun | &ival | int * | ef ff fa 0c |
| Linux 64 | &ival | int * | b8 11 e5 ff ff 7f 00 00 |
我们的参数 12,345 的十六进制表示为 `0x00003039`。对于 `int` 数据，我们在所有机器上得到相同的结果，除了字节顺序。特别是，我们可以看到最低有效字节值 `0x39` 在 Linux 32、Windows 和 Linux 64 上首先打印，表明是小端机器，而在 Sun 上最后打印，表明是大端机器。类似地，`float` 数据的字节相同，除了字节顺序。另一方面，指针值完全不同。不同的机器/操作系统配置使用不同的存储分配约定。需要注意的一个特点是 Linux 32、Windows 和 Sun 机器使用 4 字节地址，而 Linux 64 机器使用 8 字节地址。

> **C 语言新手？使用 typedef 命名数据类型**
>
> C 中的 `typedef` 声明提供了一种给数据类型命名的方式。这对于提高代码可读性很有帮助，因为深度嵌套的类型声明可能难以理解。
>
> `typedef` 的语法与声明变量完全相同，只是它使用类型名而不是变量名。因此，图 2.4 中 `byte_pointer` 的声明与声明 `unsigned char *` 类型的变量的形式相同。
>
> 例如，声明：
> ```c
> typedef int *int_pointer;
> int_pointer ip;
> ```
> 将类型 `int_pointer` 定义为指向 `int` 的指针，并声明了这种类型的变量 `ip`。或者，我们可以直接将此变量声明为 `int *ip;`

> **C 语言新手？使用 printf 格式化打印**
>
> `printf` 函数（连同它的表亲 `fprintf` 和 `sprintf`）提供了一种打印信息并对格式细节有相当控制的方式。第一个参数是格式字符串，其余参数是要打印的值。在格式字符串中，每个以 '%' 开头的字符序列指示如何格式化下一个参数。典型的例子包括 `%d` 打印十进制整数，`%f` 打印浮点数，`%c` 打印由参数给出的字符代码的字符。
>
> 指定固定大小数据类型（如 `int32_t`）的格式化稍微复杂一些，如第 103 页的旁注所述。
观察到虽然浮点数据和整数数据都编码数值 12,345，但它们有非常不同的字节模式：整数为 `0x00003039`，浮点数为 `0x4640E400`。通常，这两种格式使用不同的编码方案。如果我们将这些十六进制模式展开为二进制形式并适当移位，我们会发现一个 13 位匹配的序列，由一系列星号表示，如下所示：
3
0
3
9
00000000000000000011000000111001
*************
4
6
4
0
E
4
0
0
01000110010000001110010000000000
这不是巧合。我们在学习浮点格式时将返回到这个例子。

> **C 语言新手？指针和数组**
>
> 在函数 `show_bytes`（图 2.4）中，我们看到指针和数组之间的密切联系，这将在 3.8 节详细讨论。我们看到这个函数有一个类型为 `byte_pointer`（已定义为指向 `unsigned char` 的指针）的参数 `start`，但我们在第 8 行看到数组引用 `start[i]`。在 C 中，我们可以用数组表示法解引用指针，也可以用指针表示法引用数组元素。在这个例子中，引用 `start[i]` 表示我们想读取 `start` 指向位置之后 i 个位置的字节。

> **C 语言新手？指针的创建和解引用**
>
> 在图 2.4 的第 13、17 和 21 行，我们看到两个赋予 C（因此也是 C++）其独特特征的操作的使用。C 的"取地址"运算符 '&' 创建一个指针。在所有三行中，表达式 `&x` 创建一个指向变量 x 所指示对象所在位置的指针。这个指针的类型取决于 x 的类型，因此这三个指针分别是 `int *`、`float *` 和 `void **` 类型。（数据类型 `void *` 是一种特殊的指针，没有关联的类型信息。）
>
> 强制转换运算符将一种数据类型转换为另一种。因此，强制转换 `(byte_pointer) &x` 表示无论指针 `&x` 之前是什么类型，程序现在将引用指向 `unsigned char` 类型数据的指针。这里显示的强制转换不会改变实际的指针；它们只是指示编译器根据新的数据类型来引用被指向的数据。

> **旁注：生成 ASCII 表**
>
> 你可以通过执行命令 `man ascii` 来显示 ASCII 字符代码表。
**练习题 2.5**（答案见第 180 页）

考虑以下对 `show_bytes` 的三次调用：

```c
int a = 0x12345678;
byte_pointer ap = (byte_pointer) &a;
show_bytes(ap, 1); /* A. */
show_bytes(ap, 2); /* B. */
show_bytes(ap, 3); /* C. */
```

指出在小端机器和大端机器上每次调用将打印的值：

| 调用 | 小端 | 大端 |
|------|------|------|
| A | | |
| B | | |
| C | | |

**练习题 2.6**（答案见第 181 页）

使用 `show_int` 和 `show_float`，我们确定整数 2607352 的十六进制表示为 `0x0027C8F8`，而浮点数 3510593.0 的十六进制表示为 `0x4A1F23E0`。

- A. 写出这两个十六进制值的二进制表示。
- B. 将这两个字符串相对移动以最大化匹配位数。有多少位匹配？
- C. 字符串的哪些部分不匹配？
### 2.1.4 表示字符串

C 中的字符串由以空字符（值为 0）结尾的字符数组编码。每个字符由某种标准编码表示，最常见的是 ASCII 字符代码。因此，如果我们用参数 `"12345"` 和 6（包括终止字符）运行我们的例程 `show_bytes`，我们得到结果 `31 32 33 34 35 00`。注意十进制数字 x 的 ASCII 代码恰好是 `0x3x`，终止字节的十六进制表示是 `0x00`。在使用 ASCII 作为字符代码的任何系统上都会得到相同的结果，与字节顺序和字长约定无关。因此，文本数据比二进制数据更具平台独立性。

**练习题 2.7**（答案见第 181 页）

以下对 `show_bytes` 的调用会打印什么结果？

```c
const char *m = "mnopqr";
show_bytes((byte_pointer) m, strlen(m));
```

注意字母 'a' 到 'z' 的 ASCII 代码是 `0x61` 到 `0x7A`。
### 2.1.5 表示代码

考虑以下 C 函数：

```c
int sum(int x, int y) {
    return x + y;
}
```

当在我们的示例机器上编译时，我们生成具有以下字节表示的机器代码：

| 机器 | 字节表示 |
|------|----------|
| Linux 32 | `55 89 e5 8b 45 0c 03 45 08 c9 c3` |
| Windows | `55 89 e5 8b 45 0c 03 45 08 5d c3` |
| Sun | `81 c3 e0 08 90 02 00 09` |
| Linux 64 | `55 48 89 e5 89 7d fc 89 75 f8 03 45 fc c9 c3` |

> **旁注：文本编码的 Unicode 标准**
>
> ASCII 字符集适合编码英语文档，但它没有太多特殊字符，如法语的 'ç'。它完全不适合编码希腊语、俄语和中文等语言的文档。多年来，已经开发了各种方法来编码不同语言的文本。Unicode 联盟设计了最全面、最广泛接受的文本编码标准。当前的 Unicode 标准（版本 7.0）有超过 100,000 个字符，支持广泛的语言，包括埃及和巴比伦的古老语言。值得称赞的是，Unicode 技术委员会拒绝了一项包含克林贡语（电视系列片《星际迷航》中虚构文明的语言）标准书写的提案。
>
> 基本编码，称为 Unicode 的"通用字符集"，使用字符的 32 位表示。这似乎要求每个文本字符串由每个字符 4 字节组成。然而，可能存在替代编码，其中常见字符只需要 1 或 2 字节，而不太常见的字符需要更多。特别是，UTF-8 表示将每个字符编码为字节序列，使得标准 ASCII 字符使用与它们在 ASCII 中相同的单字节编码，这意味着所有 ASCII 字节序列在 UTF-8 中与在 ASCII 中具有相同的含义。
>
> Java 编程语言在其字符串表示中使用 Unicode。C 也有支持 Unicode 的程序库。

在这里我们发现指令编码是不同的。不同的机器类型使用不同且不兼容的指令和编码。即使运行不同操作系统的相同处理器在其编码约定上也有差异，因此不是二进制兼容的。二进制代码很少能在机器和操作系统的不同组合之间移植。

计算机系统的一个基本概念是，从机器的角度来看，程序只是一个字节序列。机器没有关于原始源程序的信息，除了可能维护一些辅助表来帮助调试。当我们在第 3 章学习机器级编程时，我们将更清楚地看到这一点。
### 2.1.6 布尔代数简介

由于二进制值是计算机编码、存储和操纵信息的核心，围绕值 0 和 1 的研究已经发展出丰富的数学知识体系。这始于乔治·布尔（1815-1864）大约 1850 年的工作，因此被称为**布尔代数**。布尔观察到，通过将逻辑值真和假编码为二进制值 1 和 0，他可以制定一个捕捉逻辑推理基本原理的代数。

最简单的布尔代数定义在二元集合 {0, 1} 上。图 2.7 定义了这个代数中的几个运算。我们表示这些运算的符号被选择为与 C 位级运算使用的符号匹配。

**图 2.7 布尔代数的运算。二进制值 1 和 0 编码逻辑值真和假，而运算 ~、&、| 和 ^ 分别编码逻辑运算非、与、或和异或。**

| ~ | | | & | 0 | 1 | | \| | 0 | 1 | | ^ | 0 | 1 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| 0 | 1 | | 0 | 0 | 0 | | 0 | 0 | 1 | | 0 | 0 | 1 |
| 1 | 0 | | 1 | 0 | 1 | | 1 | 1 | 1 | | 1 | 1 | 0 |
如稍后讨论的那样。布尔运算 `~` 对应于逻辑运算**非**，用符号 ¬ 表示。也就是说，当 P 不为真时我们说 ¬P 为真，反之亦然。相应地，当 p 等于 0 时 `~p` 等于 1，反之亦然。布尔运算 `&` 对应于逻辑运算**与**，用符号 ∧ 表示。当 P 为真且 Q 为真时，我们说 P ∧ Q 成立。相应地，只有当 p = 1 且 q = 1 时，`p & q` 才等于 1。布尔运算 `|` 对应于逻辑运算**或**，用符号 ∨ 表示。当 P 为真或 Q 为真时，我们说 P ∨ Q 成立。相应地，当 p = 1 或 q = 1 时，`p | q` 等于 1。布尔运算 `^` 对应于逻辑运算**异或**，用符号 ⊕ 表示。当 P 为真或 Q 为真，但不同时为真时，我们说 P ⊕ Q 成立。相应地，当 p = 1 且 q = 0，或者 p = 0 且 q = 1 时，`p ^ q` 等于 1。
克劳德·香农（1916-2001），后来创立了信息论领域，首先建立了布尔代数与数字逻辑之间的联系。在他 1937 年的硕士论文中，他展示了布尔代数可以应用于机电继电器网络的设计和分析。尽管计算机技术自那以后已经有了相当大的进步，布尔代数仍然在数字系统的设计和分析中起着核心作用。

我们可以将四种布尔运算扩展到也对**位向量**进行运算，位向量是某个固定长度 w 的零和一的字符串。我们根据运算对参数匹配元素的应用来定义位向量上的运算。设 a 和 b 分别表示位向量 [a_(w-1), a_(w-2), ..., a_0] 和 [b_(w-1), b_(w-2), ..., b_0]。我们将 `a & b` 定义为也是长度为 w 的位向量，其中第 i 个元素等于 a_i & b_i，对于 0 ≤ i < w。运算 `|`、`^` 和 `~` 以类似的方式扩展到位向量。

作为例子，考虑 w = 4 的情况，参数 a = [0110] 和 b = [1100]。那么四个运算 `a & b`、`a | b`、`a ^ b` 和 `~b` 产生：

```
    0110        0110        0110
  & 1100      | 1100      ^ 1100      ~ 1100
  ------      ------      ------      ------
    0100        1110        1010        0011
```
**练习题 2.8**（答案见第 181 页）

填写下表，显示对位向量求值布尔运算的结果。

| 运算 | 结果 |
|------|------|
| a | [01001110] |
| b | [11100001] |
| ~a | |
| ~b | |
| a & b | |
| a \| b | |
| a ^ b | |

> **网络旁注 DATA:BOOL：布尔代数和布尔环的更多内容**
>
> 对长度为 w 的位向量进行运算的布尔运算 `|`、`&` 和 `~` 形成布尔代数，对于任何整数 w > 0。最简单的情况是 w = 1 且只有两个元素，但对于更一般的情况，有 2^w 个长度为 w 的位向量。布尔代数具有与整数算术相同的许多性质。例如，就像乘法对加法分配一样，写作 a · (b + c) = (a · b) + (a · c)，布尔运算 `&` 对 `|` 分配，写作 `a & (b | c) = (a & b) | (a & c)`。然而，此外，布尔运算 `|` 对 `&` 分配，所以我们可以写 `a | (b & c) = (a | b) & (a | c)`，而我们不能说 a + (b · c) = (a + b) · (a + c) 对所有整数成立。
>
> 当我们考虑对长度为 w 的位向量进行运算的 `^`、`&` 和 `~` 时，我们得到一种不同的数学形式，称为**布尔环**。布尔环与整数算术有许多共同的性质。例如，整数算术的一个性质是每个值 x 都有一个加法逆元 -x，使得 x + (-x) = 0。布尔环也有类似的性质，其中 `^` 是"加法"运算，但在这种情况下，每个元素都是它自己的加法逆元。也就是说，对于任何值 a，`a ^ a = 0`，这里我们使用 0 表示全零的位向量。我们可以看到这对单个位成立，因为 0 ^ 0 = 1 ^ 1 = 0，它也扩展到位向量。即使我们重新排列项并以不同的顺序组合它们，这个性质也成立，因此 `(a ^ b) ^ a = b`。这个性质导致一些有趣的结果和巧妙的技巧，我们将在问题 2.10 中探索。
位向量的一个有用应用是表示**有限集合**。我们可以用位向量 [a_(w-1), ..., a_1, a_0] 编码任何子集 A ⊆ {0, 1, ..., w-1}，其中 a_i = 1 当且仅当 i ∈ A。例如，回顾我们将 a_(w-1) 写在左边而 a_0 写在右边，位向量 a = [01101001] 编码集合 A = {0, 3, 5, 6}，而位向量 b = [01010101] 编码集合 B = {0, 2, 4, 6}。用这种方式编码集合，布尔运算 `|` 和 `&` 分别对应于集合并和交，`~` 对应于集合补。继续我们前面的例子，运算 `a & b` 产生位向量 [01000001]，而 A ∩ B = {0, 6}。

我们将在许多实际应用中看到用位向量编码集合。例如，在第 8 章中，我们将看到有许多不同的信号可以中断程序的执行。我们可以通过指定位向量**掩码**来选择性地启用或禁用不同的信号，其中位位置 i 处的 1 表示信号 i 被启用，0 表示它被禁用。因此，掩码表示已启用信号的集合。

**练习题 2.9**（答案见第 182 页）

计算机通过混合三种不同颜色的光（红、绿、蓝）在视频屏幕或液晶显示器上生成彩色图片。想象一个简单的方案，有三种不同的灯，每个都可以打开或关闭，投射到玻璃屏幕上。

然后我们可以根据光源 R、G 和 B 的缺失（0）或存在（1）创建八种不同的颜色：

| R | G | B | 颜色 |
|---|---|---|------|
| 0 | 0 | 0 | 黑色 (Black) |
| 0 | 0 | 1 | 蓝色 (Blue) |
| 0 | 1 | 0 | 绿色 (Green) |
| 0 | 1 | 1 | 青色 (Cyan) |
| 1 | 0 | 0 | 红色 (Red) |
| 1 | 0 | 1 | 品红 (Magenta) |
| 1 | 1 | 0 | 黄色 (Yellow) |
| 1 | 1 | 1 | 白色 (White) |

每种颜色都可以表示为长度为 3 的位向量，我们可以对它们应用布尔运算。

- A. 颜色的补色是通过关闭打开的灯和打开关闭的灯来形成的。上面列出的八种颜色各自的补色是什么？
- B. 描述对以下颜色应用布尔运算的效果：
  - Blue | Green =
  - Yellow & Cyan =
  - Red ^ Magenta =

### 2.1.7 C 中的位级运算

C 的一个有用特性是它支持按位布尔运算。事实上，我们用于布尔运算的符号正是 C 使用的：`|` 表示或，`&` 表示与，`~` 表示非，`^` 表示异或。这些可以应用于任何"整数"数据类型，包括图 2.3 中列出的所有类型。以下是数据类型 `char` 的一些表达式求值示例：

| C 表达式 | 二进制表达式 | 二进制结果 | 十六进制结果 |
|----------|--------------|------------|--------------|
| `~0x41` | ~[0100 0001] | [1011 1110] | 0xBE |
| `~0x00` | ~[0000 0000] | [1111 1111] | 0xFF |
| `0x69 & 0x55` | [0110 1001] & [0101 0101] | [0100 0001] | 0x41 |
| `0x69 \| 0x55` | [0110 1001] \| [0101 0101] | [0111 1101] | 0x7D |

正如我们的例子所示，确定位级表达式效果的最佳方法是将十六进制参数展开为其二进制表示，在二进制中执行运算，然后转换回十六进制。
**练习题 2.10**（答案见第 182 页）

作为对任何位向量 a 都有 `a ^ a = 0` 这个性质的应用，考虑以下程序：

```c
```c
void inplace_swap(int *x, int *y) {
    *y = *x ^ *y;  /* Step 1 */
    *x = *x ^ *y;  /* Step 2 */
    *y = *x ^ *y;  /* Step 3 */
}
```

顾名思义，我们声称这个过程的效果是交换由指针变量 x 和 y 表示的位置存储的值。注意与通常的交换两个值的技术不同，我们不需要第三个位置来临时存储一个值同时移动另一个。这种交换方式没有性能优势；它仅仅是一种智力娱乐。

从 x 和 y 分别指向的位置中的值 a 和 b 开始，填写下表，给出过程每一步后两个位置存储的值。使用 `^` 的性质来证明达到了预期的效果。回顾每个元素都是它自己的加法逆元（即 `a ^ a = 0`）。

| 步骤 | *x | *y |
|------|----|----|
| 初始 | a | b |
| 步骤 1 | | |
| 步骤 2 | | |
| 步骤 3 | | |

**练习题 2.11**（答案见第 182 页）

有了问题 2.10 中的函数 `inplace_swap`，你决定编写代码，通过从数组两端交换元素并向中间工作来反转数组的元素。

你得出以下函数：

```c
void reverse_array(int a[], int cnt) {
    int first, last;
    for (first = 0, last = cnt-1;
         first <= last;
         first++, last--)
        inplace_swap(&a[first], &a[last]);
}
```

当你将函数应用于包含元素 1、2、3 和 4 的数组时，你发现数组现在如预期的那样有元素 4、3、2 和 1。然而，当你在包含元素 1、2、3、4 和 5 的数组上尝试时，你惊讶地发现数组现在有元素 5、4、0、2 和 1。事实上，你发现代码在偶数长度的数组上总是正确工作，但每当数组长度为奇数时，它会将中间元素设置为 0。

- A. 对于奇数长度 cnt = 2k + 1 的数组，函数 `reverse_array` 最后一次迭代中变量 `first` 和 `last` 的值是什么？
- B. 为什么这次对函数 `inplace_swap` 的调用会将数组元素设置为 0？
- C. 对 `reverse_array` 代码进行什么简单修改可以消除这个问题？
位级运算的一个常见用途是实现**掩码运算**，其中掩码是一个指示字中选定位集的位模式。例如，掩码 `0xFF`（最低有效 8 位为 1）指示字的低位字节。位级运算 `x & 0xFF` 产生一个由 x 的最低有效字节组成的值，但所有其他字节设置为 0。例如，当 x = `0x89ABCDEF` 时，表达式将产生 `0x000000EF`。表达式 `~0` 将产生全 1 的掩码，无论数据表示的大小如何。当数据类型 `int` 为 32 位时，可以将相同的掩码写成 `0xFFFFFFFF`，但这不太具有可移植性。

**练习题 2.12**（答案见第 182 页）

根据变量 x 编写以下值的 C 表达式。你的代码应该对任何字长 w ≥ 8 都有效。作为参考，我们展示了对 x = `0x87654321`、w = 32 求值表达式的结果。

- A. x 的最低有效字节，所有其他位设置为 0。[0x00000021]
- B. x 的除最低有效字节外的所有字节取补，最低有效字节保持不变。[0x789ABC21]
- C. 最低有效字节设置为全 1，x 的所有其他字节保持不变。[0x876543FF]
**练习题 2.13**（答案见第 183 页）

Digital Equipment VAX 计算机是从 1970 年代末到 1980 年代末非常流行的机器。它没有布尔运算 `and` 和 `or` 的指令，而是有指令 `bis`（位设置）和 `bic`（位清除）。两条指令都接受数据字 x 和掩码字 m。它们生成一个结果 z，由根据 m 的位修改的 x 的位组成。对于 `bis`，修改涉及在 m 为 1 的每个位位置将 z 设置为 1。对于 `bic`，修改涉及在 m 为 1 的每个位位置将 z 设置为 0。

要了解这些运算与 C 位级运算的关系，假设我们有实现位设置和位清除运算的函数 `bis` 和 `bic`，并且我们想使用这些函数来实现计算按位运算 `|` 和 `^` 的函数，而不使用任何其他 C 运算。填写下面缺失的代码。提示：为运算 `bis` 和 `bic` 编写 C 表达式。

```c
/* 实现 bis 和 bic 运算的函数声明 */
int bis(int x, int m);
int bic(int x, int m);

/* 仅使用函数 bis 和 bic 调用计算 x|y */
int bool_or(int x, int y) {
    int result = _________;
    return result;
}

/* 仅使用函数 bis 和 bic 调用计算 x^y */
int bool_xor(int x, int y) {
    int result = _________;
    return result;
}
```
### 2.1.8 C 中的逻辑运算

C 还提供了一组逻辑运算符 `||`、`&&` 和 `!`，它们对应于逻辑的或、与和非运算。这些很容易与位级运算混淆，但它们的行为完全不同。逻辑运算将任何非零参数视为表示真（true），将参数 0 视为表示假（false）。它们返回 1 或 0，分别指示真或假的结果。以下是一些表达式求值的例子：

| 表达式 | 结果 |
|--------|------|
| `!0x41` | 0x00 |
| `!0x00` | 0x01 |
| `!!0x41` | 0x01 |
| `0x69 && 0x55` | 0x01 |
| `0x69 \|\| 0x55` | 0x01 |

注意，只有在参数被限制为 0 或 1 的特殊情况下，位运算才会具有与其逻辑对应运算匹配的行为。

逻辑运算符 `&&` 和 `||` 与其位级对应运算 `&` 和 `|` 之间的第二个重要区别是，如果通过求值第一个参数就可以确定表达式的结果，则逻辑运算符不会求值它们的第二个参数。因此，例如，表达式 `a && 5/a` 永远不会导致除以零，表达式 `p && *p++` 永远不会导致对空指针的解引用。
**练习题 2.14**（答案见第 183 页）

假设 a 和 b 分别有字节值 `0x55` 和 `0x46`。填写下表，指出不同 C 表达式的字节值：

| 表达式 | 值 | 表达式 | 值 |
|--------|-----|--------|-----|
| `a & b` | | `a && b` | |
| `a \| b` | | `a \|\| b` | |
| `~a \| ~b` | | `!a \|\| !b` | |
| `a & !b` | | `a && ~b` | |

**练习题 2.15**（答案见第 184 页）

仅使用位级和逻辑运算，编写一个等价于 `x == y` 的 C 表达式。换句话说，当 x 和 y 相等时返回 1，否则返回 0。
### 2.1.9 C 中的移位运算

C 还提供了一组移位运算，用于将位模式向左和向右移动。对于具有位表示 [x_(w-1), x_(w-2), ..., x_0] 的操作数 x，C 表达式 `x << k` 产生具有位表示 [x_(w-k-1), x_(w-k-2), ..., x_0, 0, ..., 0] 的值。也就是说，x 向左移动 k 位，丢弃 k 个最高有效位，并用 k 个零填充右端。移位量应该是 0 到 w-1 之间的值。移位运算从左到右结合，所以 `x << j << k` 等价于 `(x << j) << k`。

有一个对应的右移运算，在 C 中写作 `x >> k`，但它有一些微妙的行为。通常，机器支持两种形式的右移：

- **逻辑右移**：逻辑右移用 k 个零填充左端，给出结果 [0, ..., 0, x_(w-1), x_(w-2), ..., x_k]。
- **算术右移**：算术右移用 k 个最高有效位的重复填充左端，给出结果 [x_(w-1), ..., x_(w-1), x_(w-1), x_(w-2), ..., x_k]。这个约定可能看起来很奇特，但正如我们将看到的，它对于操作有符号整数数据很有用。

作为例子，下表显示了对 8 位参数 x 的两个不同值应用不同移位运算的效果：

| 运算 | 值 1 | 值 2 |
|------|------|------|
| 参数 x | [01100011] | [10010101] |
| x << 4 | [0011**0000**] | [0101**0000**] |
| x >> 4（逻辑）| [**0000**0110] | [**0000**1001] |
| x >> 4（算术）| [**0000**0110] | [**1111**1001] |

斜体数字指示填充右端（左移）或左端（右移）的值。注意除了一个条目外，所有条目都涉及用零填充。例外是算术右移 [10010101] 的情况。由于其最高有效位是 1，这将被用作填充值。

C 标准没有精确定义应该对有符号数使用哪种类型的右移——可以使用算术或逻辑移位。不幸的是，这意味着假设一种形式或另一种形式的任何代码都可能遇到可移植性问题。然而，在实践中，几乎所有的编译器/机器组合都对有符号数据使用算术右移，许多程序员假设情况就是这样。另一方面，对于无符号数据，右移必须是逻辑的。

与 C 相比，Java 对应该如何执行右移有精确的定义。表达式 `x >> k` 将 x 算术移位 k 个位置，而 `x >>> k` 将其逻辑移位。
**练习题 2.16**（答案见第 184 页）

填写下表，显示不同移位运算对单字节量的效果。思考移位运算的最佳方法是使用二进制表示。将初始值转换为二进制，执行移位，然后转换回十六进制。每个答案应该是 8 个二进制数字或 2 个十六进制数字。

| a (十六进制) | a (二进制) | a << 2 (二进制) | a << 2 (十六进制) | a >> 3 逻辑 (二进制) | a >> 3 逻辑 (十六进制) | a >> 3 算术 (二进制) | a >> 3 算术 (十六进制) |
|-------------|-----------|-----------------|-------------------|---------------------|----------------------|---------------------|----------------------|
| 0xD4 | | | | | | | |
| 0x64 | | | | | | | |
| 0x72 | | | | | | | |
| 0x44 | | | | | | | |
0x72
0x44

> **旁注：对于较大的 k 值进行移位**
>
> 对于由 w 位组成的数据类型，当移位量 k ≥ w 时应该产生什么效果？例如，假设数据类型 `int` 有 w = 32 位，计算以下表达式应该有什么效果？
>
> ```c
> int lval = 0xFEDCBA98 << 32;
> int aval = 0xFEDCBA98 >> 36;
> unsigned uval = 0xFEDCBA98u >> 40;
> ```
>
> C 标准谨慎地避免说明在这种情况下应该怎么做。在许多机器上，移位指令在移动 w 位值时只考虑移位量的低 log₂w 位，因此移位量实际上被计算为 k mod w。例如，当 w = 32 时，上述三个移位将分别按 0、4 和 8 位来计算，得到结果：
>
> | 变量 | 结果 |
> |------|------|
> | lval | 0xFEDCBA98 |
> | aval | 0xFFEDCBA9 |
> | uval | 0x00FEDCBA |
>
> 然而，这种行为对 C 程序并不是保证的，因此移位量应该保持小于字长。
>
> 另一方面，Java 明确要求移位量应该按照我们展示的模运算方式来计算。
> **旁注：移位运算的运算符优先级问题**
>
> 可能会想写表达式 `1<<2 + 3<<4`，打算让它表示 `(1<<2) + (3<<4)`。然而，在 C 中，前一个表达式等价于 `1 << (2+3) << 4`，因为加法（和减法）比移位有更高的优先级。从左到右的结合性规则然后导致它被括起来为 `(1 << (2+3)) << 4`，给出值 512，而不是预期的 52。
>
> 在 C 表达式中弄错优先级是程序错误的常见来源，而且这些错误通常很难通过检查发现。如有疑问，请使用括号！
## 2.2 整数表示

在本节中，我们描述两种不同的方式来使用位编码整数——一种只能表示非负数，另一种可以表示负数、零和正数。我们稍后将看到它们在数学性质和机器级实现上都密切相关。我们还研究扩展或收缩编码整数以适应不同长度表示的效果。

**图 2.8 整数数据和算术运算的术语。下标 w 表示数据表示中的位数。"页码"列指示定义该术语的页面。**

| 符号 | 类型 | 含义 | 页码 |
|------|------|------|------|
| B2T_w | 函数 | 二进制到补码 | 100 |
| B2U_w | 函数 | 二进制到无符号 | 98 |
| U2B_w | 函数 | 无符号到二进制 | 100 |
| U2T_w | 函数 | 无符号到补码 | 107 |
| T2B_w | 函数 | 补码到二进制 | 101 |
| T2U_w | 函数 | 补码到无符号 | 107 |
| TMin_w | 常量 | 最小补码值 | 101 |
| TMax_w | 常量 | 最大补码值 | 101 |
| UMax_w | 常量 | 最大无符号值 | 99 |
| +^t_w | 运算 | 补码加法 | 126 |
| +^u_w | 运算 | 无符号加法 | 121 |
| *^t_w | 运算 | 补码乘法 | 133 |
| *^u_w | 运算 | 无符号乘法 | 132 |
| -^t_w | 运算 | 补码取反 | 131 |
| -^u_w | 运算 | 无符号取反 | 125 |

这些术语将在介绍过程中引入。这里包含该图作为参考。
### 2.2.1 整数数据类型

C 支持多种整数数据类型——表示有限整数范围的类型。这些在图 2.9 和 2.10 中显示，以及它们在"典型" 32 位和 64 位程序中可以具有的值范围。每种类型都可以用关键字 `char`、`short`、`long` 指定大小，以及指示所表示的数是否都是非负的（声明为 `unsigned`）或可能是负的（默认）。正如我们在图 2.3 中看到的，为不同大小分配的字节数根据程序是为 32 位还是 64 位编译而变化。基于字节分配，不同的大小允许表示不同的值范围。唯一指示的机器相关范围是大小指示符 `long`。大多数 64 位程序使用 8 字节表示，提供比 32 位程序使用的 4 字节表示更宽的值范围。

图 2.9 和 2.10 中需要注意的一个重要特征是范围不是对称的——负数的范围比正数的范围多延伸一个。当我们考虑如何表示负数时，将看到为什么会这样。

**图 2.9 32 位程序的 C 整数数据类型的典型范围**

| C 数据类型 | 最小值 | 最大值 |
|-----------|--------|--------|
| `[signed] char` | −128 | 127 |
| `unsigned char` | 0 | 255 |
| `short` | −32,768 | 32,767 |
| `unsigned short` | 0 | 65,535 |
| `int` | −2,147,483,648 | 2,147,483,647 |
| `unsigned` | 0 | 4,294,967,295 |
| `long` | −2,147,483,648 | 2,147,483,647 |
| `unsigned long` | 0 | 4,294,967,295 |
| `int32_t` | −2,147,483,648 | 2,147,483,647 |
| `uint32_t` | 0 | 4,294,967,295 |
| `int64_t` | −9,223,372,036,854,775,808 | 9,223,372,036,854,775,807 |
| `uint64_t` | 0 | 18,446,744,073,709,551,615 |

**图 2.10 64 位程序的 C 整数数据类型的典型范围**

| C 数据类型 | 最小值 | 最大值 |
|-----------|--------|--------|
| `[signed] char` | −128 | 127 |
| `unsigned char` | 0 | 255 |
| `short` | −32,768 | 32,767 |
| `unsigned short` | 0 | 65,535 |
| `int` | −2,147,483,648 | 2,147,483,647 |
| `unsigned` | 0 | 4,294,967,295 |
| `long` | −9,223,372,036,854,775,808 | 9,223,372,036,854,775,807 |
| `unsigned long` | 0 | 18,446,744,073,709,551,615 |
| `int32_t` | −2,147,483,648 | 2,147,483,647 |
| `uint32_t` | 0 | 4,294,967,295 |
| `int64_t` | −9,223,372,036,854,775,808 | 9,223,372,036,854,775,807 |
| `uint64_t` | 0
18,446,744,073,709,551,615
Figure 2.10
Typical ranges for C integral data types for 64-bit programs.
The C standards deﬁne minimum ranges of values that each data type must
be able to represent. As shown in Figure 2.11, their ranges are the same or smaller
than the typical implementations shown in Figures 2.9 and 2.10. In particular,
with the exception of the ﬁxed-size data types, we see that they require only a

> **C 语言新手？C、C++ 和 Java 中的有符号和无符号数**
>
> C 和 C++ 都支持有符号（默认）和无符号数。Java 只支持有符号数。

**图 2.11 C 整数数据类型的保证范围。C 标准要求数据类型至少具有这些值范围。**

| C 数据类型 | 最小值 | 最大值 |
|-----------|--------|--------|
| `[signed] char` | −127 | 127 |
| `unsigned char` | 0 | 255 |
| `short` | −32,767 | 32,767 |
| `unsigned short` | 0 | 65,535 |
| `int` | −32,767 | 32,767 |
| `unsigned` | 0 | 65,535 |
| `long` | −2,147,483,647 | 2,147,483,647 |
| `unsigned long` | 0 | 4,294,967,295 |
| `int32_t` | −2,147,483,648 | 2,147,483,647 |
| `uint32_t` | 0 | 4,294,967,295 |
| `int64_t` | −9,223,372,036,854,775,808 | 9,223,372,036,854,775,807 |
| `uint64_t` | 0 | 18,446,744,073,709,551,615 |

注意图 2.11 中要求的范围允许正负数对称范围。我们还看到数据类型 `int` 可以用 2 字节数实现，尽管这主要是 16 位机器时代的遗留。我们还看到 `long` 可以用 4 字节数实现，对于 32 位程序通常是这样。固定大小的数据类型保证值的范围将恰好是图 2.9 中典型数字给出的值，包括负数和正数之间的不对称性。
### 2.2.2 无符号编码

让我们考虑一个 w 位的整数数据类型。我们将位向量写成 **x⃗** 来表示整个向量，或写成 [x_(w-1), x_(w-2), ..., x_0] 来表示向量内的各个位。将 **x⃗** 视为用二进制表示法写的数，我们得到 **x⃗** 的无符号解释。在这种编码中，每个位 x_i 的值为 0 或 1，后者表示值 2^i 应该作为数值的一部分包含进来。我们可以将这种解释表示为函数 B2U_w（"binary to unsigned"，长度 w）：

**原理：无符号编码的定义**

对于向量 **x⃗** = [x_(w-1), x_(w-2), ..., x_0]：

B2U_w(**x⃗**) = Σ(i=0 到 w-1) x_i × 2^i

**图 2.12 w = 4 的无符号数示例。当二进制表示中的位 i 的值为 1 时，它对值贡献 2^i。**

示例：
- [0001] = 1
- [0101] = 5
[1011]
[1111]
**原理：无符号编码的定义**
对于向量 $\vec{x} = [x_{w-1}, x_{w-2}, \ldots, x_0]$：
$$B2U_w(\vec{x}) \doteq \sum_{i=0}^{w-1} x_i 2^i \tag{2.1}$$
在这个方程中，符号 ≐ 表示左侧被定义为等于右侧。函数 B2U_w 将长度为 w 的零和一的字符串映射到非负整数。作为例子，图 2.12 显示了由 B2U 给出的从位向量到整数的映射，对于以下情况：
B2U4([0001])
=
0 . 23 + 0 . 22 + 0 . 21 + 1 . 20
=
0 + 0 + 0 + 1
=
1
B2U4([0101])
=
0 . 23 + 1 . 22 + 0 . 21 + 1 . 20
=
0 + 4 + 0 + 1
=
5
B2U4([1011])
=
1 . 23 + 0 . 22 + 1 . 21 + 1 . 20
=
8 + 0 + 2 + 1
=
11
B2U4([1111])
=
1 . 23 + 1 . 22 + 1 . 21 + 1 . 20
=
8 + 4 + 2 + 1
=
15
(2.2)
在图中，我们用长度为 $2^i$ 的向右指向的蓝色条来表示每个位位置 i。
与位向量关联的数值等于所有对应位值为 1 的条的长度之和。

让我们考虑使用 w 位可以表示的值的范围。
最小值由位向量 [00...0] 给出，其整数值为 0；最大值由位向量 [11...1] 给出，
其整数值为 $UMax_w \doteq \sum_{i=0}^{w-1} 2^i = 2^w - 1$。


以 4 位的情况为例，我们有 $UMax_4 = B2U_4([1111]) = 2^4 - 1 = 15$。
因此，函数 $B2U_w$ 可以定义为映射 $B2U_w: \{0, 1\}^w \rightarrow \{0, \ldots, UMax_w\}$。

无符号二进制表示有一个重要的性质：
0 到 $2^w - 1$ 之间的每个数都有唯一的 w 位值编码。例如，

十进制值 11 作为无符号 4 位数只有一种表示——即 [1011]。
我们将其作为一个数学原理来强调，先陈述然后解释。

**原理：无符号编码的唯一性**
函数 $B2U_w$ 是一个双射。
数学术语**双射（bijection）**指的是一个双向函数 f：
它将值 x 映射到值 y（其中 y = f(x)），但它也可以反向运算，
因为对于每个 y，存在唯一的值 x 使得 f(x) = y。
这由逆函数 $f^{-1}$ 给出，对于我们的例子，$x = f^{-1}(y)$。函数 $B2U_w$
将每个长度为 w 的位向量映射到 0 到 $2^w - 1$ 之间的唯一数，
it has an inverse, which we call U2Bw (for “unsigned to binary”), that maps each
将 0 到 $2^w - 1$ 范围内的每个数映射到唯一的 w 位模式。
### 2.2.3 补码编码

对于许多应用，我们也希望表示负值。有符号数最常见的计算机表示被称为**补码（two's-complement）**形式。它通过将字的最高有效位解释为具有负权重来定义。我们将这种解释表示为函数 B2T_w（"binary to two's complement"，长度 w）：
principle: Deﬁnition of two’s-complement encoding
对于向量 $\vec{x} = [x_{w-1}, x_{w-2}, \ldots, x_0]$：
B2Tw(⃗x) .= −xw−12w−1 +




(2.3)
最高有效位 x_(w-1) 也称为**符号位**。它的"权重"是 -2^(w-1)，是其在无符号表示中权重的取反。当符号位设置为 1 时，表示的值是负的；当设置为 0 时，值是非负的。
作为例子，图 2.13 显示了由 B2T 给出的从位向量到整数的映射，对于以下情况：

B2T4([0001])
=
−0 . 23 + 0 . 22 + 0 . 21 + 1 . 20
=
0 + 0 + 0 + 1
=
1
B2T4([0101])
=
−0 . 23 + 1 . 22 + 0 . 21 + 1 . 20
=
0 + 4 + 0 + 1
=
5
B2T4([1011])
=
−1 . 23 + 0 . 22 + 1 . 21 + 1 . 20
=
−8 + 0 + 2 + 1
=
−5
B2T4([1111])
=
−1 . 23 + 1 . 22 + 1 . 21 + 1 . 20
=
−8 + 4 + 2 + 1
=
−1
(2.4)
在图中，我们通过将符号位显示为向左指向的灰色条来表示它具有负权重。与位向量关联的数值然后由可能的向左指向的灰色条和向右指向的蓝色条的组合给出。

**图 2.13 w = 4 的补码数示例。位 3 作为符号位；当设置为 1 时，它对值贡献 -2³ = -8。这种权重显示为向左指向的灰色条。**
8
7
6
5
4
3
2
1
0
–1
–2
–3
–4
–5
–6
–7
–8
20 = 1
21 = 2
22 = 4
–23 = –8
[0001]
[0101]
[1011]
[1111]
我们看到图 2.12 和 2.13 的位模式是相同的（方程 2.2 和 2.4 也是如此），但当最高有效位为 1 时值不同，因为在一种情况下它的权重是 +8，而在另一种情况下它的权重是 -8。
让我们考虑可以表示为 w 位补码数的值的范围。最小可表示值由位向量 [10...0]（设置具有负权重的位但清除所有其他位）给出，其整数值为 TMin_w ≐ -2^(w-1)。最大值由位向量 [01...1]（清除具有负权重的位但设置所有其他位）给出，其整数值为 TMax_w ≐
w−2

$\sum_{i=0}^{w-2} 2^i = 2^{w-1} - 1$。以 4 位的情况为例，我们有 $TMin_4 = B2T_4([1000]) =
-2^3 = -8$ 且 $TMax_4 = B2T_4([0111]) = 2^2 + 2^1 + 2^0 = 4 + 2 + 1 = 7$。
我们可以看到 B2T_w 是长度为 w 的位模式到 TMin_w 和 TMax_w 之间数字的映射，写作 B2T_w: {0, 1}^w → {TMin_w, ..., TMax_w}。正如我们在无符号表示中看到的，可表示范围内的每个数字都有一个唯一的 w 位补码数编码。这导致了补码数的一个原理，类似于无符号数的原理：

**原理：补码编码的唯一性**

函数 B2T_w 是一个双射。

我们定义函数 T2B_w（"two's complement to binary"）为 B2T_w 的逆函数。也就是说，对于一个数 x，使得 TMin_w ≤ x ≤ TMax_w，T2B_w(x) 是编码 x 的（唯一的）w 位模式。
**练习题 2.17**（答案见第 184 页）

假设 w = 4，我们可以为每个可能的十六进制数字分配一个数值，假设无符号或补码解释。根据这些解释填写下表，写出方程 2.1 和 2.3 中所示求和中非零的 2 的幂：

| **x⃗** 十六进制 | 二进制 | B2U₄(**x⃗**) | B2T₄(**x⃗**) |
|----------------|--------|--------------|--------------|
| 0xA | [1010] | 2³ + 2¹ = 10 | -2³ + 2¹ = -6 |
| 0x1 | | | |
| 0xB | | | |
| 0x2 | | | |
| 0x7 | | | |
| 0xC | | | |
图 2.14 显示了不同字长的几个重要数字的位模式和数值。前三个给出了以 UMax_w、TMin_w 和 TMax_w 的值表示的可表示整数的范围。我们在接下来的讨论中将经常引用这三个特殊值。当可以从上下文推断 w 或 w 不是讨论的核心时，我们将省略下标 w 并引用值 UMax、TMin 和 TMax。

关于这些数字有几点值得强调。首先，正如在图 2.9 和 2.10 中观察到的，补码范围是不对称的：|TMin| = |TMax| + 1；也就是说，TMin 没有正数对应值。正如我们将看到的，这导致了补码算术的一些特殊性质，并且可能是微妙程序错误的来源。这种不对称性产生是因为一半的位模式（符号位设置为 1 的那些）表示负数，而另一半（符号位设置为 0 的那些）表示非负数。由于 0 是非负数，这意味着它可以表示的正数比负数少一个。其次，最大无符号值略大于最大补码值的两倍：UMax = 2·TMax + 1。所有在补码表示法中表示负数的位模式在无符号表示中变成正值。
**图 2.14 重要数字。显示了数值和十六进制表示。**

| 值 | 字长 w=8 | w=16 | w=32 | w=64 |
|----|----------|------|------|------|
| UMax_w (十六进制) | 0xFF | 0xFFFF | 0xFFFFFFFF | 0xFFFFFFFFFFFFFFFF |
| UMax_w (十进制) | 255 | 65,535 | 4,294,967,295 | 18,446,744,073,709,551,615 |
| TMin_w (十六进制) | 0x80 | 0x8000 | 0x80000000 | 0x8000000000000000 |
| TMin_w (十进制) | −128 | −32,768 | −2,147,483,648 | −9,223,372,036,854,775,808 |
| TMax_w (十六进制) | 0x7F | 0x7FFF | 0x7FFFFFFF | 0x7FFFFFFFFFFFFFFF |
| TMax_w (十进制) | 127 | 32,767 | 2,147,483,647 | 9,223,372,036,854,775,807 |
| −1 (十六进制) | 0xFF | 0xFFFF | 0xFFFFFFFF | 0xFFFFFFFFFFFFFFFF |
| 0 (十六进制) | 0x00 | 0x0000 | 0x00000000 | 0x0000000000000000 |

> **旁注：固定大小整数类型的更多内容**
>
> 对于某些程序，使用具有特定大小的表示来编码数据类型是必不可少的。例如，当编写程序使机器能够根据标准协议通过互联网通信时，拥有与协议指定的数据类型兼容的数据类型很重要。我们已经看到某些 C 数据类型，特别是 `long`，在不同的机器上有不同的范围，
而且实际上 C 标准只规定了任何数据类型的最小范围，而不是精确的范围。
虽然我们可以选择在大多数机器上与标准表示兼容的数据类型，
但不能保证可移植性。
我们已经遇到了固定大小整数类型的 32 位和 64 位版本（图 2.3）；
它们是更大的数据类型类的一部分。ISO C99 标准在文件 stdint.h 中引入了这类整数类型。
该文件定义了一组具有 intN_t 和 uintN_t 形式声明的数据类型，
指定不同 N 值的 N 位有符号和无符号整数。
N 的确切值依赖于实现，但大多数编译器允许 8、16、32 和 64 的值。因此，我们可以
通过给变量类型 uint16_t 来明确声明一个无符号 16 位变量，
并用 int32_t 声明一个 32 位有符号变量。
这些数据类型还附带了一组宏，定义每个 N 值的最小和最大值。
这些宏的名称形式为 INTN_MIN、INTN_MAX 和 UINTN_MAX。
使用固定宽度类型进行格式化打印需要使用宏，这些宏以系统相关的方式展开为格式字符串。
例如，类型为 int32_t 和 uint64_t 的变量 x 和 y 的值可以通过以下 printf 调用来打印：

printf("x = %" PRId32 ", y = %" PRIu64 "\n", x, y);
当编译为 64 位程序时，宏 PRId32 展开为字符串 "d"，而 PRIu64 展开为一对字符串 "l" "u"。
当 C 预处理器遇到仅由空格（或其他空白字符）分隔的字符串常量序列时，
它会将它们连接在一起。因此，上述 printf 调用变为：

printf("x = %d, y = %lu\n", x, y);
使用宏可以确保无论代码如何编译，都会生成正确的格式字符串。

图 2.14 还显示了常量 -1 和 0 的表示。注意 -1 与 UMax 具有相同的位表示——全是 1 的字符串。
数值 0 在两种表示中都表示为全零的字符串。

The C standards do not require signed integers to be represented in two’s-
关心在所有可能机器上最大化可移植性的程序员
不应该假设任何特定的可表示值范围（超出图 2.11 中指示的范围），
也不应该假设有符号数的任何特定表示。

On the other hand, many programs are written assuming a two’s-complement
representation of signed numbers, and the “typical” ranges shown in Figures 2.9
C 库中的文件 <limits.h> 定义了一组常量，


> **旁注：有符号数的其他表示方法**
>
> 有符号数还有两种其他的标准表示方法：
>
> **反码（Ones' complement）**：除了最高有效位的权重是 $-(2^{w-1} - 1)$ 而不是 $-2^{w-1}$ 之外，与补码相同：
>
> $$B2O_w(\vec{x}) \doteq -x_{w-1}(2^{w-1} - 1) + \sum_{i=0}^{w-2} x_i 2^i$$



> **原码（Sign magnitude）**：最高有效位是符号位，决定其余各位应该取负权重还是正权重：
$$B2S_w(\vec{x}) \doteq (-1)^{x_{w-1}} \cdot \sum_{i=0}^{w-2} x_i 2^i$$
w−2




> 这两种表示方法都有一个奇特的性质：数字 0 有两种不同的编码方式。对于这两种表示，$[00 \ldots 0]$ 都被解释为 $+0$。在原码中，$-0$ 可以表示为 $[10 \ldots 0]$；在反码中，$-0$ 可以表示为 $[11 \ldots 1]$。虽然过去曾经制造过基于反码表示的机器，但几乎所有现代机器都使用补码。我们将会看到，原码编码用于浮点数。
>
> 注意撇号的不同位置：two's complement（补码）versus ones' complement（反码）。术语 "two's complement" 源于这样一个事实：对于非负数 x，我们可以计算 $-x$ 的 w 位表示为 $2^w - x$（一个 2）。术语 "ones' complement" 来自于这样的性质：在这种表示法中，我们可以将 $-x$ 计算为 $[111 \ldots 1] - x$（多个 1）。
界定特定机器上运行的编译器的不同整数数据类型的范围。
on which the compiler is running. For example, it deﬁnes constants INT_MAX, INT_
MIN, and UINT_MAX describing the ranges of signed and unsigned integers. For a
two’s-complement machine in which data type int has w bits, these constants
correspond to the values of TMaxw, TMinw, and UMaxw.
The Java standard is quite speciﬁc about integer data type ranges and repre-
sentations. It requires a two’s-complement representation with the exact ranges
shown for the 64-bit case (Figure 2.10). In Java, the single-byte data type is called
byte instead of char. These detailed requirements are intended to enable Java
programs to behave identically regardless of the machines or operating systems
running them.
To get a better understanding of the two’s-complement representation, con-
sider the following code example:
1
short x = 12345;
2
short mx = -x;
3
4
show_bytes((byte_pointer) &x, sizeof(short));
5
show_bytes((byte_pointer) &mx, sizeof(short));

12,345
−12,345
53,191
Weight
Bit
Value
Bit
Value
Bit
Value
1
1
1
1
1
1
1
2
0
0
1
2
1
2
4
0
0
1
4
1
4
8
1
8
0
0
0
0
16
1
16
0
0
0
0
32
1
32
0
0
0
0
64
0
0
1
64
1
64
128
0
0
1
128
1
128
256
0
0
1
256
1
256
512
0
0
1
512
1
512
1,024
0
0
1
1,024
1
1,024
2,048
0
0
1
2,048
1
2,048
4,096
1
4,096
0
0
0
0
8,192
1
8,192
0
0
0
0
16,384
0
0
1
16,384
1
16,384
±32,768
0
0
1
−32,768
1
32,768
Total
12,345
−12,345
53,191
Figure 2.15
Two’s-complement representations of 12,345 and −12,345, and
unsigned representation of 53,191. Note that the latter two have identical bit
representations.
When run on a big-endian machine, this code prints 30 39 and cf c7, indi-
cating that x has hexadecimal representation 0x3039, while mx has hexadeci-
mal representation 0xCFC7. Expanding these into binary, we get bit patterns
[0011000000111001] for x and [1100111111000111] for mx. As Figure 2.15 shows,
Equation 2.3 yields values 12,345 and −12,345 for these two bit patterns.
Practice Problem 2.18 (solution page 185)
In Chapter 3, we will look at listings generated by a disassembler, a program that
converts an executable program ﬁle back to a more readable ASCII form. These
ﬁles contain many hexadecimal numbers, typically representing values in two’s-
complement form. Being able to recognize these numbers and understand their
signiﬁcance (for example, whether they are negative or positive) is an important
skill.
For the lines labeled A–I (on the right) in the following listing, convert the
hexadecimal values (in 32-bit two’s-complement form) shown to the right of the
instruction names (sub, mov, and add) into their decimal equivalents:

4004d0:
48 81 ec e0 02 00 00
sub
$0x2e0,%rsp
A.
4004d7:
48 8b 44 24 a8
mov
-0x58(%rsp),%rax
B.
4004dc:
48 03 47 28
add
0x28(%rdi),%rax
C.
4004e0:
48 89 44 24 d0
mov
%rax,-0x30(%rsp)
D.
4004e5:
48 8b 44 24 78
mov
0x78(%rsp),%rax
E.
4004ea:
48 89 87 88 00 00 00
mov
%rax,0x88(%rdi)
F.
4004f1:
48 8b 84 24 f8 01 00
mov
0x1f8(%rsp),%rax
G.
4004f8:
00
4004f9:
48 03 44 24 08
add
0x8(%rsp),%rax
4004fe:
48 89 84 24 c0 00 00
mov
%rax,0xc0(%rsp)
H.
400505:
00
400506:
48 8b 44 d4 b8
mov
-0x48(%rsp,%rdx,8),%rax
I.
### 2.2.4 有符号和无符号之间的转换

C 允许在不同的数值数据类型之间进行强制转换。例如，假设
变量 x 声明为 int，u 声明为 unsigned。表达式 (unsigned) x
将 x 的值转换为无符号值，(int) u 将 u 的值转换为有符号整数。
将有符号值强制转换为无符号，或反过来，应该有什么效果？
or vice versa? From a mathematical perspective, one can imagine several different
显然，我们想保留可以在两种形式中表示的任何值。
另一方面，将负值转换为无符号可能产生零。
zero. Converting an unsigned value that is too large to be represented in two’s-
然而，对于大多数 C 实现，这个问题的答案是基于位级视角的，
而不是数值视角。

例如，考虑以下代码：
1
short
int
v
= -12345;
2
unsigned short uv = (unsigned short) v;
3
printf("v = %d, uv = %u\n", v, uv);
When run on a two’s-complement machine, it generates the following output:
v = -12345, uv = 53191
我们在这里看到的是，强制转换的效果是保持位值不变
但改变这些位的解释方式。我们在图 2.15 中看到，-12,345 的 16 位
two’s-complement representation of −12,345 is identical to the 16-bit unsigned
从 short 到 unsigned short 的转换改变了数值，但没有改变位表示。

类似地，考虑以下代码：
1
unsigned u = 4294967295u;
/* UMax */
2
int
tu = (int) u;

3
printf("u = %u, tu = %d\n", u, tu);
When run on a two’s-complement machine, it generates the following output:
u = 4294967295, tu = -1
从图 2.14 可以看出，对于 32 位字长，以无符号形式表示 4,294,967,295 (UMax32)
ing 4,294,967,295 (UMax32) in unsigned form and −1 in two’s-complement form
在从 unsigned 转换到 int 时，底层的位表示保持不变。

这是大多数 C 实现处理相同字长的有符号和无符号数之间转换的一般规则——
数值可能改变，但位模式不变。
让我们用更数学化的形式来表达这个想法。
我们定义了函数 U2Bw 和 T2Bw，将数字映射到它们以无符号或补码形式的位表示。
numbers to their bit representations in either unsigned or two’s-complement form.
也就是说，给定范围 $0 \leq x < UMax_w$ 内的整数 x，函数 $U2B_w(x)$
给出 x 的唯一 w 位无符号表示。类似地，当 x 在范围
range TMinw ≤x ≤TMaxw, the function T2Bw(x) gives the unique w-bit two’s-

现在定义函数 $T2U_w$ 为 $T2U_w(x) \doteq B2U_w(T2B_w(x))$。这个函数
取 $TMin_w$ 和 $TMax_w$ 之间的数并产生 0 到 $UMax_w$ 之间的数，
其中两个数具有相同的位表示，只是参数有补码表示而结果是无符号的。
the argument has a two’s-complement representation while the result is unsigned.
类似地，对于 0 到 $UMax_w$ 之间的 x，函数 $U2T_w$ 定义为 $U2T_w(x) \doteq
B2Tw(U2Bw(x)), yields the number having the same two’s-complement represen-

继续我们前面的例子，我们从图 2.15 看到 $T2U_{16}(-12,345)
= 53,191$，而 $U2T_{16}(53,191) = -12,345$。也就是说，以十六进制写为 0xCFC7 的 16 位模式
hexadecimal as 0xCFC7 is both the two’s-complement representation of −12,345
还要注意 $12,345 + 53,191 = 65,536 = 2^{16}$。
这个性质推广为给定位模式表示的两个数值（补码和无符号）之间的关系。
meric values (two’s complement and unsigned) represented by a given bit pat-
类似地，从图 2.14 我们看到 $T2U_{32}(-1) = 4,294,967,295$，且
$U2T_{32}(4,294,967,295) = -1$。也就是说，UMax 以无符号形式的位表示
signed form as does −1 in two’s-complement form. We can also see the relationship

We see, then, that function T2U describes the conversion of a two’s-
而 U2T 以相反的方向转换。
posite direction. These describe the effect of casting between these data types in
most C implementations.
Practice Problem 2.19 (solution page 185)
Using the table you ﬁlled in when solving Problem 2.17, ﬁll in the following table
describing the function T2U4:

x
T2U4(x)
−1
−5
−6
−4
1
8
The relationship we have seen, via several examples, between the two’s-
complement and unsigned values for a given bit pattern can be expressed as a
property of the function T2U:
principle: Conversion from two’s complement to unsigned
For x such that TMinw ≤x ≤TMaxw:
T2Uw(x) =
 x + 2w,
x < 0
x,
x ≥0
(2.5)
For example, we saw that T2U16(−12,345) = −12,345 + 216 = 53,191, and also
that T2Uw(−1) = −1 + 2w = UMaxw.
This property can be derived by comparing Equations 2.1 and 2.3.
derivation: Conversion from two’s complement to unsigned
Comparing Equations 2.1 and 2.3, we can see that for bit pattern ⃗x, if we compute
the difference B2Uw(⃗x) −B2Tw(⃗x), the weighted sums for bits from 0 to w −2 will
cancel each other, leaving a value B2Uw(⃗x) −B2Tw(⃗x) = xw−1(2w−1 −−2w−1) =
xw−12w. This gives a relationship B2Uw(⃗x) = B2Tw(⃗x) + xw−12w. We therefore
have
B2Uw(T2Bw(x)) = T2Uw(x) = x + xw−12w
(2.6)
In a two’s-complement representation of x, bit xw−1 determines whether or not x
is negative, giving the two cases of Equation 2.5.
As examples, Figure 2.16 compares how functions B2U and B2T assign values
to bit patterns for w = 4. For the two’s-complement case, the most signiﬁcant bit
serves as the sign bit, which we diagram as a leftward-pointing gray bar. For the
unsigned case, this bit has positive weight, which we show as a rightward-pointing
black bar. In going from two’s complement to unsigned, the most signiﬁcant bit
changes its weight from −8 to +8. As a consequence, the values that are nega-
tive in a two’s-complement representation increase by 24 = 16 with an unsigned
representation. Thus, −5 becomes +11, and −1 becomes +15.

Figure 2.16
Comparing unsigned
and two’s-complement
representations for w = 4.
The weight of the most
signiﬁcant bit is −8 for
two’s complement and +8
for unsigned, yielding a net
difference of 16.
8
7
6
5
4
3
2
1
16
15
14
13
12
11
10
9
0
–1
–2
–3
–4
–5
–6
–7
–8
20 = 1
21 = 2
22 = 4
–23 = –8
[1011]
[1111]
23 = 8
+16
+16
Figure 2.17
Conversion from two’s
complement to unsigned.
Function T2U converts
negative numbers to large
positive numbers.
+2w–1
0
–2w–1
2w
0
2w–1
Two’s
complement
Unsigned
Figure 2.17 illustrates the general behavior of function T2U. As it shows, when
mapping a signed number to its unsigned counterpart, negative numbers are con-
verted to large positive numbers, while nonnegative numbers remain unchanged.
Practice Problem 2.20 (solution page 185)
Explain how Equation 2.5 applies to the entries in the table you generated when
解决练习题 2.19。
反过来，我们可以陈述无符号数 u 与其有符号对应值 $U2T_w(u)$ 之间的关系：

principle: Unsigned to two’s-complement conversion
对于 $0 \leq u \leq UMax_w$ 的 u：
U2Tw(u) =
 u,
u ≤TMaxw
u −2w,
u > TMaxw
(2.7)

Figure 2.18
Conversion from
unsigned to two’s
complement. Function
U2T converts numbers
greater than 2w−1 −1 to
negative values.
+2w–1
0
–2w–1
2w
0
2w–1
Two’s
complement
Unsigned
这个原理可以如下证明：
derivation: Unsigned to two’s-complement conversion
Let ⃗u = U2Bw(u). This bit vector will also be the two’s-complement representation
方程 2.1 和 2.3 可以组合得到：
U2Tw(u) = −uw−12w + u
(2.8)
在 u 的无符号表示中，位 $u_{w-1}$ 决定 u 是否大于 $TMax_w = 2^{w-1} - 1$，给出方程 2.7 的两种情况。

函数 U2T 的行为如图 2.18 所示。对于小的（$\leq TMax_w$）数，
从无符号到有符号的转换保持数值不变。
大的（$> TMax_w$）数被转换为负值。
总结一下，我们考虑了在无符号和补码表示之间双向转换的效果。
between unsigned and two’s-complement representations. For values x in the
我们有 $T2U_w(x) = x$ 且 $U2T_w(x) = x$。
bers in this range have identical unsigned and two’s-complement representations.
对于这个范围之外的值，转换要么加上要么减去 $2^w$。
例如，$T2U_w(-1) = -1 + 2^w = UMax_w$——最接近零的负数映射到最大的无符号数。

在另一个极端，可以看到 $T2U_w(TMin_w) = -2^{w-1} + 2^w = 2^{w-1} = TMax_w + 1$——
最负的数映射到正好在正补码数范围之外的无符号数。
two’s-complement numbers. Using the example of Figure 2.15, we can see that

### 2.2.5 C 中的有符号与无符号

如图 2.9 和 2.10 所示，C 支持其所有整数数据类型的有符号和无符号算术。
虽然 C 标准没有规定有符号数的特定表示，但几乎所有机器都使用补码。
ify a particular representation of signed numbers, almost all machines use two’s
通常，大多数数默认是有符号的。例如，
当声明像 12345 或 0x1A2B 这样的常量时，该值被认为是有符号的。
Adding character ‘U’ or ‘u’ as a sufﬁx creates an unsigned constant; for example,


C 允许在无符号和有符号之间转换。虽然 C 标准没有精确规定应该如何进行这种转换，
但大多数系统遵循底层位表示不改变的规则。

这条规则的效果是在从无符号转换到有符号时应用函数 $U2T_w$，
在从有符号转换到无符号时应用 $T2U_w$，其中 w 是数据类型的位数。

转换可能由于显式强制转换而发生，如以下代码：
1
int tx, ty;
2
unsigned ux, uy;
3
4
tx = (int) ux;
5
uy = (unsigned) ty;
或者，当一种类型的表达式被赋值给另一种类型的变量时，它们可以隐式发生，如以下代码：

1
int tx, ty;
2
unsigned ux, uy;
3
4
tx = ux; /* Cast to signed */
5
uy = ty; /* Cast to unsigned */
当使用 printf 打印数值时，指令 %d、%u 和 %x
分别用于将数字打印为有符号十进制、无符号十进制和十六进制格式。
注意 printf 不使用任何类型信息，因此可以用指令 %u 打印 int 类型的值，
用指令 %d 打印 unsigned 类型的值。

例如，考虑以下代码：
1
int x = -1;
2
unsigned u = 2147483648; /* 2 to the 31st */
3
4
printf("x = %u = %d\n", x, x);
5
printf("u = %u = %d\n", u, u);
当编译为 32 位程序时，它打印以下内容：
x = 4294967295 = -1
u = 2147483648 = -2147483648
在两种情况下，printf 首先将字当作无符号数打印，然后当作有符号数打印。
我们可以看到转换例程的作用：$T2U_{32}(-1) = UMax_{32} = 2^{32} - 1$
且 $U2T_{32}(2^{31}) = 2^{31} - 2^{32} = -2^{31} = TMin_{32}$。

Some possibly nonintuitive behavior arises due to C’s handling of expres-
当执行一个操作数是有符号而另一个是无符号的操作时，
C 隐式地将有符号参数转换为无符号，并假设数是非负的来执行操作。


Expression
Type
Evaluation
0
==
0U
Unsigned
1
-1
<
0
Signed
1
-1
<
0U
Unsigned
0 *
2147483647
>
-2147483647-1
Signed
1
2147483647U >
-2147483647-1
Unsigned
0 *
2147483647
>
(int) 2147483648U
Signed
1 *
-1
>
-2
Signed
1
(unsigned) -1
>
-2
Unsigned
1
Figure 2.19
Effects of C promotion rules. Nonintuitive cases are marked by ‘*’. When


正如我们将看到的，这个约定对标准算术运算几乎没有区别，
但它会导致关系运算符如 < 和 > 的不直观结果。
图 2.19 显示了一些示例关系表达式及其结果求值，当数据类型 int 有 32 位补码表示时。

32-bit two’s-complement representation. Consider the comparison -1 < 0U. Since
第一个被隐式转换为无符号，因此表达式等价于比较 4294967295U < 0U
（回忆 $T2U_w(-1) = UMax_w$），当然是假的。其他情况可以通过类似分析理解。


**练习题 2.21**（答案见第 185 页）
假设在使用补码算术的机器上执行 32 位程序时对这些表达式求值，
chine that uses two’s-complement arithmetic, ﬁll in the following table describing

Expression
Type
Evaluation
-2147483647-1 == 2147483648U
-2147483647-1 < 2147483647
-2147483647-1U < 2147483647
-2147483647-1 < -2147483647
-2147483647-1U < -2147483647
### 2.2.6 扩展数字的位表示

一个常见的操作是在保持相同数值的情况下在不同字长的整数之间转换。
当然，当目标数据类型太小而无法表示所需值时，这可能不可行。
然而，从较小的数据类型转换到较大的数据类型应该总是可能的。


> **Web 旁注 DATA:TMIN - 在 C 中写 TMin**
>
> 在图 2.19 和习题 2.21 中，我们仔细地将 TMin₃₂ 的值写成 -2,147,483,647-1。为什么不直接写成 -2,147,483,648 或 0x80000000 呢？查看 C 头文件 limits.h，我们发现它们使用与我们类似的方法来写 TMin₃₂ 和 TMax₃₂：
>
> ```c
> /* signed int 能容纳的最小值和最大值 */
> #define INT_MAX 2147483647
> #define INT_MIN (-INT_MAX - 1)
> ```
>
> 不幸的是，补码表示的不对称性和 C 的转换规则之间存在一种奇特的交互作用，迫使我们以这种不寻常的方式写 TMin₃₂。虽然理解这个问题需要我们深入 C 语言标准的一个更模糊的角落，但它将帮助我们理解整数数据类型和表示的一些微妙之处。
要将无符号数转换为更大的数据类型，我们可以简单地在表示中添加前导零；
这个操作称为**零扩展（zero extension）**，由以下原理表达：

**原理：通过零扩展扩展无符号数**
定义宽度为 w 的位向量 $\vec{u} = [u_{w-1}, u_{w-2}, \ldots, u_0]$ 和
宽度为 w' 的位向量 $\vec{u}' = [0, \ldots, 0, u_{w-1}, u_{w-2}, \ldots, u_0]$，其中 w' > w。则 $B2U_w(\vec{u}) = B2U_{w'}(\vec{u}')$。
这个原理可以直接从方程 2.1 给出的无符号编码定义得出。

要将补码数转换为更大的数据类型，规则是将最高有效位的副本添加到表示中，由以下原理表达。我们用蓝色显示符号位 $x_{w-1}$ 来突出其在符号扩展中的作用。

**原理：通过符号扩展扩展补码数**

定义宽度为 w 的位向量 $\vec{x} = [x_{w-1}, x_{w-2}, \ldots, x_0]$ 和宽度为 w' 的位向量 $\vec{x}' = [x_{w-1}, \ldots, x_{w-1}, x_{w-1}, x_{w-2}, \ldots, x_0]$，其中 w' > w。则 $B2T_w(\vec{x}) = B2T_{w'}(\vec{x}')$。
例如，考虑以下代码：

```c
short sx = -12345;          /* -12345 */
unsigned short usx = sx;    /* 53191 */
int x = sx;                 /* -12345 */
unsigned ux = usx;          /* 53191 */

printf("sx  = %d:\t", sx);
show_bytes((byte_pointer) &sx, sizeof(short));
printf("usx = %u:\t", usx);
show_bytes((byte_pointer) &usx, sizeof(unsigned short));
printf("x   = %d:\t", x);
show_bytes((byte_pointer) &x, sizeof(int));
printf("ux  = %u:\t", ux);
show_bytes((byte_pointer) &ux, sizeof(unsigned));
```

当在使用补码的大端机器上作为 32 位程序运行时，产生以下输出：

```
sx  = -12345:   cf c7
usx = 53191:    cf c7
x   = -12345:   ff ff cf c7
ux  = 53191:    00 00 cf c7
```

我们看到，虽然 −12,345 和 53,191 的补码和无符号表示对于 16 位字长是相同的，但对于 32 位字长它们不同。特别是，-12,345 的十六进制表示是 0xFFFFCFC7，而 53,191 的十六进制表示是 0x0000CFC7。前者是符号扩展的——最高有效位 1 的 16 个副本（十六进制表示为 0xFFFF）被添加为前导位。后者用 16 个前导零扩展（十六进制表示为 0x0000）。

作为示例，图 2.20 显示了通过符号扩展从字长 w = 3 扩展到 w = 4 的结果。位向量 [101] 表示值 −4 + 1 = −3。应用符号扩展得到位向量 [1101]，表示值 −8 + 4 + 1 = −3。我们可以看到，对于 w = 4，两个最高有效位的组合值 −8 + 4 = −4，与 w = 3 时符号位的值相匹配。类似地，位向量 [111] 和 [1111] 都表示值 −1。

有了这个直觉，我们现在可以证明符号扩展保持补码数的值。

**图 2.20 从 w = 3 到 w = 4 的符号扩展示例。对于 w = 4，高 2 位的组合权重是 −8 + 4 = −4，与 w = 3 时符号位的权重相匹配。**

<!-- 数据表保持原样 -->
| | 8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0 | –1 | –2 | –3 | –4 | –5 | –6 | –7 | –8 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| $2^0 = 1$ | | | | | | | | | | | | | | | | | |
| $2^1 = 2$ | | | | | | | | | | | | | | | | | |
| $2^2 = 4$ | | | | | | | | | | | | | | | | | |
| $-2^3 = -8$ | | | | | | | | | | | | | | | | | |
| [101] | | | | | | | | | | | | ● | | | | | |
| [1101] | | | | | | | | | | | | ● | | | | | |
| [111] | | | | | | | | | | ● | | | | | | | |
| [1111] | | | | | | | | | | ● | | | | | | | |
| $-2^2 = -4$ | | | | | | | | | | | | | | | | | |

**推导：通过符号扩展扩展补码数**

设 w' = w + k。我们要证明的是：

$$B2T_{w+k}([\underbrace{x_{w-1}, \ldots, x_{w-1}}_{k \text{ 次}}, x_{w-1}, x_{w-2}, \ldots, x_0]) = B2T_w([x_{w-1}, x_{w-2}, \ldots, x_0])$$

证明通过对 k 进行归纳。也就是说，如果我们能证明符号扩展 1 位保持数值不变，那么符号扩展任意位数时这个性质也成立。因此，任务简化为证明：

$$B2T_{w+1}([x_{w-1}, x_{w-1}, x_{w-2}, \ldots, x_0]) = B2T_w([x_{w-1}, x_{w-2}, \ldots, x_0])$$

用公式 2.3 展开左边的表达式得到：

$$\begin{aligned}B2T_{w+1}([x_{w-1}, x_{w-1}, x_{w-2}, \ldots, x_0]) &= -x_{w-1} \cdot 2^w + x_{w-1} \cdot 2^{w-1} + \sum_{i=0}^{w-2} x_i \cdot 2^i \\
&= -x_{w-1}(2^w - 2^{w-1}) + \sum_{i=0}^{w-2} x_i \cdot 2^i \\
&= -x_{w-1} \cdot 2^{w-1} + \sum_{i=0}^{w-2} x_i \cdot 2^i \\
&= B2T_w([x_{w-1}, x_{w-2}, \ldots, x_0])
\end{aligned}$$

我们利用的关键性质是 $2^w - 2^{w-1} = 2^{w-1}$。因此，添加一个权重为 $-2^w$ 的位，并将权重为 $-2^{w-1}$ 的位转换为权重为 $2^{w-1}$ 的位，其组合效果是保持原始数值不变。
**练习题 2.22**（答案见第 186 页）

通过应用公式 2.3，证明以下每个位向量都是 −4 的补码表示：

- A. [1100]
- B. [11100]
- C. [111100]

观察第二个和第三个位向量可以通过符号扩展从第一个推导出来。

值得指出的一点是，从一种数据大小转换到另一种数据大小以及在有符号和无符号之间转换的相对顺序可能会影响程序的行为。例如，考虑以下代码：

```c
short sx = -12345;      /* -12345 */
unsigned uy = sx;       /* 神秘值！ */

printf("uy  = %u:\t", uy);
show_bytes((byte_pointer) &uy, sizeof(unsigned));
```

当在大端机器上运行时，这段代码会打印以下输出：

```
uy  = 4294954951:    ff ff cf c7
```

这表明，当从 short 转换到 unsigned 时，程序首先改变大小，然后改变类型。也就是说，`(unsigned) sx` 等价于 `(unsigned) (int) sx`，计算结果为 4,294,954,951，而不是 `(unsigned) (unsigned short) sx`，后者计算结果为 53,191。事实上，C 标准要求采用这种约定。
**练习题 2.23**（答案见第 186 页）

考虑以下 C 函数：

```c
int fun1(unsigned word) {
    return (int) ((word << 24) >> 24);
}

int fun2(unsigned word) {
    return ((int) word << 24) >> 24;
}
```

假设这些函数在使用补码运算的机器上作为 32 位程序执行。还假设有符号值的右移是算术移位，而无符号值的右移是逻辑移位。

A. 填写下表，显示这些函数对几个示例参数的效果。使用十六进制表示会更方便。只需记住十六进制数字 8 到 F 的最高有效位等于 1。

| w | fun1(w) | fun2(w) |
|---|---------|---------|
| 0x00000076 | | |
| 0x87654321 | | |
| 0x000000C9 | | |
| 0xEDCBA987 | | |

B. 用文字描述这两个函数各自执行的有用计算。

### 2.2.7 截断数字

假设我们不是用额外的位扩展一个值，而是减少表示数字的位数。例如，考虑以下代码：

```c
int x = 53191;
short sx = (short) x;    /* -12345 */
int y = sx;              /* -12345 */
```

将 x 强制转换为 short 会将 32 位 int 截断为 16 位 short。正如我们之前所见，这个 16 位模式是 −12,345 的补码表示。当将其转换回 int 时，符号扩展会将高 16 位设置为 1，产生 −12,345 的 32 位补码表示。
当将 w 位数字 $\vec{x} = [x_{w-1}, x_{w-2}, \ldots, x_0]$ 截断为 k 位数字时，我们丢弃高 w−k 位，得到位向量 $\vec{x}' = [x_{k-1}, x_{k-2}, \ldots, x_0]$。截断数字可能改变其值——这是一种溢出形式。对于无符号数，我们可以很容易地描述结果的数值。

**原理：无符号数的截断**

设 $\vec{x}$ 为位向量 $[x_{w-1}, x_{w-2}, \ldots, x_0]$，$\vec{x}'$ 为将其截断到 k 位的结果：$\vec{x}' = [x_{k-1}, x_{k-2}, \ldots, x_0]$。设 $x = B2U_w(\vec{x})$，$x' = B2U_k(\vec{x}')$。则 $x' = x \mod 2^k$。

这个原理背后的直觉很简单：所有被截断的位的权重形式为 $2^i$（其中 $i \geq k$），因此在取模运算下，这些权重都变为零。这由以下推导形式化：

**推导：无符号数的截断**

对公式 2.1 应用取模运算得到：

$$B2U_w([x_{w-1}, x_{w-2}, \ldots, x_0]) \mod 2^k = \sum_{i=0}^{w-1} x_i \cdot 2^i \mod 2^k = \sum_{i=0}^{k-1} x_i \cdot 2^i = B2U_k([x_{k-1}, x_{k-2}, \ldots, x_0])$$

在这个推导中，我们利用了性质：对于任意 $i \geq k$，$2^i \mod 2^k = 0$。
A similar property holds for truncating a two’s-complement number, except
that it then converts the most signiﬁcant bit into a sign bit:

principle: Truncation of a two’s-complement number
Let ⃗x be the bit vector [xw−1, xw−2, . . . , x0], and let ⃗
′x be the result of truncating
it to k bits: ⃗x′ = [xk−1, xk−2, . . . , x0]. Let x = B2Tw(⃗x) and x′ = B2Tk(⃗x′). Then
x′ = U2Tk(x mod 2k).
In this formulation, x mod 2k will be a number between 0 and 2k −1. Applying
function U2Tk to it will have the effect of converting the most signiﬁcant bit xk−1
from having weight 2k−1 to having weight −2k−1. We can see this with the example
of converting value x = 53,191from int to short. Since 216 = 65,536 ≥x, we have
x mod 216 = x. But when we convert this number to a 16-bit two’s-complement
number, we get x′ = 53,191 −65,536 = −12,345.
derivation: Truncation of a two’s-complement number
Using a similar argument to the one we used for truncation of an unsigned number
shows that
B2Tw([xw−1, xw−2, . . . , x0]) mod 2k = B2Uk([xk−1, xk−2, . . . , x0])
That is, x mod 2k can be represented by an unsigned number having bit-level rep-
resentation [xk−1, xk−2, . . . , x0]. Converting this to a two’s-complement number
gives x′ = U2Tk(x mod 2k).
Summarizing, the effect of truncation for unsigned numbers is
B2Uk([xk−1, xk−2, . . . , x0]) = B2Uw([xw−1, xw−2, . . . , x0]) mod 2k
(2.9)
while the effect for two’s-complement numbers is
B2Tk([xk−1, xk−2, . . . , x0]) = U2Tk(B2Uw([xw−1, xw−2, . . . , x0]) mod 2k) (2.10)
Practice Problem 2.24 (solution page 186)
Suppose we truncate a 4-bit value (represented by hex digits 0 through F) to a 3-
bit value (represented as hex digits 0 through 7.) Fill in the table below showing
the effect of this truncation for some cases, in terms of the unsigned and two’s-
complement interpretations of those bit patterns.
Hex
Unsigned
Two’s complement
Original
Truncated
Original
Truncated
Original
Truncated
1
1
1
1
3
3
3
3
5
5
5
5
C
4
12
−4
E
6
14
−2
Explain how Equations 2.9 and 2.10 apply to these cases.

2.2.8
Advice on Signed versus Unsigned
As we have seen, the implicit casting of signed to unsigned leads to some non-
intuitive behavior. Nonintuitive features often lead to program bugs, and ones
involving the nuances of implicit casting can be especially difﬁcult to see. Since the
casting takes place without any clear indication in the code, programmers often
overlook its effects.
The following two practice problems illustrate some of the subtle errors that
can arise due to implicit casting and the unsigned data type.
Practice Problem 2.25 (solution page 187)
Consider the following code that attempts to sum the elements of an array a, where
the number of elements is given by parameter length:
1
/* WARNING: This is buggy code */
2
float sum_elements(float a[], unsigned length) {
3
int i;
4
float result = 0;
5
6
for (i = 0; i <= length-1; i++)
7
result += a[i];
8
return result;
9
}
When run with argument length equal to 0, this code should return 0.0.
Instead, it encounters a memory error. Explain why this happens. Show how this
code can be corrected.
Practice Problem 2.26 (solution page 187)
You are given the assignment of writing a function that determines whether one
string is longer than another. You decide to make use of the string library function
strlen having the following declaration:
/* Prototype for library function strlen */
size_t strlen(const char *s);
Here is your ﬁrst attempt at the function:
/* Determine whether string s is longer than string t */
/* WARNING: This function is buggy */
int strlonger(char *s, char *t) {
return strlen(s) - strlen(t) > 0;
}
When you test this on some sample data, things do not seem to work quite
right. You investigate further and determine that, when compiled as a 32-bit

program, data type size_t is deﬁned (via typedef) in header ﬁle stdio.h to be
unsigned.
A. For what cases will this function produce an incorrect result?
B. Explain how this incorrect result comes about.
C. Show how to ﬁx the code so that it will work reliably.
We have seen multiple ways in which the subtle features of unsigned arith-
metic, and especially the implicit conversion of signed to unsigned, can lead to
errors or vulnerabilities. One way to avoid such bugs is to never use unsigned
numbers. In fact, few languages other than C support unsigned integers. Appar-
ently, these other language designers viewed them as more trouble than they are
worth. For example, Java supports only signed integers, and it requires that they
be implemented with two’s-complement arithmetic. The normal right shift oper-
ator >> is guaranteed to perform an arithmetic shift. The special operator >>> is
deﬁned to perform a logical right shift.
Unsigned values are very useful when we want to think of words as just col-
lections of bits with no numeric interpretation. This occurs, for example, when
packing a word with ﬂags describing various Boolean conditions. Addresses are
naturally unsigned, so systems programmers ﬁnd unsigned types to be helpful.
Unsigned values are also useful when implementing mathematical packages for
modular arithmetic and for multiprecision arithmetic, in which numbers are rep-
resented by arrays of words.
2.3
Integer Arithmetic
Many beginning programmers are surprised to ﬁnd that adding two positive num-
bers can yield a negative result, and that the comparison x < y can yield a different
result than the comparison x-y < 0. These properties are artifacts of the ﬁnite na-
ture of computer arithmetic. Understanding the nuances of computer arithmetic
can help programmers write more reliable code.
2.3.1
Unsigned Addition
Consider two nonnegative integers x and y, such that 0 ≤x, y < 2w. Each of
these values can be represented by a w-bit unsigned number. If we compute their
sum, however, we have a possible range 0 ≤x + y ≤2w+1 −2. Representing this
sum could require w + 1 bits. For example, Figure 2.21 shows a plot of the func-
tion x + y when x and y have 4-bit representations. The arguments (shown on
the horizontal axes) range from 0 to 15, but the sum ranges from 0 to 30. The
shape of the function is a sloping plane (the function is linear in both dimen-
sions). If we were to maintain the sum as a (w + 1)-bit number and add it to
another value, we may require w + 2 bits, and so on. This continued “word size

32
28
24
20
16
12
8
4
0
2
0
4
6
8
10
12
14
0
2
4
6
8
10
12
14
Figure 2.21
Integer addition. With a 4-bit word size, the sum could require 5 bits.
inﬂation” means we cannot place any bound on the word size required to fully rep-
resent the results of arithmetic operations. Some programming languages, such
as Lisp, actually support arbitrary size arithmetic to allow integers of any size
(within the memory limits of the computer, of course.) More commonly, pro-
gramming languages support ﬁxed-size arithmetic, and hence operations such
as “addition” and “multiplication” differ from their counterpart operations over
integers.
Let us deﬁne the operation +u
w for arguments x and y, where 0 ≤x, y < 2w,
as the result of truncating the integer sum x + y to be w bits long and then
viewing the result as an unsigned number. This can be characterized as a form
of modular arithmetic, computing the sum modulo 2w by simply discarding any
bits with weight greater than 2w−1 in the bit-level representation of x + y. For
example, consider a 4-bit number representation with x = 9 and y = 12, having
bit representations [1001] and [1100], respectively. Their sum is 21, having a 5-bit
representation [10101]. But if we discard the high-order bit, we get [0101], that is,
decimal value 5. This matches the value 21 mod 16 = 5.

Aside
Security vulnerability in getpeername
In 2002, programmers involved in the FreeBSD open-source operating systems project realized that
their implementation of the getpeername library function had a security vulnerability. A simpliﬁed
version of their code went something like this:
1
/*
2
* Illustration of code vulnerability similar to that found in
3
* FreeBSD’s implementation of getpeername()
4
*/
5
6
/* Declaration of library function memcpy */
7
void *memcpy(void *dest, void *src, size_t n);
8
9
/* Kernel memory region holding user-accessible data */
10
#define KSIZE 1024
11

```c
char kbuf[KSIZE];
```

12
13
/* Copy at most maxlen bytes from kernel region to user buffer */
14
int copy_from_kernel(void *user_dest, int maxlen) {
15
/* Byte count len is minimum of buffer size and maxlen */
16
int len = KSIZE < maxlen ? KSIZE : maxlen;
17
memcpy(user_dest, kbuf, len);
18
return len;
19
}
In this code, we show the prototype for library function memcpy on line 7, which is designed to copy
a speciﬁed number of bytes n from one region of memory to another.
The function copy_from_kernel, starting at line 14, is designed to copy some of the data main-
tained by the operating system kernel to a designated region of memory accessible to the user. Most
of the data structures maintained by the kernel should not be readable by a user, since they may con-
tain sensitive information about other users and about other jobs running on the system, but the region
shown as kbuf was intended to be one that the user could read. The parameter maxlen is intended to be
the length of the buffer allocated by the user and indicated by argument user_dest. The computation
at line 16 then makes sure that no more bytes are copied than are available in either the source or the
destination buffer.
Suppose, however, that some malicious programmer writes code that calls copy_from_kernel with
a negative value of maxlen. Then the minimum computation on line 16 will compute this value for len,
which will then be passed as the parameter n to memcpy. Note, however, that parameter n is declared as
having data type size_t. This data type is declared (via typedef) in the library ﬁle stdio.h. Typically, it
is deﬁned to be unsigned for 32-bit programs and unsigned long for 64-bit programs. Since argument
n is unsigned, memcpy will treat it as a very large positive number and attempt to copy that many bytes
from the kernel region to the user’s buffer. Copying that many bytes (at least 231) will not actually
work, because the program will encounter invalid addresses in the process, but the program could read
regions of the kernel memory for which it is not authorized.

Aside
Security vulnerability in getpeername (continued)
We can see that this problem arises due to the mismatch between data types: in one place the
length parameter is signed; in another place it is unsigned. Such mismatches can be a source of bugs
and, as this example shows, can even lead to security vulnerabilities. Fortunately, there were no reported
cases where a programmer had exploited the vulnerability in FreeBSD. They issued a security advisory
“FreeBSD-SA-02:38.signed-error” advising system administrators on how to apply a patch that would
remove the vulnerability. The bug can be ﬁxed by declaring parameter maxlen to copy_from_kernel
to be of type size_t, to be consistent with parameter n of memcpy. We should also declare local variable
len and the return value to be of type size_t.
We can characterize operation +u
w as follows:
principle: Unsigned addition
For x and y such that 0 ≤x, y < 2w:
x +u
w y =
 x + y,
x + y < 2w
Normal
x + y −2w,
2w ≤x + y < 2w+1
Overﬂow
(2.11)
The two cases of Equation 2.11 are illustrated in Figure 2.22, showing the
sum x + y on the left mapping to the unsigned w-bit sum x +u
w y on the right. The
normal case preserves the value of x + y, while the overﬂow case has the effect of
decrementing this sum by 2w.
derivation: Unsigned addition
In general, we can see that if x + y < 2w, the leading bit in the (w + 1)-bit represen-
tation of the sum will equal 0, and hence discarding it will not change the numeric
value. On the other hand, if 2w ≤x + y < 2w+1, the leading bit in the (w + 1)-bit
representation of the sum will equal 1, and hence discarding it is equivalent to
subtracting 2w from the sum.
An arithmetic operation is said to overﬂow when the full integer result cannot
ﬁt within the word size limits of the data type. As Equation 2.11 indicates, overﬂow
2w
0
2w+1
Overflow
Normal
x +uy
x + y
Figure 2.22
Relation between integer addition and unsigned addition. When x + y
is greater than 2w −1, the sum overﬂows.

16
14
12
10
8
6
4
2
0
Overflow
Normal
0
2
4
6
8
10
12
14
0
2
4
6
8
10
12
14
Figure 2.23
Unsigned addition. With a 4-bit word size, addition is performed
modulo 16.
occurs when the two operands sum to 2w or more. Figure 2.23 shows a plot of the
unsigned addition function for word size w = 4. The sum is computed modulo
24 = 16. When x + y < 16, there is no overﬂow, and x +u
4 y is simply x + y. This is
shown as the region forming a sloping plane labeled “Normal.” When x + y ≥16,
the addition overﬂows, having the effect of decrementing the sum by 16. This is
shown as the region forming a sloping plane labeled “Overﬂow.”
When executing C programs, overﬂows are not signaled as errors. At times,
however, we might wish to determine whether or not overﬂow has occurred.
principle: Detecting overﬂow of unsigned addition
For x and y in the range 0 ≤x, y ≤UMaxw, let s .= x +u
w y. Then the computation
of s overﬂowed if and only if s < x (or equivalently, s < y).
As an illustration, in our earlier example, we saw that 9 +u
4 12 = 5. We can see
that overﬂow occurred, since 5 < 9.

derivation: Detecting overﬂow of unsigned addition
Observe that x + y ≥x, and hence if s did not overﬂow, we will surely have s ≥x.
On the other hand, if s did overﬂow, we have s = x + y −2w. Given that y < 2w,
we have y −2w < 0, and hence s = x + (y −2w) < x.
Practice Problem 2.27 (solution page 188)
Write a function with the following prototype:
/* Determine whether arguments can be added without overflow */
int uadd_ok(unsigned x, unsigned y);
This function should return 1 if arguments x and y can be added without
causing overﬂow.
Modular addition forms a mathematical structure known as an abelian group,
named after the Norwegian mathematician Niels Henrik Abel (1802–1829). That
is, it is commutative (that’s where the “abelian” part comes in) and associative;
it has an identity element 0, and every element has an additive inverse. Let us
consider the set of w-bit unsigned numbers with addition operation +u
w. For every
value x, there must be some value -u
w x such that -u
w x +u
w x = 0. This additive
inverse operation can be characterized as follows:
principle: Unsigned negation
For any number x such that 0 ≤x < 2w, its w-bit unsigned negation -u
w x is given
by the following:
-u
w x =
 x,
x = 0
2w −x,
x > 0
(2.12)
This result can readily be derived by case analysis:
derivation: Unsigned negation
When x = 0, the additive inverse is clearly 0. For x > 0, consider the value 2w −x.
Observe that this number is in the range 0 < 2w −x < 2w. We can also see that
(x + 2w −x) mod 2w = 2w mod 2w = 0. Hence it is the inverse of x under +u
w.
Practice Problem 2.28 (solution page 188)
We can represent a bit pattern of length w = 4 with a single hex digit. For an
unsigned interpretation of these digits, use Equation 2.12 to ﬁll in the following
table giving the values and the bit representations (in hex) of the unsigned additive
inverses of the digits shown.

x
-u
4 x
Hex
Decimal
Decimal
Hex
1
4
7
A
E
2.3.2
Two’s-Complement Addition
With two’s-complement addition, we must decide what to do when the result is
either too large (positive) or too small (negative) to represent. Given integer
values x and y in the range −2w−1 ≤x, y ≤2w−1 −1, their sum is in the range
−2w ≤x + y ≤2w −2, potentially requiring w + 1 bits to represent exactly. As
before, we avoid ever-expanding data sizes by truncating the representation to w
bits. The result is not as familiar mathematically as modular addition, however.
Let us deﬁne x +t
w y to be the result of truncating the integer sum x + y to be w
bits long and then viewing the result as a two’s-complement number.
principle: Two’s-complement addition
For integer values x and y in the range −2w−1 ≤x, y ≤2w−1 −1:
x +t
w y =
⎧
⎪⎨
⎪⎩
x + y −2w,
2w−1 ≤x + y
Positive overﬂow
x + y,
−2w−1 ≤x + y < 2
Normal
x + y + 2w,
x + y < −2
Negative overﬂow
(2.13)
This principle is illustrated in Figure 2.24, where the sum x + y is shown on the
left, having a value in the range −2w ≤x + y ≤2w −2, and the result of truncating
the sum to a w-bit two’s-complement number is shown on the right. (The labels
“Case 1” to “Case 4” in this ﬁgure are for the case analysis of the formal derivation
of the principle.) When the sum x + y exceeds TMaxw (case 4), we say that positive
overﬂow has occurred. In this case, the effect of truncation is to subtract 2w from
the sum. When the sum x + y is less than TMinw (case 1), we say that negative
overﬂow has occurred. In this case, the effect of truncation is to add 2w to the sum.
The w-bit two’s-complement sum of two numbers has the exact same bit-level
representation as the unsigned sum. In fact, most computers use the same machine
instruction to perform either unsigned or signed addition.
derivation: Two’s-complement addition
Since two’s-complement addition has the exact same bit-level representation as
unsigned addition, we can characterize the operation +t
w as one of converting its
arguments to unsigned, performing unsigned addition, and then converting back
to two’s complement:

Figure 2.24
Relation between integer
and two’s-complement
addition. When x + y is
less than −2w−1, there is a
negative overﬂow. When
it is greater than or equal
to 2w−1, there is a positive
overﬂow.
+2w
–2w
0
0
+2w–1
+2w–1
–2w–1
–2w–1
Negative overflow
Positive overflow
Case 4
Case 3
Case 2
Case 1
Normal
x +ty
x + y
x +t
w y = U2Tw(T2Uw(x) +u
w T2Uw(y))
(2.14)
By Equation 2.6, we can write T2Uw(x) as xw−12w + x and T2Uw(y) as
yw−12w + y. Using the property that +u
w is simply addition modulo 2w, along with
the properties of modular addition, we then have
x +t
w y = U2Tw(T2Uw(x) +u
w T2Uw(y))
= U2Tw[(xw−12w + x + yw−12w + y) mod 2w]
= U2Tw[(x + y) mod 2w]
The terms xw−12w and yw−12w drop out since they equal 0 modulo 2w.
To better understand this quantity, let us deﬁne z as the integer sum z .= x + y,
z′ as z′ .= z mod 2w, and z′′ as z′′ .= U2Tw(z′). The value z′′ is equal to x +t
w y. We
can divide the analysis into four cases as illustrated in Figure 2.24:
1. −2w ≤z < −2w−1. Then we will have z′ = z + 2w. This gives 0 ≤z′ < −2w−1 +
2w = 2w−1. Examining Equation 2.7, we see that z′ is in the range such that
z′′ = z′. This is the case of negative overﬂow. We have added two negative
numbers x and y (that’s the only way we can have z < −2w−1) and obtained
a nonnegative result z′′ = x + y + 2w.
2. −2w−1 ≤z < 0. Then we will again have z′ = z + 2w, giving −2w−1 + 2w =
2w−1 ≤z′ < 2w. Examining Equation 2.7, we see that z′ is in such a range that
z′′ = z′ −2w, and therefore z′′ = z′ −2w = z + 2w −2w = z. That is, our two’s-
complement sum z′′ equals the integer sum x + y.
3. 0 ≤z < 2w−1. Then we will have z′ = z, giving 0 ≤z′ < 2w−1, and hence z′′ =
z′ = z. Again, the two’s-complement sum z′′ equals the integer sum x + y.
4. 2w−1 ≤z < 2w. We will again have z′ = z, giving 2w−1 ≤z′ < 2w. But in this
range we have z′′ = z′ −2w, giving z′′ = x + y −2w. This is the case of positive
overﬂow. We have added two positive numbers x and y (that’s the only way
we can have z ≥2w−1) and obtained a negative result z′′ = x + y −2w.

x
y
x + y
x +t
4 y
Case
−8
−5
−13
3
1
[1000]
[1011]
[10011]
[0011]
−8
−8
−16
0
1
[1000]
[1000]
[10000]
[0000]
−8
5
−3
−3
2
[1000]
[0101]
[11101]
[1101]
2
5
7
7
3
[0010]
[0101]
[00111]
[0111]
5
5
10
−6
4
[0101]
[0101]
[01010]
[1010]
Figure 2.25
Two’s-complement addition examples. The bit-level representation of
the 4-bit two’s-complement sum can be obtained by performing binary addition of the
operands and truncating the result to 4 bits.
As illustrations of two’s-complement addition, Figure 2.25 shows some exam-
ples when w = 4. Each example is labeled by the case to which it corresponds in
the derivation of Equation 2.13. Note that 24 = 16, and hence negative overﬂow
yields a result 16 more than the integer sum, and positive overﬂow yields a result 16
less. We include bit-level representations of the operands and the result. Observe
that the result can be obtained by performing binary addition of the operands and
truncating the result to 4 bits.
Figure 2.26 illustrates two’s-complement addition for word size w = 4. The
operands range between −8 and 7. When x + y < −8, two’s-complement addition
has a negative overﬂow, causing the sum to be incremented by 16. When −8 ≤
x + y < 8, the addition yields x + y. When x + y ≥8, the addition has a positive
overﬂow, causing the sum to be decremented by 16. Each of these three ranges
forms a sloping plane in the ﬁgure.
Equation 2.13 also lets us identify the cases where overﬂow has occurred:
principle: Detecting overﬂow in two’s-complement addition
For x and y in the range TMinw ≤x, y ≤TMaxw, let s .= x +t
w y. Then the compu-
tation of s has had positive overﬂow if and only if x > 0 and y > 0 but s ≤0. The
computation has had negative overﬂow if and only if x < 0 and y < 0 but s ≥0.
Figure 2.25 shows several illustrations of this principle for w = 4. The ﬁrst
entry shows a case of negative overﬂow, where two negative numbers sum to a
positive one. The ﬁnal entry shows a case of positive overﬂow, where two positive
numbers sum to a negative one.

Normal
Negative
overflow
Positive
overflow
8
6
4
2
0
22
24
26
28
28
28
26
22
24
0
2
4
6
26
24
22
0
2
4
6
Figure 2.26
Two’s-complement addition. With a 4-bit word size, addition can have a
negative overﬂow when x + y < −8 and a positive overﬂow when x + y ≥8.
derivation: Detecting overﬂow of two’s-complement addition
Let us ﬁrst do the analysis for positive overﬂow. If both x > 0 and y > 0 but s ≤0,
then clearly positive overﬂow has occurred. Conversely, positive overﬂow requires
(1) that x > 0 and y > 0 (otherwise, x + y < TMaxw) and (2) that s ≤0 (from
Equation 2.13). A similar set of arguments holds for negative overﬂow.
Practice Problem 2.29 (solution page 188)
Fill in the following table in the style of Figure 2.25. Give the integer values of
the 5-bit arguments, the values of both their integer and two’s-complement sums,
the bit-level representation of the two’s-complement sum, and the case from the
derivation of Equation 2.13.
x
y
x + y
x +t
5 y
Case
[10100]
[10001]

x
y
x + y
x +t
5 y
Case
[11000]
[11000]
[10111]
[01000]
[00010]
[00101]
[01100]
[00100]
Practice Problem 2.30 (solution page 189)
Write a function with the following prototype:
/* Determine whether arguments can be added without overflow */
int tadd_ok(int x, int y);
This function should return 1 if arguments x and y can be added without
causing overﬂow.
Practice Problem 2.31 (solution page 189)
Your coworker gets impatient with your analysis of the overﬂow conditions for
two’s-complement addition and presents you with the following implementation
of tadd_ok:
/* Determine whether arguments can be added without overflow */
/* WARNING: This code is buggy. */
int tadd_ok(int x, int y) {
int sum = x+y;
return (sum-x == y) && (sum-y == x);
}
You look at the code and laugh. Explain why.
Practice Problem 2.32 (solution page 189)
You are assigned the task of writing code for a function tsub_ok, with arguments
x and y, that will return 1 if computing x-y does not cause overﬂow. Having just
written the code for Problem 2.30, you write the following:
/* Determine whether arguments can be subtracted without overflow */
/* WARNING: This code is buggy. */
int tsub_ok(int x, int y) {

return tadd_ok(x, -y);
}
For what values of x and y will this function give incorrect results? Writing a
correct version of this function is left as an exercise (Problem 2.74).
2.3.3
Two’s-Complement Negation
We can see that every number x in the range TMinw ≤x ≤TMaxw has an additive
inverse under +t
w, which we denote -t
w x as follows:
principle: Two’s-complement negation
For x in the range TMinw ≤x ≤TMaxw, its two’s-complement negation -t
w x is
given by the formula
-t
w x =
 TMinw,
x = TMinw
−x,
x > TMinw
(2.15)
That is, for w-bit two’s-complement addition, TMinw is its own additive in-
verse, while any other value x has −x as its additive inverse.
derivation: Two’s-complement negation
Observe that TMinw + TMinw = −2w−1 + −2w−1 = −2w. This would cause nega-
tive overﬂow, and hence TMinw +t
w TMinw = −2w + 2w = 0. For values of x such
that x > TMinw, the value −x can also be represented as a w-bit two’s-complement
number, and their sum will be −x + x = 0.
Practice Problem 2.33 (solution page 189)
We can represent a bit pattern of length w = 4 with a single hex digit. For a two’s-
complement interpretation of these digits, ﬁll in the following table to determine
the additive inverses of the digits shown:
x
-t
4 x
Hex
Decimal
Decimal
Hex
2
3
9
B
C
What do you observe about the bit patterns generated by two’s-complement
and unsigned (Problem 2.28) negation?

Web Aside DATA:TNEG
Bit-level representation of two’s-complement negation
There are several clever ways to determine the two’s-complement negation of a value represented
at the bit level. The following two techniques are both useful, such as when one encounters the value
0xfffffffa when debugging a program, and they lend insight into the nature of the two’s-complement
representation.
One technique for performing two’s-complement negation at the bit level is to complement the bits
and then increment the result. In C, we can state that for any integer value x, computing the expressions
-x and ~x + 1 will give identical results.
Here are some examples with a 4-bit word size:
⃗x
~⃗x
incr(~⃗x)
[0101]
5
[1010]
−6
[1011]
−5
[0111]
7
[1000]
−8
[1001]
−7
[1100]
−4
[0011]
3
[0100]
4
[0000]
0
[1111]
−1
[0000]
0
[1000]
−8
[0111]
7
[1000]
−8
For our earlier example, we know that the complement of 0xf is 0x0 and the complement of 0xa
is 0x5, and so 0xfffffffa is the two’s-complement representation of −6.
A second way to perform two’s-complement negation of a number x is based on splitting the bit
vector into two parts. Let k be the position of the rightmost 1, so the bit-level representation of x has the
form [xw−1, xw−2, . . . , xk+1, 1, 0, . . . 0]. (This is possible as long as x ̸= 0.) The negation is then written
in binary form as [~xw−1, ~xw−2, . . . ~ xk+1, 1, 0, . . . , 0]. That is, we complement each bit to the left of
bit position k.
We illustrate this idea with some 4-bit numbers, where we highlight the rightmost pattern 1, 0, . . . , 0
in italics:
x
−x
[1100]
−4
[0100]
4
[1000]
−8
[1000]
−8
[0101]
5
[1011]
−5
[0111]
7
[1001]
−7
2.3.4
Unsigned Multiplication
Integers x and y in the range 0 ≤x, y ≤2w −1 can be represented as w-bit un-
signed numbers, but their product x . y can range between 0 and (2w −1)2 =
22w −2w+1 + 1. This could require as many as 2w bits to represent. Instead, un-
signed multiplication in C is deﬁned to yield the w-bit value given by the low-order
w bits of the 2w-bit integer product. Let us denote this value as x *u
w y.
Truncating an unsigned number to w bits is equivalent to computing its value
modulo 2w, giving the following:

principle: Unsigned multiplication
For x and y such that 0 ≤x, y ≤UMaxw:
x *u
w y = (x . y) mod 2w
(2.16)
2.3.5
Two’s-Complement Multiplication
Integers x and y in the range −2w−1 ≤x, y ≤2w−1 −1 can be represented as w-bit
two’s-complement numbers, but their product x . y can range between −2w−1 .
(2w−1 −1) = −22w−2 + 2w−1 and −2w−1 . −2w−1 = 22w−2. This could require as
many as 2w bits to represent in two’s-complement form. Instead, signed multi-
plication in C generally is performed by truncating the 2w-bit product to w bits.
We denote this value as x *t
w y. Truncating a two’s-complement number to w bits
is equivalent to ﬁrst computing its value modulo 2w and then converting from
unsigned to two’s complement, giving the following:
principle: Two’s-complement multiplication
For x and y such that TMinw ≤x, y ≤TMaxw:
x *t
w y = U2Tw((x . y) mod 2w)
(2.17)
We claim that the bit-level representation of the product operation is identical
for both unsigned and two’s-complement multiplication, as stated by the following
principle:
principle: Bit-level equivalence of unsigned and two’s-complement multipli-
cation
Let ⃗x and ⃗y be bit vectors of length w. Deﬁne integers x and y as the values repre-
sented by these bits in two’s-complement form: x = B2Tw(⃗x) and y = B2Tw(⃗y).
Deﬁne nonnegative integers x′ and y′ as the values represented by these bits in
unsigned form: x′ = B2Uw(⃗x) and y′ = B2Uw(⃗y). Then
T2Bw(x *t
w y) = U2Bw(x′ *u
w y′)
As illustrations, Figure 2.27 shows the results of multiplying different 3-bit
numbers. For each pair of bit-level operands, we perform both unsigned and
two’s-complement multiplication, yielding 6-bit products, and then truncate these
to 3 bits. The unsigned truncated product always equals x . y mod 8. The bit-
level representations of both truncated products are identical for both unsigned
and two’s-complement multiplication, even though the full 6-bit representations
differ.

Mode
x
y
x . y
Truncated x . y
Unsigned
5
[101]
3
[011]
15
[001111]
7
[111]
Two’s complement
−3
[101]
3
[011]
−9
[110111]
−1
[111]
Unsigned
4
[100]
7
[111]
28
[011100]
4
[100]
Two’s complement
−4
[100]
−1
[111]
4
[000100]
−4
[100]
Unsigned
3
[011]
3
[011]
9
[001001]
1
[001]
Two’s complement
3
[011]
3
[011]
9
[001001]
1
[001]
Figure 2.27
Three-bit unsigned and two’s-complement multiplication examples.
Although the bit-level representations of the full products may differ, those of the
truncated products are identical.
derivation: Bit-level equivalence of unsigned and two’s-complement multipli-
cation
From Equation 2.6, we have x′ = x + xw−12w and y′ = y + yw−12w. Computing the
product of these values modulo 2w gives the following:
(x′ . y′) mod 2w = [(x + xw−12w) . (y + yw−12w)] mod 2w
(2.18)
= [x . y + (xw−1y + yw−1x)2w + xw−1yw−122w] mod 2w
= (x . y) mod 2w
The terms with weight 2w and 22w drop out due to the modulus operator. By Equa-
tion 2.17, we have x *t
w y = U2Tw((x . y) mod 2w). We can apply the operation
T2Uw to both sides to get
T2Uw(x *t
w y) = T2Uw(U2Tw((x . y) mod 2w)) = (x . y) mod 2w
Combining this result with Equations 2.16 and 2.18 shows that T2Uw(x *t
w y) =
(x′ . y′) mod 2w = x′ *u
w y′. We can then apply U2Bw to both sides to get
U2Bw(T2Uw(x *t
w y)) = T2Bw(x *t
w y) = U2Bw(x′ *u
w y′)
Practice Problem 2.34 (solution page 189)
Fill in the following table showing the results of multiplying different 3-bit num-
bers, in the style of Figure 2.27:
Mode
x
y
x . y
Truncated x . y
Unsigned
[100]
[101]
Two’s complement
[100]
[101]
Unsigned
[010]
[111]
Two’s complement
[010]
[111]

Mode
x
y
x . y
Truncated x . y
Unsigned
[110]
[110]
Two’s complement
[110]
[110]
Practice Problem 2.35 (solution page 190)
You are given the assignment to develop code for a function tmult_ok that will
determine whether two arguments can be multiplied without causing overﬂow.
Here is your solution:
/* Determine whether arguments can be multiplied without overflow */
int tmult_ok(int x, int y) {
int p = x*y;
/* Either x is zero, or dividing p by x gives y */
return !x || p/x == y;
}
You test this code for a number of values of x and y, and it seems to work
properly. Your coworker challenges you, saying, “If I can’t use subtraction to
test whether addition has overﬂowed (see Problem 2.31), then how can you use
division to test whether multiplication has overﬂowed?”
Devise a mathematical justiﬁcation of your approach, along the following
lines. First, argue that the case x = 0 is handled correctly. Otherwise, consider
w-bit numbers x (x ̸= 0), y, p, and q, where p is the result of performing two’s-
complement multiplication on x and y, and q is the result of dividing p by x.
1. Show that x . y, the integer product of x and y, can be written in the form
x . y = p + t2w, where t ̸= 0 if and only if the computation of p overﬂows.
2. Show that p can be written in the form p = x . q + r, where |r| < |x|.
3. Show that q = y if and only if r = t = 0.
Practice Problem 2.36 (solution page 190)
For the case where data type int has 32 bits, devise a version of tmult_ok (Prob-
lem 2.35) that uses the 64-bit precision of data type int64_t, without using
division.
Practice Problem 2.37 (solution page 191)
You are given the task of patching the vulnerability in the XDR code shown in
the aside on page 136 for the case where both data types int and size_t are 32
bits. You decide to eliminate the possibility of the multiplication overﬂowing by
computing the number of bytes to allocate using data type uint64_t. You replace

Aside
Security vulnerability in the XDR library
In 2002, it was discovered that code supplied by Sun Microsystems to implement the XDR library, a
widely used facility for sharing data structures between programs, had a security vulnerability arising
from the fact that multiplication can overﬂow without any notice being given to the program.
Code similar to that containing the vulnerability is shown below:
1
/* Illustration of code vulnerability similar to that found in
2
* Sun’s XDR library.
3
*/
4
void* copy_elements(void *ele_src[], int ele_cnt, size_t ele_size) {
5
/*
6
* Allocate buffer for ele_cnt objects, each of ele_size bytes
7
* and copy from locations designated by ele_src
8
*/
9
void *result = malloc(ele_cnt * ele_size);
10
if (result == NULL)
11
/* malloc failed */
12
return NULL;
13
void *next = result;
14
int i;
15
for (i = 0; i < ele_cnt; i++) {
16
/* Copy object i to destination */
17
memcpy(next, ele_src[i], ele_size);
18
/* Move pointer to next memory region */
19
next += ele_size;
20
}
21
return result;
22
}
The function copy_elements is designed to copy ele_cnt data structures, each consisting of ele_
size bytes into a buffer allocated by the function on line 9. The number of bytes required is computed
as ele_cnt * ele_size.
Imagine, however, that a malicious programmer calls this function with ele_cnt being 1,048,577
(220 + 1) and ele_size being 4,096 (212) with the program compiled for 32 bits. Then the multiplication
on line 9 will overﬂow, causing only 4,096 bytes to be allocated, rather than the 4,294,971,392 bytes
required to hold that much data. The loop starting at line 15 will attempt to copy all of those bytes,
overrunning the end of the allocated buffer, and therefore corrupting other data structures. This could
cause the program to crash or otherwise misbehave.
The Sun code was used by almost every operating system and in such widely used programs as
Internet Explorer and the Kerberos authentication system. The Computer Emergency Response Team
(CERT), an organization run by the Carnegie Mellon Software Engineering Institute to track security
vulnerabilities and breaches, issued advisory “CA-2002-25,” and many companies rushed to patch their
code. Fortunately, there were no reported security breaches caused by this vulnerability.
A similar vulnerability existed in many implementations of the library function calloc. These
have since been patched. Unfortunately, many programmers call allocation functions, such as malloc,
using arithmetic expressions as arguments, without checking these expressions for overﬂow. Writing a
reliable version of calloc is left as an exercise (Problem 2.76).

the original call to malloc (line 9) as follows:
uint64_t asize =
ele_cnt * (uint64_t) ele_size;
void *result = malloc(asize);
Recall that the argument to malloc has type size_t.
A. Does your code provide any improvement over the original?
B. How would you change the code to eliminate the vulnerability?
2.3.6
Multiplying by Constants
Historically, the integer multiply instruction on many machines was fairly slow,
requiring 10 or more clock cycles, whereas other integer operations—such as
addition, subtraction, bit-level operations, and shifting—required only 1 clock
cycle. Even on the Intel Core i7 Haswell we use as our reference machine, integer
multiply requires 3 clock cycles. As a consequence, one important optimization
used by compilers is to attempt to replace multiplications by constant factors with
combinations of shift and addition operations. We will ﬁrst consider the case of
multiplying by a power of 2, and then we will generalize this to arbitrary constants.
principle: Multiplication by a power of 2
Let x be the unsigned integer represented by bit pattern [xw−1, xw−2, . . . , x0].
Then for any k ≥0, the w + k-bit unsigned representation of x2k is given by
[xw−1, xw−2, . . . , x0, 0, . . . , 0], where k zeros have been added to the right.
So, for example, 11 can be represented for w = 4 as [1011]. Shifting this left
by k = 2 yields the 6-bit vector [101100], which encodes the unsigned number
11 . 4 = 44.
derivation: Multiplication by a power of 2
This property can be derived using Equation 2.1:
B2Uw+k([xw−1, xw−2, . . . , x0, 0, . . . , 0]) =



xi2i+k
=





. 2k
= x2k
When shifting left by k for a ﬁxed word size, the high-order k bits are discarded,
yielding
[xw−k−1, xw−k−2, . . . , x0, 0, . . . , 0]

but this is also the case when performing multiplication on ﬁxed-size words. We
can therefore see that shifting a value left is equivalent to performing unsigned
multiplication by a power of 2:
principle: Unsigned multiplication by a power of 2
For C variables x and k with unsigned values x and k, such that 0 ≤k < w, the C
expression x << k yields the value x *u
w 2k.
Since the bit-level operation of ﬁxed-size two’s-complement arithmetic is
equivalent to that for unsigned arithmetic, we can make a similar statement about
the relationship between left shifts and multiplication by a power of 2 for two’s-
complement arithmetic:
principle: Two’s-complement multiplication by a power of 2
For C variables x and k with two’s-complement value x and unsigned value k, such
that 0 ≤k < w, the C expression x << k yields the value x *t
w 2k.
Note that multiplying by a power of 2 can cause overﬂow with either unsigned
or two’s-complement arithmetic. Our result shows that even then we will get the
same effect by shifting. Returning to our earlier example, we shifted the 4-bit
pattern [1011] (numeric value 11) left by two positions to get [101100] (numeric
value 44). Truncating this to 4 bits gives [1100] (numeric value 12 = 44 mod 16).
Given that integer multiplication is more costly than shifting and adding, many
C compilers try to remove many cases where an integer is being multiplied by a
constant with combinations of shifting, adding, and subtracting. For example, sup-
pose a program contains the expression x*14. Recognizing that 14 = 23 + 22 + 21,
the compiler can rewrite the multiplication as (x<<3) + (x<<2) + (x<<1), replac-
ing one multiplication with three shifts and two additions. The two computations
will yield the same result, regardless of whether x is unsigned or two’s comple-
ment, and even if the multiplication would cause an overﬂow. Even better, the
compiler can also use the property 14 = 24 −21 to rewrite the multiplication as
(x<<4) - (x<<1), requiring only two shifts and a subtraction.
Practice Problem 2.38 (solution page 191)
As we will see in Chapter 3, the lea instruction can perform computations of
the form (a<<k) + b, where k is either 0, 1, 2, or 3, and b is either 0 or some
program value. The compiler often uses this instruction to perform multiplications
by constant factors. For example, we can compute 3*a as (a<<1) + a.
Considering cases where b is either 0 or equal to a, and all possible values of k,
what multiples of a can be computed with a single lea instruction?
Generalizing from our example, consider the task of generating code for
the expression x * K, for some constant K. The compiler can express the binary
representation of K as an alternating sequence of zeros and ones:

[(0 . . . 0) (1 . . . 1) (0 . . . 0) . . . (1 . . . 1)]
For example, 14 can be written as [(0 . . . 0)(111)(0)]. Consider a run of ones from
bit position n down to bit position m (n ≥m). (For the case of 14, we have n = 3
and m = 1.) We can compute the effect of these bits on the product using either of
two different forms:
Form A: (x<<n) + (x<<(n −1)) + . . . + (x<<m)
Form B: (x<<(n + 1)) - (x<<m)
By adding together the results for each run, we are able to compute x * K with-
out any multiplications. Of course, the trade-off between using combinations of
shifting, adding, and subtracting versus a single multiplication instruction depends
on the relative speeds of these instructions, and these can be highly machine de-
pendent. Most compilers only perform this optimization when a small number of
shifts, adds, and subtractions sufﬁce.
Practice Problem 2.39 (solution page 192)
How could we modify the expression for form B for the case where bit position n
is the most signiﬁcant bit?
Practice Problem 2.40 (solution page 192)
For each of the following values of K, ﬁnd ways to express x * K using only the
speciﬁed number of operations, where we consider both additions and subtrac-
tions to have comparable cost. You may need to use some tricks beyond the simple
form A and B rules we have considered so far.
K
Shifts
Add/Subs
Expression
7
1
1
30
4
3
28
2
1
55
2
2
Practice Problem 2.41 (solution page 192)
For a run of ones starting at bit position n down to bit position m (n ≥m), we saw
that we can generate two forms of code, A and B. How should the compiler decide
which form to use?
2.3.7
Dividing by Powers of 2
Integer division on most machines is even slower than integer multiplication—
requiring 30 or more clock cycles. Dividing by a power of 2 can also be performed

k
>> k (binary)
Decimal
12,340/2k
0
0011000000110100
12,340
12,340.0
1
0001100000011010
6,170
6,170.0
4
0000001100000011
771
771.25
8
0000000000110000
48
48.203125
Figure 2.28
Dividing unsigned numbers by powers of 2. The examples illustrate
how performing a logical right shift by k has the same effect as dividing by 2k and then
rounding toward zero.
using shift operations, but we use a right shift rather than a left shift. The two
different right shifts—logical and arithmetic—serve this purpose for unsigned and
two’s-complement numbers, respectively.
Integer division always rounds toward zero. To deﬁne this precisely, let us
introduce some notation. For any real number a, deﬁne ⌊a⌋to be the unique
integer a′ such that a′ ≤a < a′ + 1. As examples, ⌊3.14⌋= 3, ⌊−3.14⌋= −4, and
⌊3⌋= 3. Similarly, deﬁne ⌈a⌉to be the unique integer a′ such that a′ −1 < a ≤a′.
As examples, ⌈3.14⌉= 4, ⌈−3.14⌉= −3, and ⌈3⌉= 3. For x ≥0 and y > 0, integer
division should yield ⌊x/y⌋, while for x < 0 and y > 0, it should yield ⌈x/y⌉. That
is, it should round down a positive result but round up a negative one.
The case for using shifts with unsigned arithmetic is straightforward, in part
because right shifting is guaranteed to be performed logically for unsigned values.
principle: Unsigned division by a power of 2
For C variables x and k with unsigned values x and k, such that 0 ≤k < w, the C
expression x >> k yields the value ⌊x/2k⌋.
As examples, Figure 2.28 shows the effects of performing logical right shifts
on a 16-bit representation of 12,340 to perform division by 1, 2, 16, and 256. The
zeros shifted in from the left are shown in italics. We also show the result we would
obtain if we did these divisions with real arithmetic. These examples show that the
result of shifting consistently rounds toward zero, as is the convention for integer
division.
derivation: Unsigned division by a power of 2
Let x be the unsigned integer represented by bit pattern [xw−1, xw−2, . . . , x0], and
let k be in the range 0 ≤k < w. Let x′ be the unsigned number with w −k-bit
representation [xw−1, xw−2, . . . , xk], and let x′′ be the unsigned number with k-bit
representation [xk−1, . . . , x0]. We can therefore see that x = 2kx′ + x′′, and that
0 ≤x′′ < 2k. It therefore follows that ⌊x/2k⌋= x′.
Performing a logical right shift of bit vector [xw−1, xw−2, . . . , x0] by k yields
the bit vector
[0, . . . , 0, xw−1, xw−2, . . . , xk]

k
>> k (binary)
Decimal
−12,340/2k
0
1100111111001100
−12,340
−12,340.0
1
1110011111100110
−6,170
−6,170.0
4
1111110011111100
−772
−771.25
8
1111111111001111
−49
−48.203125
Figure 2.29
Applying arithmetic right shift. The examples illustrate that arithmetic
right shift is similar to division by a power of 2, except that it rounds down rather than
toward zero.
This bit vector has numeric value x′, which we have seen is the value that would
result by computing the expression x >> k.
The case for dividing by a power of 2 with two’s-complement arithmetic is
slightly more complex. First, the shifting should be performed using an arithmetic
right shift, to ensure that negative values remain negative. Let us investigate what
value such a right shift would produce.
principle: Two’s-complement division by a power of 2, rounding down
Let C variables x and k have two’s-complement value x and unsigned value
k, respectively, such that 0 ≤k < w. The C expression x >> k, when the shift is
performed arithmetically, yields the value ⌊x/2k⌋.
For x ≥0, variable x has 0 as the most signiﬁcant bit, and so the effect of an
arithmetic shift is the same as for a logical right shift. Thus, an arithmetic right shift
by k is the same as division by 2k for a nonnegative number. As an example of a
negative number, Figure 2.29 shows the effect of applying arithmetic right shift to
a 16-bit representation of −12,340 for different shift amounts. For the case when
no rounding is required (k = 1), the result will be x/2k. When rounding is required,
shifting causes the result to be rounded downward. For example, the shifting right
by four has the effect of rounding −771.25 down to −772. We will need to adjust
our strategy to handle division for negative values of x.
derivation: Two’s-complement division by a power of 2, rounding down
Let x be the two’s-complement integer represented by bit pattern [xw−1, xw−2,
. . . , x0], and let k be in the range 0 ≤k < w. Let x′ be the two’s-complement
number represented by the w −k bits [xw−1, xw−2, . . . , xk], and let x′′ be the
unsigned number represented by the low-order k bits [xk−1, . . . , x0]. By a similar
analysis as the unsigned case, we have x = 2kx′ + x′′ and 0 ≤x′′ < 2k, giving x′ =
⌊x/2k⌋. Furthermore, observe that shifting bit vector [xw−1, xw−2, . . . , x0] right
arithmetically by k yields the bit vector
[xw−1, . . . , xw−1, xw−1, xw−2, . . . , xk]
which is the sign extension from w −k bits to w bits of [xw−1, xw−2, . . . , xk]. Thus,
this shifted bit vector is the two’s-complement representation of ⌊x/2k⌋.

k
Bias
−12,340 + bias (binary)
>> k (binary)
Decimal
−12,340/2k
0
0
1100111111001100
1100111111001100
−12,340
−12,340.0
1
1
1100111111001101
1110011111100110
−6,170
−6,170.0
4
15
1100111111011011
1111110011111101
−771
−771.25
8
255
1101000011001011
1111111111010000
−48
−48.203125
Figure 2.30
Dividing two’s-complement numbers by powers of 2. By adding a bias
before the right shift, the result is rounded toward zero.
We can correct for the improper rounding that occurs when a negative number
is shifted right by “biasing” the value before shifting.
principle: Two’s-complement division by a power of 2, rounding up
Let C variables x and k have two’s-complement value x and unsigned value k,
respectively, such that 0 ≤k < w. The C expression (x + (1 << k) - 1) >> k, when
the shift is performed arithmetically, yields the value ⌈x/2k⌉.
Figure 2.30 demonstrates how adding the appropriate bias before performing
the arithmetic right shift causes the result to be correctly rounded. In the third
column, we show the result of adding the bias value to −12,340, with the lower k
bits (those that will be shifted off to the right) shown in italics. We can see that
the bits to the left of these may or may not be incremented. For the case where no
rounding is required (k = 1), adding the bias only affects bits that are shifted off.
For the cases where rounding is required, adding the bias causes the upper bits to
be incremented, so that the result will be rounded toward zero.
The biasing technique exploits the property that ⌈x/y⌉= ⌊(x + y −1)/y⌋for
integers x and y such that y > 0. As examples, when x = −30 and y = 4, we have
x + y −1 = −27 and ⌈−30/4⌉= −7 = ⌊−27/4⌋. When x = −32 and y = 4, we have
x + y −1 = −29 and ⌈−32/4⌉= −8 = ⌊−29/4⌋.
derivation: Two’s-complement division by a power of 2, rounding up
To see that ⌈x/y⌉= ⌊(x + y −1)/y⌋, suppose that x = qy + r, where 0 ≤r < y,
giving (x + y −1)/y = q + (r + y −1)/y, and so ⌊(x + y −1)/y⌋= q + ⌊(r + y −
1)/y⌋. The latter term will equal 0 when r = 0 and 1 when r > 0. That is, by adding
a bias of y −1 to x and then rounding the division downward, we will get q when
y divides x and q + 1 otherwise.
Returning to the case where y = 2k, the C expression x + (1 << k) - 1 yields
the value x + 2k −1. Shifting this right arithmetically by k therefore yields ⌈x/2k⌉.
These analyses show that for a two’s-complement machine using arithmetic
right shifts, the C expression
(x<0 ? x+(1<<k)-1 : x) >> k
will compute the value x/2k.

Practice Problem 2.42 (solution page 192)
Write a function div16 that returns the value x/16 for integer argument x. Your
function should not use division, modulus, multiplication, any conditionals (if or
?:), any comparison operators (e.g., <, >, or ==), or any loops. You may assume
that data type int is 32 bits long and uses a two’s-complement representation, and
that right shifts are performed arithmetically.
We now see that division by a power of 2 can be implemented using logical or
arithmetic right shifts. This is precisely the reason the two types of right shifts are
available on most machines. Unfortunately, this approach does not generalize to
division by arbitrary constants. Unlike multiplication, we cannot express division
by arbitrary constants K in terms of division by powers of 2.
Practice Problem 2.43 (solution page 193)
In the following code, we have omitted the deﬁnitions of constants M and N:
#define M
/* Mystery number 1 */
#define N
/* Mystery number 2 */
int arith(int x, int y) {
int result = 0;
result = x*M + y/N; /* M and N are mystery numbers. */
return result;
}
We compiled this code for particular values of M and N. The compiler opti-
mized the multiplication and division using the methods we have discussed. The
following is a translation of the generated machine code back into C:
/* Translation of assembly code for arith */
int optarith(int x, int y) {
int t = x;
x <<= 5;
x -= t;
if (y < 0) y += 7;
y >>= 3;
/* Arithmetic shift */
return x+y;
}
What are the values of M and N?
2.3.8
Final Thoughts on Integer Arithmetic
As we have seen, the “integer” arithmetic performed by computers is really
a form of modular arithmetic. The ﬁnite word size used to represent numbers

limits the range of possible values, and the resulting operations can overﬂow.
We have also seen that the two’s-complement representation provides a clever
way to represent both negative and positive values, while using the same bit-level
implementations as are used to perform unsigned arithmetic—operations such as
addition, subtraction, multiplication, and even division have either identical or
very similar bit-level behaviors, whether the operands are in unsigned or two’s-
complement form.
We have seen that some of the conventions in the C language can yield some
surprising results, and these can be sources of bugs that are hard to recognize or
understand. We have especially seen that the unsigned data type, while conceptu-
ally straightforward, can lead to behaviors that even experienced programmers do
not expect. We have also seen that this data type can arise in unexpected ways—for
example, when writing integer constants and when invoking library routines.
Practice Problem 2.44 (solution page 193)
Assume data type int is 32 bits long and uses a two’s-complement representation
for signed values. Right shifts are performed arithmetically for signed values and
logically for unsigned values. The variables are declared and initialized as follows:
int x = foo();
/* Arbitrary value */
int y = bar();
/* Arbitrary value */
unsigned ux = x;
unsigned uy = y;
For each of the following C expressions, either (1) argue that it is true (evalu-
ates to 1) for all values of x and y, or (2) give values of x and y for which it is false
(evaluates to 0):
A. (x > 0) || (x-1 < 0)
B. (x & 7) != 7 || (x<<29 < 0)
C. (x * x) >= 0
D. x < 0 || -x <= 0
E. x > 0 || -x >= 0
F.
x+y == uy+ux
G. x*~y + uy*ux == -x
2.4
Floating Point
A ﬂoating-point representation encodes rational numbers of the form V = x × 2y.
It is useful for performing computations involving very large numbers (|V | ≫0),

Aside
The IEEE
The Institute of Electrical and Electronics Engineers (IEEE—pronounced “eye-triple-ee”) is a pro-
fessional society that encompasses all of electronic and computer technology. It publishes journals,
sponsors conferences, and sets up committees to deﬁne standards on topics ranging from power trans-
mission to software engineering. Another example of an IEEE standard is the 802.11 standard for
wireless networking.
numbers very close to 0 (|V | ≪1), and more generally as an approximation to real
arithmetic.
Up until the 1980s, every computer manufacturer devised its own conventions
for how ﬂoating-point numbers were represented and the details of the operations
performed on them. In addition, they often did not worry too much about the
accuracy of the operations, viewing speed and ease of implementation as being
more critical than numerical precision.
All of this changed around 1985 with the advent of IEEE Standard 754, a
carefully crafted standard for representing ﬂoating-point numbers and the oper-
ations performed on them. This effort started in 1976 under Intel’s sponsorship
with the design of the 8087, a chip that provided ﬂoating-point support for the 8086
processor. Intel hired William Kahan, a professor at the University of California,
Berkeley, as a consultant to help design a ﬂoating-point standard for its future
processors. They allowed Kahan to join forces with a committee generating an
industry-wide standard under the auspices of the Institute of Electrical and Elec-
tronics Engineers (IEEE). The committee ultimately adopted a standard close to
the one Kahan had devised for Intel. Nowadays, virtually all computers support
what has become known as IEEE ﬂoating point. This has greatly improved the
portability of scientiﬁc application programs across different machines.
In this section, we will see how numbers are represented in the IEEE ﬂoating-
point format. We will also explore issues of rounding, when a number cannot be
represented exactly in the format and hence must be adjusted upward or down-
ward. We will then explore the mathematical properties of addition, multiplica-
tion, and relational operators. Many programmers consider ﬂoating point to be
at best uninteresting and at worst arcane and incomprehensible. We will see that
since the IEEE format is based on a small and consistent set of principles, it is
really quite elegant and understandable.
2.4.1
Fractional Binary Numbers
A ﬁrst step in understanding ﬂoating-point numbers is to consider binary numbers
having fractional values. Let us ﬁrst examine the more familiar decimal notation.
Decimal notation uses a representation of the form
dm dm−1 . . . d1 d0 . d−1 d−2 . . . d−n

Figure 2.31
Fractional binary
representation. Digits
to the left of the binary
point have weights of the
form 2i, while those to the
right have weights of the
form 1/2i.
bm
bm–1 · · ·
· · ·
b2
b1
b0
b–1
1
1/2
1/4
1/8
1/2n–1
1/2n
2
4
2m–1
2m
b–2
b–3
· · ·
·
· · ·
b–n+1 b–n
where each decimal digit di ranges between 0 and 9. This notation represents a
value d deﬁned as
d =
m

i=−n
10i × di
The weighting of the digits is deﬁned relative to the decimal point symbol (‘.’),
meaning that digits to the left are weighted by nonnegative powers of 10, giving
integral values, while digits to the right are weighted by negative powers of 10,
giving fractional values. For example, 12.3410 represents the number 1 × 101 +
2 × 100 + 3 × 10−1 + 4 × 10−2 = 12 34
100.
By analogy, consider a notation of the form
bm bm−1 . . . b1 b0 . b−1 b−2 . . . b−n+1 b−n
where each binary digit, or bit, bi ranges between 0 and 1, as is illustrated in
Figure 2.31. This notation represents a number b deﬁned as
b =
m

i=−n
2i × bi
(2.19)
The symbol ‘.’ now becomes a binary point, with bits on the left being weighted
by nonnegative powers of 2, and those on the right being weighted by negative
powers of 2. For example, 101.112 represents the number 1 × 22 + 0 × 21 + 1 ×
20 + 1 × 2−1 + 1 × 2−2 = 4 + 0 + 1 + 1
2 + 1
4 = 53
4.
One can readily see from Equation 2.19 that shifting the binary point one
position to the left has the effect of dividing the number by 2. For example, while
101.112 represents the number 53
4, 10.1112 represents the number 2 + 0 + 1
2 +

1
4 + 1
8 = 2 7
8. Similarly, shifting the binary point one position to the right has the
effect of multiplying the number by 2. For example, 1011.12 represents the number
8 + 0 + 2 + 1 + 1
2 = 111
2.
Note that numbers of the form 0.11 . . . 12 represent numbers just below 1. For
example, 0.1111112 represents 63
64. We will use the shorthand notation 1.0 −ϵ to
represent such values.
Assuming we consider only ﬁnite-length encodings, decimal notation cannot
represent numbers such as 1
3 and 5
7 exactly. Similarly, fractional binary notation
can only represent numbers that can be written x × 2y. Other values can only be
approximated. For example, the number 1
5 can be represented exactly as the frac-
tional decimal number 0.20. As a fractional binary number, however, we cannot
represent it exactly and instead must approximate it with increasing accuracy by
lengthening the binary representation:
Representation
Value
Decimal
0.02
0
2
0.010
0.012
1
4
0.2510
0.0102
2
8
0.2510
0.00112
3
16
0.187510
0.001102
6
32
0.187510
0.0011012
13
64
0.20312510
0.00110102
26
128
0.20312510
0.001100112
51
256
0.1992187510
Practice Problem 2.45 (solution page 193)
Fill in the missing information in the following table:
Fractional value
Binary representation
Decimal representation
1
8
0.001
0.125
3
4
5
16
10.1011
1.001
5.875
3.1875
Practice Problem 2.46 (solution page 194)
The imprecision of ﬂoating-point arithmetic can have disastrous effects. On Febru-
ary 25, 1991, during the ﬁrst Gulf War, an American Patriot Missile battery in
Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud missile. The
Scud struck an American Army barracks and killed 28 soldiers. The US General

Accounting Ofﬁce (GAO) conducted a detailed analysis of the failure [76] and de-
termined that the underlying cause was an imprecision in a numeric calculation.
In this exercise, you will reproduce part of the GAO’s analysis.
The Patriot system contains an internal clock, implemented as a counter
that is incremented every 0.1 seconds. To determine the time in seconds, the
program would multiply the value of this counter by a 24-bit quantity that was
a fractional binary approximation to
1
10. In particular, the binary representation
of 1
10 is the nonterminating sequence 0.000110011[0011] . . .2, where the portion in
brackets is repeated indeﬁnitely. The program approximated 0.1, as a value x, by
considering just the ﬁrst 23 bits of the sequence to the right of the binary point:
x = 0.00011001100110011001100. (See Problem 2.51 for a discussion of how they
could have approximated 0.1 more precisely.)
A. What is the binary representation of 0.1 −x?
B. What is the approximate decimal value of 0.1 −x?
C. The clock starts at 0 when the system is ﬁrst powered up and keeps counting
up from there. In this case, the system had been running for around 100 hours.
What was the difference between the actual time and the time computed by
the software?
D. The system predicts where an incoming missile will appear based on its
velocity and the time of the last radar detection. Given that a Scud travels
at around 2,000 meters per second, how far off was its prediction?
Normally, a slight error in the absolute time reported by a clock reading would
not affect a tracking computation. Instead, it should depend on the relative time
between two successive readings. The problem was that the Patriot software had
been upgraded to use a more accurate function for reading time, but not all of
the function calls had been replaced by the new code. As a result, the tracking
software used the accurate time for one reading and the inaccurate time for the
other [103].
2.4.2
IEEE Floating-Point Representation
Positional notation such as considered in the previous section would not be ef-
ﬁcient for representing very large numbers. For example, the representation of
5 × 2100 would consist of the bit pattern 101 followed by 100 zeros. Instead, we
would like to represent numbers in a form x × 2y by giving the values of x and y.
The IEEE ﬂoating-point standard represents a number in a form V = (−1)s ×
M × 2E:
. The sign s determines whether the number is negative (s = 1) or positive
(s = 0), where the interpretation of the sign bit for numeric value 0 is handled
as a special case.
. The signiﬁcand M is a fractional binary number that ranges either between 1
and 2 −ϵ or between 0 and 1 −ϵ.
. The exponent E weights the value by a (possibly negative) power of 2.

31
s
exp
frac
30
Single precision
23
0
22
63
s
exp
frac (51:32)
62
Double precision
52
32
51
31
frac (31:0)
0
Figure 2.32
Standard ﬂoating-point formats. Floating-point numbers are represented
by three ﬁelds. For the two most common formats, these are packed in 32-bit (single-
precision) or 64-bit (double-precision) words.
The bit representation of a ﬂoating-point number is divided into three ﬁelds to
encode these values:
. The single sign bit s directly encodes the sign s.
. The k-bit exponent ﬁeld exp = ek−1 . . . e1e0 encodes the exponent E.
. The n-bit fraction ﬁeld frac = fn−1 . . . f1f0 encodes the signiﬁcand M, but
the value encoded also depends on whether or not the exponent ﬁeld equals
0.
Figure 2.32 shows the packing of these three ﬁelds into words for the two
most common formats. In the single-precision ﬂoating-point format (a float
in C), ﬁelds s, exp, and frac are 1, k = 8, and n = 23 bits each, yielding a 32-
bit representation. In the double-precision ﬂoating-point format (a double in C),
ﬁelds s, exp, and frac are 1, k = 11, and n = 52 bits each, yielding a 64-bit
representation.
The value encoded by a given bit representation can be divided into three
different cases (the latter having two variants), depending on the value of exp.
These are illustrated in Figure 2.33 for the single-precision format.
Case 1: Normalized Values
This is the most common case. It occurs when the bit pattern of exp is neither
all zeros (numeric value 0) nor all ones (numeric value 255 for single precision,
2047 for double). In this case, the exponent ﬁeld is interpreted as representing a
signed integer in biased form. That is, the exponent value is E = e −Bias, where
e is the unsigned number having bit representation ek−1 . . . e1e0 and Bias is a bias
value equal to 2k−1 −1 (127 for single precision and 1023 for double). This yields
exponent ranges from −126 to +127 for single precision and −1022 to +1023 for
double precision.
The fraction ﬁeld frac is interpreted as representing the fractional value f ,
where 0 ≤f < 1, having binary representation 0.fn−1 . . . f1f0, that is, with the

Aside
Why set the bias this way for denormalized values?
Having the exponent value be 1 −Bias rather than simply −Bias might seem counterintuitive. We will
see shortly that it provides for smooth transition from denormalized to normalized values.
s 0 0 0 0 0 0 0 0
f
≠ 0
2. Denormalized
s 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
3a. Infinity
s 1 1 1 1 1 1 1 1
3b. NaN
s
≠ 0 and ≠ 255
f
1. Normalized
Figure 2.33
Categories of single-precision ﬂoating-point values. The value of the
exponent determines whether the number is (1) normalized, (2) denormalized, or (3) a
special value.
binary point to the left of the most signiﬁcant bit. The signiﬁcand is deﬁned to be
M = 1 + f . This is sometimes called an implied leading 1 representation, because
we can view M to be the number with binary representation 1.fn−1fn−2 . . . f0. This
representation is a trick for getting an additional bit of precision for free, since we
can always adjust the exponent E so that signiﬁcand M is in the range 1 ≤M < 2
(assuming there is no overﬂow). We therefore do not need to explicitly represent
the leading bit, since it always equals 1.
Case 2: Denormalized Values
When the exponent ﬁeld is all zeros, the represented number is in denormalized
form. In this case, the exponent value is E = 1 −Bias, and the signiﬁcand value is
M = f , that is, the value of the fraction ﬁeld without an implied leading 1.
Denormalized numbers serve two purposes. First, they provide a way to
represent numeric value 0, since with a normalized number we must always have
M ≥1, and hence we cannot represent 0. In fact, the ﬂoating-point representation
of +0.0 has a bit pattern of all zeros: the sign bit is 0, the exponent ﬁeld is all
zeros (indicating a denormalized value), and the fraction ﬁeld is all zeros, giving
M = f = 0. Curiously, when the sign bit is 1, but the other ﬁelds are all zeros, we
get the value −0.0. With IEEE ﬂoating-point format, the values −0.0 and +0.0
are considered different in some ways and the same in others.

A second function of denormalized numbers is to represent numbers that are
very close to 0.0. They provide a property known as gradual underﬂow in which
possible numeric values are spaced evenly near 0.0.
Case 3: Special Values
A ﬁnal category of values occurs when the exponent ﬁeld is all ones. When the
fraction ﬁeld is all zeros, the resulting values represent inﬁnity, either +∞when
s = 0 or −∞when s = 1. Inﬁnity can represent results that overﬂow, as when we
multiply two very large numbers, or when we divide by zero. When the fraction
ﬁeld is nonzero, the resulting value is called a NaN, short for “not a number.” Such
values are returned as the result of an operation where the result cannot be given
as a real number or as inﬁnity, as when computing
√
−1 or ∞−∞. They can also
be useful in some applications for representing uninitialized data.
2.4.3
Example Numbers
Figure 2.34 shows the set of values that can be represented in a hypothetical 6-bit
format having k = 3 exponent bits and n = 2 fraction bits. The bias is 23−1 −1 = 3.
Part (a) of the ﬁgure shows all representable values (other than NaN). The two
inﬁnities are at the extreme ends. The normalized numbers with maximum mag-
nitude are ±14. The denormalized numbers are clustered around 0. These can be
seen more clearly in part (b) of the ﬁgure, where we show just the numbers be-
tween −1.0 and +1.0. The two zeros are special cases of denormalized numbers.
Observe that the representable numbers are not uniformly distributed—they are
denser nearer the origin.
Figure 2.35 shows some examples for a hypothetical 8-bit ﬂoating-point for-
mat having k = 4 exponent bits and n = 3 fraction bits. The bias is 24−1 −1 = 7.
The ﬁgure is divided into three regions representing the three classes of numbers.
The different columns show how the exponent ﬁeld encodes the exponent E,
while the fraction ﬁeld encodes the signiﬁcand M, and together they form the
10
0.8
0.6
0.4
0.2
0.2
0
0
0.4
0.6
0.8
1
0
1
5
0
5
10


Denormalized
Normalized
Infinity
Denormalized
Normalized
Infinity
(a) Complete range
(b) Values between 1.0 and 1.0
Figure 2.34
Representable values for 6-bit ﬂoating-point format. There are k = 3
exponent bits and n = 2 fraction bits. The bias is 3.

Exponent
Fraction
Value
Description
Bit representation
e
E
2E
f
M
2E × M
V
Decimal
Zero
0 0000 000
0
−6
1
64
0
8
0
8
0
512
0
0.0
Smallest positive
0 0000 001
0
−6
1
64
1
8
1
8
1
512
1
512
0.001953
0 0000 010
0
−6
1
64
2
8
2
8
2
512
1
256
0.003906
0 0000 011
0
−6
1
64
3
8
3
8
3
512
3
512
0.005859
...
Largest denormalized
0 0000 111
0
−6
1
64
7
8
7
8
7
512
7
512
0.013672
Smallest normalized
0 0001 000
1
−6
1
64
0
8
8
8
8
512
1
64
0.015625
0 0001 001
1
−6
1
64
1
8
9
8
9
512
9
512
0.017578
...
0 0110 110
6
−1
1
2
6
8
14
8
14
16
7
8
0.875
0 0110 111
6
−1
1
2
7
8
15
8
15
16
15
16
0.9375
One
0 0111 000
7
0
1
0
8
8
8
8
8
1
1.0
0 0111 001
7
0
1
1
8
9
8
9
8
9
8
1.125
0 0111 010
7
0
1
2
8
10
8
10
8
5
4
1.25
...
0 1110 110
14
7
128
6
8
14
8
1792
8
224
224.0
Largest normalized
0 1110 111
14
7
128
7
8
15
8
1920
8
240
240.0
Inﬁnity
0 1111 000
—
—
—
—
—
—
∞
—
Figure 2.35
Example nonnegative values for 8-bit ﬂoating-point format. There are k = 4 exponent bits
and n = 3 fraction bits. The bias is 7.
represented value V = 2E × M. Closest to 0 are the denormalized numbers, start-
ing with 0 itself. Denormalized numbers in this format have E = 1 −7 = −6, giv-
ing a weight 2E = 1
64. The fractions f and signiﬁcands M range over the values
0, 1
8, . . . , 7
8, giving numbers V in the range 0 to 1
64 × 7
8 =
7
512.
The smallest normalized numbers in this format also have E = 1 −7 = −6,
and the fractions also range over the values 0, 1
8, . . . 7
8. However, the signiﬁcands
then range from 1 + 0 = 1 to 1 + 7
8 = 15
8 , giving numbers V in the range
8
512 = 1
64
to 15
512.
Observe the smooth transition between the largest denormalized number
7
512
and the smallest normalized number
8
512. This smoothness is due to our deﬁnition
of E for denormalized values. By making it 1 −Bias rather than −Bias, we com-
pensate for the fact that the signiﬁcand of a denormalized number does not have
an implied leading 1.

As we increase the exponent, we get successively larger normalized values,
passing through 1.0 and then to the largest normalized number. This number has
exponent E = 7, giving a weight 2E = 128. The fraction equals 7
8, giving a signiﬁ-
cand M = 15
8 . Thus, the numeric value is V = 240. Going beyond this overﬂows to
+∞.
One interesting property of this representation is that if we interpret the bit
representations of the values in Figure 2.35 as unsigned integers, they occur in
ascending order, as do the values they represent as ﬂoating-point numbers. This is
no accident—the IEEE format was designed so that ﬂoating-point numbers could
be sorted using an integer sorting routine. A minor difﬁculty occurs when dealing
with negative numbers, since they have a leading 1 and occur in descending order,
but this can be overcome without requiring ﬂoating-point operations to perform
comparisons (see Problem 2.84).
Practice Problem 2.47 (solution page 194)
Consider a 5-bit ﬂoating-point representation based on the IEEE ﬂoating-point
format, with one sign bit, two exponent bits (k = 2), and two fraction bits (n = 2).
The exponent bias is 22−1 −1 = 1.
The table that follows enumerates the entire nonnegative range for this 5-bit
ﬂoating-point representation. Fill in the blank table entries using the following
directions:
e: The value represented by considering the exponent ﬁeld to be an unsigned
integer
E: The value of the exponent after biasing
2E: The numeric weight of the exponent
f : The value of the fraction
M: The value of the signiﬁcand
2E × M: The (unreduced) fractional value of the number
V : The reduced fractional value of the number
Decimal: The decimal representation of the number
Express the values of 2E, f , M, 2E × M, and V either as integers (when
possible) or as fractions of the form x
y , where y is a power of 2. You need not
ﬁll in entries marked —.
Bits
e
E
2E
f
M
2E × M
V
Decimal
0 00 00
0 00 01
0 00 10
0 00 11
0 01 00
0 01 01
1
0
1
1
4
5
4
5
4
5
4
1.25

Bits
e
E
2E
f
M
2E × M
V
Decimal
0 01 10
0 01 11
0 10 00
0 10 01
0 10 10
0 10 11
0 11 00
—
—
—
—
—
—
—
0 11 01
—
—
—
—
—
—
—
0 11 10
—
—
—
—
—
—
—
0 11 11
—
—
—
—
—
—
—
Figure 2.36 shows the representations and numeric values of some important
single- and double-precision ﬂoating-point numbers. As with the 8-bit format
shown in Figure 2.35, we can see some general properties for a ﬂoating-point
representation with a k-bit exponent and an n-bit fraction:
. The value +0.0 always has a bit representation of all zeros.
. The smallest positive denormalized value has a bit representation consisting of
a 1 in the least signiﬁcant bit position and otherwise all zeros. It has a fraction
(and signiﬁcand) value M = f = 2−n and an exponent value E = −2k−1 + 2.
The numeric value is therefore V = 2−n−2k−1+2.
. The largest denormalized value has a bit representation consisting of an
exponent ﬁeld of all zeros and a fraction ﬁeld of all ones. It has a fraction
(and signiﬁcand) value M = f = 1 −2−n (which we have written 1 −ϵ) and
an exponent value E = −2k−1 + 2. The numeric value is therefore V = (1 −
2−n) × 2−2k−1+2, which is just slightly smaller than the smallest normalized
value.
. The smallest positive normalized value has a bit representation with a 1 in
the least signiﬁcant bit of the exponent ﬁeld and otherwise all zeros. It has a
Single precision
Double precision
Description
exp
frac
Value
Decimal
Value
Decimal
Zero
00 . . . 00
0 . . . 00
0
0.0
0
0.0
Smallest denormalized
00 . . . 00
0 . . . 01
2−23 × 2−126
1.4 × 10−45
2−52 × 2−1022
4.9 × 10−324
Largest denormalized
00 . . . 00
1 . . . 11
(1 −ϵ) × 2−126
1.2 × 10−38
(1 −ϵ) × 2−1022
2.2 × 10−308
Smallest normalized
00 . . . 01
0 . . . 00
1 × 2−126
1.2 × 10−38
1 × 2−1022
2.2 × 10−308
One
01 . . . 11
0 . . . 00
1 × 20
1.0
1 × 20
1.0
Largest normalized
11 . . . 10
1 . . . 11
(2 −ϵ) × 2127
3.4 × 1038
(2 −ϵ) × 21023
1.8 × 10308
Figure 2.36
Examples of nonnegative ﬂoating-point numbers.

signiﬁcand value M = 1 and an exponent value E = −2k−1 + 2. The numeric
value is therefore V = 2−2k−1+2.
. The value 1.0 has a bit representation with all but the most signiﬁcant bit of
the exponent ﬁeld equal to 1 and all other bits equal to 0. Its signiﬁcand value
is M = 1 and its exponent value is E = 0.
. The largest normalized value has a bit representation with a sign bit of 0, the
least signiﬁcant bit of the exponent equal to 0, and all other bits equal to 1. It
has a fraction value of f = 1 −2−n, giving a signiﬁcand M = 2 −2−n (which we
have written 2 −ϵ.) It has an exponent value E = 2k−1 −1, giving a numeric
value V = (2 −2−n) × 22k−1−1 = (1 −2−n−1) × 22k−1.
One useful exercise for understanding ﬂoating-point representations is to con-
vert sample integer values into ﬂoating-point form. For example, we saw in Figure
2.15 that 12,345 has binary representation [11000000111001]. We create a normal-
ized representation of this by shifting 13 positions to the right of a binary point,
giving 12,345 = 1.10000001110012 × 213. To encode this in IEEE single-precision
format, we construct the fraction ﬁeld by dropping the leading 1 and adding 10
zeros to the end, giving binary representation [10000001110010000000000]. To
construct the exponent ﬁeld, we add bias 127 to 13, giving 140, which has bi-
nary representation [10001100]. We combine this with a sign bit of 0 to get the
ﬂoating-point representation in binary of [01000110010000001110010000000000].
Recall from Section 2.1.3 that we observed the following correlation in the bit-
level representations of the integer value 12345 (0x3039) and the single-precision
ﬂoating-point value 12345.0 (0x4640E400):
0
0
0
0
3
0
3
9
00000000000000000011000000111001
*************
4
6
4
0
E
4
0
0
01000110010000001110010000000000
We can now see that the region of correlation corresponds to the low-order
bits of the integer, stopping just before the most signiﬁcant bit equal to 1 (this bit
forms the implied leading 1), matching the high-order bits in the fraction part of
the ﬂoating-point representation.
Practice Problem 2.48 (solution page 195)
As mentioned in Problem 2.6, the integer 3,510,593 has hexadecimal represen-
tation 0x00359141, while the single-precision ﬂoating-point number 3,510,593.0
has hexadecimal representation 0x4A564504. Derive this ﬂoating-point represen-
tation and explain the correlation between the bits of the integer and ﬂoating-point
representations.

Practice Problem 2.49 (solution page 195)
A. For a ﬂoating-point format with an n-bit fraction, give a formula for the
smallest positive integer that cannot be represented exactly (because it
would require an (n + 1)-bit fraction to be exact). Assume the exponent
ﬁeld size k is large enough that the range of representable exponents does
not provide a limitation for this problem.
B. What is the numeric value of this integer for single-precision format (n =
23)?
2.4.4
Rounding
Floating-point arithmetic can only approximate real arithmetic, since the repre-
sentation has limited range and precision. Thus, for a value x, we generally want
a systematic method of ﬁnding the “closest” matching value x′ that can be rep-
resented in the desired ﬂoating-point format. This is the task of the rounding
operation. One key problem is to deﬁne the direction to round a value that is
halfway between two possibilities. For example, if I have $1.50 and want to round
it to the nearest dollar, should the result be $1 or $2? An alternative approach is
to maintain a lower and an upper bound on the actual number. For example, we
could determine representable values x−and x+ such that the value x is guaran-
teed to lie between them: x−≤x ≤x+. The IEEE ﬂoating-point format deﬁnes
four different rounding modes. The default method ﬁnds a closest match, while
the other three can be used for computing upper and lower bounds.
Figure 2.37 illustrates the four rounding modes applied to the problem of
rounding a monetary amount to the nearest whole dollar. Round-to-even (also
called round-to-nearest) is the default mode. It attempts to ﬁnd a closest match.
Thus, it rounds $1.40 to $1 and $1.60 to $2, since these are the closest whole dollar
values. The only design decision is to determine the effect of rounding values
that are halfway between two possible results. Round-to-even mode adopts the
convention that it rounds the number either upward or downward such that the
least signiﬁcant digit of the result is even. Thus, it rounds both $1.50 and $2.50
to $2.
The other three modes produce guaranteed bounds on the actual value. These
can be useful in some numerical applications. Round-toward-zero mode rounds
positive numbers downward and negative numbers upward, giving a value ˆx such
Mode
$1.40
$1.60
$1.50
$2.50
$–1.50
Round-to-even
$1
$2
$2
$2
$–2
Round-toward-zero
$1
$1
$1
$2
$–1
Round-down
$1
$1
$1
$2
$–2
Round-up
$2
$2
$2
$3
$–1
Figure 2.37
Illustration of rounding modes for dollar rounding. The ﬁrst rounds to
a nearest value, while the other three bound the result above or below.

that |ˆx| ≤|x|. Round-down mode rounds both positive and negative numbers
downward, giving a value x−such that x−≤x. Round-up mode rounds both
positive and negative numbers upward, giving a value x+ such that x ≤x+.
Round-to-even at ﬁrst seems like it has a rather arbitrary goal—why is there
any reason to prefer even numbers? Why not consistently round values halfway
between two representable values upward? The problem with such a convention
is that one can easily imagine scenarios in which rounding a set of data values
would then introduce a statistical bias into the computation of an average of the
values. The average of a set of numbers that we rounded by this means would
be slightly higher than the average of the numbers themselves. Conversely, if we
always rounded numbers halfway between downward, the average of a set of
rounded numbers would be slightly lower than the average of the numbers them-
selves. Rounding toward even numbers avoids this statistical bias in most real-life
situations. It will round upward about 50% of the time and round downward about
50% of the time.
Round-to-even rounding can be applied even when we are not rounding to
a whole number. We simply consider whether the least signiﬁcant digit is even
or odd. For example, suppose we want to round decimal numbers to the nearest
hundredth. We would round 1.2349999 to 1.23 and 1.2350001 to 1.24, regardless
of rounding mode, since they are not halfway between 1.23 and 1.24. On the other
hand, we would round both 1.2350000 and 1.2450000 to 1.24, since 4 is even.
Similarly, round-to-even rounding can be applied to binary fractional num-
bers. We consider least signiﬁcant bit value 0 to be even and 1 to be odd. In
general, the rounding mode is only signiﬁcant when we have a bit pattern of the
form XX . . . X.YY . . . Y100 . . ., where X and Y denote arbitrary bit values with
the rightmost Y being the position to which we wish to round. Only bit patterns
of this form denote values that are halfway between two possible results. As ex-
amples, consider the problem of rounding values to the nearest quarter (i.e., 2 bits
to the right of the binary point.) We would round 10.000112 (2 3
32) down to 10.002
(2), and 10.001102 (2 3
16) up to 10.012 (2 1
4), because these values are not halfway
between two possible values. We would round 10.111002 (2 7
8) up to 11.002 (3) and
10.101002 (2 5
8) down to 10.102 (2 1
2), since these values are halfway between two
possible results, and we prefer to have the least signiﬁcant bit equal to zero.
Practice Problem 2.50 (solution page 195)
Show how the following binary fractional values would be rounded to the nearest
half (1 bit to the right of the binary point), according to the round-to-even rule.
In each case, show the numeric values, both before and after rounding.
A. 10.1112
B. 11.0102
C. 11.0002
D. 10.1102

Practice Problem 2.51 (solution page 195)
We saw in Problem 2.46 that the Patriot missile software approximated 0.1 as x =
0.000110011001100110011002. Suppose instead that they had used IEEE round-
to-even mode to determine an approximation x′ to 0.1 with 23 bits to the right of
the binary point.
A. What is the binary representation of x′?
B. What is the approximate decimal value of x′ −0.1?
C. How far off would the computed clock have been after 100 hours of opera-
tion?
D. How far off would the program’s prediction of the position of the Scud
missile have been?
Practice Problem 2.52 (solution page 196)
Consider the following two 7-bit ﬂoating-point representations based on the IEEE
ﬂoating-point format. Neither has a sign bit—they can only represent nonnegative
numbers.
1. Format A
There are k = 3 exponent bits. The exponent bias is 3.
There are n = 4 fraction bits.
2. Format B
There are k = 4 exponent bits. The exponent bias is 7.
There are n = 3 fraction bits.
Below, you are given some bit patterns in format A, and your task is to convert
them to the closest value in format B. If necessary, you should apply the round-to-
even rounding rule. In addition, give the values of numbers given by the format A
and format B bit patterns. Give these as whole numbers (e.g., 17) or as fractions
(e.g., 17/64).
Format A
Format B
Bits
Value
Bits
Value
011 0000
1
0111 000
1
101 1110
010 1001
110 1111
000 0001
2.4.5
Floating-Point Operations
The IEEE standard speciﬁes a simple rule for determining the result of an arith-
metic operation such as addition or multiplication. Viewing ﬂoating-point values x

and y as real numbers, and some operation ⊙deﬁned over real numbers, the com-
putation should yield Round(x ⊙y), the result of applying rounding to the exact
result of the real operation. In practice, there are clever tricks ﬂoating-point unit
designers use to avoid performing this exact computation, since the computation
need only be sufﬁciently precise to guarantee a correctly rounded result. When
one of the arguments is a special value, such as −0, ∞, or NaN, the standard spec-
iﬁes conventions that attempt to be reasonable. For example, 1/−0 is deﬁned to
yield −∞, while 1/+0 is deﬁned to yield +∞.
One strength of the IEEE standard’s method of specifying the behavior of
ﬂoating-point operations is that it is independent of any particular hardware or
software realization. Thus, we can examine its abstract mathematical properties
without considering how it is actually implemented.
We saw earlier that integer addition, both unsigned and two’s complement,
forms an abelian group. Addition over real numbers also forms an abelian group,
but we must consider what effect rounding has on these properties. Let us deﬁne
x +f y to be Round(x + y). This operation is deﬁned for all values of x and y,
although it may yield inﬁnity even when both x and y are real numbers due to
overﬂow. The operation is commutative, with x +f y = y +f x for all values of x and
y. On the other hand, the operation is not associative. For example, with single-
precision ﬂoating point the expression (3.14+1e10)-1e10 evaluates to 0.0—the
value 3.14 is lost due to rounding. On the other hand, the expression 3.14+(1e10-
1e10) evaluates to 3.14. As with an abelian group, most values have inverses
under ﬂoating-point addition, that is, x +f −x = 0. The exceptions are inﬁnities
(since +∞−∞= NaN), and NaNs, since NaN +f x = NaN for any x.
The lack of associativity in ﬂoating-point addition is the most important group
property that is lacking. It has important implications for scientiﬁc programmers
and compiler writers. For example, suppose a compiler is given the following code
fragment:
x = a + b + c;
y = b + c + d;
The compiler might be tempted to save one ﬂoating-point addition by generating
the 例如，考虑以下代码：
t = b + c;
x = a + t;
y = t + d;
However, this computation might yield a different value for x than would the
original, since it uses a different association of the addition operations. In most
applications, the difference would be so small as to be inconsequential. Unfor-
tunately, compilers have no way of knowing what trade-offs the user is willing to
make between efﬁciency and faithfulness to the exact behavior of the original pro-
gram. As a result, they tend to be very conservative, avoiding any optimizations
that could have even the slightest effect on functionality.

On the other hand, ﬂoating-point addition satisﬁes the following monotonicity
property: if a ≥b, then x +f a ≥x +f b for any values of a, b, and x other than NaN.
This property of real (and integer) addition is not obeyed by unsigned or two’s-
complement addition.
Floating-point multiplication also obeys many of the properties one normally
associates with multiplication. Let us deﬁne x *f y to be Round(x × y). This oper-
ation is closed under multiplication (although possibly yielding inﬁnity or NaN),
it is commutative, and it has 1.0 as a multiplicative identity. On the other hand,
it is not associative, due to the possibility of overﬂow or the loss of precision
due to rounding. For example, with single-precision ﬂoating point, the expression
(1e20*1e20)*1e-20 evaluates to +∞, while 1e20*(1e20*1e-20) evaluates to
1e20. In addition, ﬂoating-point multiplication does not distribute over addition.
For example, with single-precision ﬂoating point, the expression 1e20*(1e20-
1e20) evaluates to 0.0, while 1e20*1e20-1e20*1e20 evaluates to NaN.
On the other hand, ﬂoating-point multiplication satisﬁes the following mono-
tonicity properties for any values of a, b, and c other than NaN:
a ≥b
and
c ≥0 ⇒a *f c ≥b *f c
a ≥b
and
c ≤0 ⇒a *f c ≤b *f c
In addition, we are also guaranteed that a *f a ≥0, as long as a ̸= NaN. As we
saw earlier, none of these monotonicity properties hold for unsigned or two’s-
complement multiplication.
This lack of associativity and distributivity is of serious concern to scientiﬁc
programmers and to compiler writers. Even such a seemingly simple task as writing
code to determine whether two lines intersect in three-dimensional space can be
a major challenge.
2.4.6
Floating Point in C
All versions of C provide two different ﬂoating-point data types: float and dou-
ble. On machines that support IEEE ﬂoating point, these data types correspond
to single- and double-precision ﬂoating point. In addition, the machines use the
round-to-even rounding mode. Unfortunately, since the C standards do not re-
quire the machine to use IEEE ﬂoating point, there are no standard methods to
change the rounding mode or to get special values such as −0, +∞, −∞, or NaN.
Most systems provide a combination of include (.h) ﬁles and procedure libraries
to provide access to these features, but the details vary from one system to an-
other. For example, the GNU compiler gcc deﬁnes program constants INFINITY
(for +∞) and NAN (for NaN) when the following sequence occurs in the program
ﬁle:
#define _GNU_SOURCE 1

```c
#include <math.h>
```

Practice Problem 2.53 (solution page 196)
Fill in the following macro deﬁnitions to generate the double-precision values +∞,
−∞, and −0:
#define POS_INFINITY
#define NEG_INFINITY
#define NEG_ZERO
You cannot use any include ﬁles (such as math.h), but you can make use of the
fact that the largest ﬁnite number that can be represented with double precision
is around 1.8 × 10308.
When casting values between int, float, and double formats, the program
changes the numeric values and the bit representations as follows (assuming data
type int is 32 bits):
. From int to float, the number cannot overﬂow, but it may be rounded.
. From int or float to double, the exact numeric value can be preserved be-
cause double has both greater range (i.e., the range of representable values),
as well as greater precision (i.e., the number of signiﬁcant bits).
. From double to float, the value can overﬂow to +∞or −∞, since the range
is smaller. Otherwise, it may be rounded, because the precision is smaller.
. From float or double to int, the value will be rounded toward zero. For
example, 1.999 will be converted to 1, while −1.999 will be converted to
−1. Furthermore, the value may overﬂow. The C standards do not specify
a ﬁxed result for this case. Intel-compatible microprocessors designate the
bit pattern [10 . . . 00] (TMinw for word size w) as an integer indeﬁnite value.
Any conversion from ﬂoating point to integer that cannot assign a reasonable
integer approximation yields this value. Thus, the expression (int) +1e10
yields -21483648, generating a negative value from a positive one.
Practice Problem 2.54 (solution page 196)
Assume variables x, f, and d are of type int, float, and double, respectively.
Their values are arbitrary, except that neither f nor d equals +∞, −∞, or NaN.
For each of the following C expressions, either argue that it will always be true
(i.e., evaluate to 1) or give a value for the variables such that it is not true (i.e.,
evaluates to 0).
A. x == (int)(double) x
B. x == (int)(float) x
C. d == (double)(float) d
D. f == (float)(double) f
E. f == -(-f)

F.
1.0/2 == 1/2.0
G. d*d >= 0.0
H. (f+d)-f == d
2.5
Summary
Computers encode information as bits, generally organized as sequences of bytes.
Different encodings are used for representing integers, real numbers, and charac-
ter strings. Different models of computers use different conventions for encoding
numbers and for ordering the bytes within multi-byte data.
The C language is designed to accommodate a wide range of different imple-
mentations in terms of word sizes and numeric encodings. Machines with 64-bit
word sizes have become increasingly common, replacing the 32-bit machines that
dominated the market for around 30 years. Because 64-bit machines can also run
programs compiled for 32-bit machines, we have focused on the distinction be-
tween 32- and 64-bit programs, rather than machines. The advantage of 64-bit pro-
grams is that they can go beyond the 4 GB address limitation of 32-bit programs.
Most machines encode signed numbers using a two’s-complement representa-
tion and encode ﬂoating-point numbers using IEEE Standard 754. Understanding
these encodings at the bit level, as well as understanding the mathematical char-
acteristics of the arithmetic operations, is important for writing programs that
operate correctly over the full range of numeric values.
When casting between signed and unsigned integers of the same size, most
C implementations follow the convention that the underlying bit pattern does
not change. On a two’s-complement machine, this behavior is characterized by
functions T2Uw and U2Tw, for a w-bit value. The implicit casting of C gives results
that many programmers do not anticipate, often leading to program bugs.
Due to the ﬁnite lengths of the encodings, computer arithmetic has properties
quite different from conventional integer and real arithmetic. The ﬁnite length can
cause numbers to overﬂow, when they exceed the range of the representation.
Floating-point values can also underﬂow, when they are so close to 0.0 that they
are changed to zero.
The ﬁnite integer arithmetic implemented by C, as well as most other pro-
gramming languages, has some peculiar properties compared to true integer arith-
metic. For example, the expression x*x can evaluate to a negative number due
to overﬂow. Nonetheless, both unsigned and two’s-complement arithmetic satisfy
many of the other properties of integer arithmetic, including associativity, com-
mutativity, and distributivity. This allows compilers to do many optimizations. For
example, in replacing the expression 7*x by (x<<3)-x, we make use of the as-
sociative, commutative, and distributive properties, along with the relationship
between shifting and multiplying by powers of 2.
We have seen several clever ways to exploit combinations of bit-level opera-
tions and arithmetic operations. For example, we saw that with two’s-complement
arithmetic, ~x+1 is equivalent to -x. As another example, suppose we want a bit

Aside
Ariane 5: The high cost of ﬂoating-point overﬂow
Converting large ﬂoating-point numbers to integers is a common source of programming errors. Such
an error had disastrous consequences for the maiden voyage of the Ariane 5 rocket, on June 4, 1996. Just
37 seconds after liftoff, the rocket veered off its ﬂight path, broke up, and exploded. Communication
satellites valued at $500 million were on board the rocket.
A later investigation [73, 33] showed that the computer controlling the inertial navigation system
had sent invalid data to the computer controlling the engine nozzles. Instead of sending ﬂight control
information, it had sent a diagnostic bit pattern indicating that an overﬂow had occurred during the
conversion of a 64-bit ﬂoating-point number to a 16-bit signed integer.
The value that overﬂowed measured the horizontal velocity of the rocket, which could be more
than ﬁve times higher than that achieved by the earlier Ariane 4 rocket. In the design of the Ariane 4
software, they had carefully analyzed the numeric values and determined that the horizontal velocity
would never overﬂow a 16-bit number. Unfortunately, they simply reused this part of the software in
the Ariane 5 without checking the assumptions on which it had been based.
pattern of the form [0, . . . , 0, 1, . . . , 1], consisting of w −k zeros followed by k
ones. Such bit patterns are useful for masking operations. This pattern can be gen-
erated by the C expression (1<<k)-1, exploiting the property that the desired
bit pattern has numeric value 2k −1. For example, the expression (1<<8)-1 will
generate the bit pattern 0xFF.
Floating-point representations approximate real numbers by encoding num-
bers of the form x × 2y. IEEE Standard 754 provides for several different preci-
sions, with the most common being single (32 bits) and double (64 bits). IEEE
ﬂoating point also has representations for special values representing plus and
minus inﬁnity, as well as not-a-number.
Floating-point arithmetic must be used very carefully, because it has only
limited range and precision, and because it does not obey common mathematical
properties such as associativity.
Bibliographic Notes
Reference books on C [45, 61] discuss properties of the different data types and
operations. Of these two, only Steele and Harbison [45] cover the newer features
found in ISO C99. There do not yet seem to be any books that cover the features
found in ISO C11. The C standards do not specify details such as precise word sizes
or numeric encodings. Such details are intentionally omitted to make it possible
to implement C on a wide range of different machines. Several books have been
written giving advice to C programmers [59, 74] that warn about problems with
overﬂow, implicit casting to unsigned, and some of the other pitfalls we have
covered in this chapter. These books also provide helpful advice on variable
naming, coding styles, and code testing. Seacord’s book on security issues in C
and C++ programs [97] combines information about C programs, how they are
compiled and executed, and how vulnerabilities may arise. Books on Java (we

recommend the one coauthored by James Gosling, the creator of the language [5])
describe the data formats and arithmetic operations supported by Java.
Most books on logic design [58, 116] have a section on encodings and arith-
metic operations. Such books describe different ways of implementing arithmetic
circuits. Overton’s book on IEEE ﬂoating point [82] provides a detailed descrip-
tion of the format as well as the properties from the perspective of a numerical
applications programmer.
Homework Problems
2.55 ◆
Compile and run the sample code that uses show_bytes (ﬁle show-bytes.c) on
different machines to which you have access. Determine the byte orderings used
by these machines.
2.56 ◆
Try running the code for show_bytes for different sample values.
2.57 ◆
Write procedures show_short, show_long, and show_double that print the byte
representations of C objects of types short, long, and double, respectively. Try
these out on several machines.
2.58 ◆◆
Write a procedure is_little_endian that will return 1 when compiled and run
on a little-endian machine, and will return 0 when compiled and run on a big-
endian machine. This program should run on any machine, regardless of its word
size.
2.59 ◆◆
Write a C expression that will yield a word consisting of the least signiﬁcant byte of
x and the remaining bytes of y. For operands x = 0x89ABCDEF and y = 0x76543210,
this would give 0x765432EF.
2.60 ◆◆
Suppose we number the bytes in a w-bit word from 0 (least signiﬁcant) to w/8 −1
(most signiﬁcant). Write code for the following C function, which will return an
unsigned value in which byte i of argument x has been replaced by byte b:
unsigned replace_byte (unsigned x, int i, unsigned char b);
Here are some examples showing how the function should work:
replace_byte(0x12345678, 2, 0xAB) --> 0x12AB5678
replace_byte(0x12345678, 0, 0xAB) --> 0x123456AB
Bit-Level Integer Coding Rules
In several of the following problems, we will artiﬁcially restrict what programming
constructs you can use to help you gain a better understanding of the bit-level,

logic, and arithmetic operations of C. In answering these problems, your code
must follow these rules:
. Assumptions
Integers are represented in two’s-complement form.
Right shifts of signed data are performed arithmetically.
Data type int is w bits long. For some of the problems, you will be given a
speciﬁc value for w, but otherwise your code should work as long as w is a
multiple of 8. You can use the expression sizeof(int)<<3 to compute w.
. Forbidden
Conditionals (if or ?:), loops, switch statements, function calls, and macro
invocations.
Division, modulus, and multiplication.
Relative comparison operators (<, >, <=, and >=).
. Allowed operations
All bit-level and logic operations.
Left and right shifts, but only with shift amounts between 0 and w −1.
Addition and subtraction.
Equality (==) and inequality (!=) tests. (Some of the problems do not allow
these.)
Integer constants INT_MIN and INT_MAX.
Casting between data types int and unsigned, either explicitly or im-
plicitly.
Even with these rules, you should try to make your code readable by choosing
descriptive variable names and using comments to describe the logic behind your
solutions. As an example, the following code extracts the most signiﬁcant byte
from integer argument x:
/* Get most significant byte from x */
int get_msb(int x) {
/* Shift by w-8 */
int shift_val = (sizeof(int)-1)<<3;
/* Arithmetic shift */
int xright = x >> shift_val;
/* Zero all but LSB */
return xright & 0xFF;
}
2.61 ◆◆
Write C expressions that evaluate to 1 when the following conditions are true and
to 0 when they are false. Assume x is of type int.
A. Any bit of x equals 1.
B. Any bit of x equals 0.

C. Any bit in the least signiﬁcant byte of x equals 1.
D. Any bit in the most signiﬁcant byte of x equals 0.
Your code should follow the bit-level integer coding rules (page 164), with the
additional restriction that you may not use equality (==) or inequality (!=) tests.
2.62 ◆◆◆
Write a function int_shifts_are_arithmetic() that yields 1 when run on a
machine that uses arithmetic right shifts for data type int and yields 0 otherwise.
Your code should work on a machine with any word size. Test your code on several
machines.
2.63 ◆◆◆
Fill in code for the following C functions. Function srl performs a logical right
shift using an arithmetic right shift (given by value xsra), followed by other oper-
ations not including right shifts or division. Function sra performs an arithmetic
right shift using a logical right shift (given by value xsrl), followed by other
operations not including right shifts or division. You may use the computation
8*sizeof(int) to determine w, the number of bits in data type int. The shift
amount k can range from 0 to w −1.
unsigned srl(unsigned x, int k) {
/* Perform shift arithmetically */
unsigned xsra = (int) x >> k;
......
}
int sra(int x, int k) {
/* Perform shift logically */
int xsrl = (unsigned) x >> k;
......
}
2.64 ◆
Write code to implement the following function:
/* Return 1 when any odd bit of x equals 1; 0 otherwise.
Assume w=32 */
int any_odd_one(unsigned x);
Your function should follow the bit-level integer coding rules (page 164),
except that you may assume that data type int has w = 32 bits.

2.65 ◆◆◆◆
Write code to implement the following function:
/* Return 1 when x contains an odd number of 1s; 0 otherwise.
Assume w=32 */
int odd_ones(unsigned x);
Your function should follow the bit-level integer coding rules (page 164),
except that you may assume that data type int has w = 32 bits.
Your code should contain a total of at most 12 arithmetic, bitwise, and logical
operations.
2.66 ◆◆◆
Write code to implement the following function:
/*
* Generate mask indicating leftmost 1 in x.
Assume w=32.
* For example, 0xFF00 -> 0x8000, and 0x6600 --> 0x4000.
* If x = 0, then return 0.
*/
int leftmost_one(unsigned x);
Your function should follow the bit-level integer coding rules (page 164),
except that you may assume that data type int has w = 32 bits.
Your code should contain a total of at most 15 arithmetic, bitwise, and logical
operations.
Hint: First transform x into a bit vector of the form [0 . . . 011 . . . 1].
2.67 ◆◆
You are given the task of writing a procedure int_size_is_32() that yields 1
when run on a machine for which an int is 32 bits, and yields 0 otherwise. You are
not allowed to use the sizeof operator. Here is a ﬁrst attempt:
1
/* The following code does not run properly on some machines */
2
int bad_int_size_is_32() {
3
/* Set most significant bit (msb) of 32-bit machine */
4
int set_msb = 1 << 31;
5
/* Shift past msb of 32-bit word */
6
int beyond_msb = 1 << 32;
7
8
/* set_msb is nonzero when word size >= 32
9
beyond_msb is zero when word size <= 32
*/
10
return set_msb && !beyond_msb;
11
}
When compiled and run on a 32-bit SUN SPARC, however, this procedure
returns 0. The following compiler message gives us an indication of the problem:
warning: left shift count >= width of type

A. In what way does our code fail to comply with the C standard?
B. Modify the code to run properly on any machine for which data type int is
at least 32 bits.
C. Modify the code to run properly on any machine for which data type int is
at least 16 bits.
2.68 ◆◆
Write code for a function with the following prototype:
/*
* Mask with least signficant n bits set to 1
* Examples: n = 6 --> 0x3F, n = 17 --> 0x1FFFF
* Assume 1 <= n <= w
*/
int lower_one_mask(int n);
Your function should follow the bit-level integer coding rules (page 164). Be
careful of the case n = w.
2.69 ◆◆◆
Write code for a function with the following prototype:
/*
* Do rotating left shift.
Assume 0 <= n < w
* Examples when x = 0x12345678 and w = 32:
*
n=4 -> 0x23456781, n=20 -> 0x67812345
*/
unsigned rotate_left(unsigned x, int n);
Your function should follow the bit-level integer coding rules (page 164). Be
careful of the case n = 0.
2.70 ◆◆
Write code for the function with the following prototype:
/*
* Return 1 when x can be represented as an n-bit, 2’s-complement
* number; 0 otherwise
* Assume 1 <= n <= w
*/
int fits_bits(int x, int n);
Your function should follow the bit-level integer coding rules (page 164).
2.71 ◆
You just started working for a company that is implementing a set of procedures
to operate on a data structure where 4 signed bytes are packed into a 32-bit
unsigned. Bytes within the word are numbered from 0 (least signiﬁcant) to 3

(most signiﬁcant). You have been assigned the task of implementing a function
for a machine using two’s-complement arithmetic and arithmetic right shifts with
the following prototype:
/* Declaration of data type where 4 bytes are packed
into an unsigned */
typedef unsigned packed_t;
/* Extract byte from word.
Return as signed integer */
int xbyte(packed_t word, int bytenum);
That is, the function will extract the designated byte and sign extend it to be
a 32-bit int.
Your predecessor (who was ﬁred for incompetence) wrote the 例如，考虑以下代码：
/* Failed attempt at xbyte */
int xbyte(packed_t word, int bytenum)
{
return (word >> (bytenum << 3)) & 0xFF;
}
A. What is wrong with this code?
B. Give a correct implementation of the function that uses only left and right
shifts, along with one subtraction.
2.72 ◆◆
You are given the task of writing a function that will copy an integer val into a
buffer buf, but it should do so only if enough space is available in the buffer.
Here is the code you write:
/* Copy integer into buffer if space is available */
/* WARNING: The following code is buggy */

```c
void copy_int(int val, void *buf, int maxbytes) {
```

if (maxbytes-sizeof(val) >= 0)
memcpy(buf, (void *) &val, sizeof(val));
}
This code makes use of the library function memcpy. Although its use is a bit
artiﬁcial here, where we simply want to copy an int, it illustrates an approach
commonly used to copy larger data structures.
You carefully test the code and discover that it always copies the value to the
buffer, even when maxbytes is too small.
A. Explain why the conditional test in the code always succeeds. Hint: The
sizeof operator returns a value of type size_t.
B. Show how you can rewrite the conditional test to make it work properly.

2.73 ◆◆
Write code for a function with the following prototype:
/* Addition that saturates to TMin or TMax */
int saturating_add(int x, int y);
Instead of overﬂowing the way normal two’s-complement addition does, sat-
urating addition returns TMax when there would be positive overﬂow, and TMin
when there would be negative overﬂow. Saturating arithmetic is commonly used
in programs that perform digital signal processing.
Your function should follow the bit-level integer coding rules (page 164).
2.74 ◆◆
Write a function with the following prototype:
/* Determine whether arguments can be subtracted without overflow */
int tsub_ok(int x, int y);
This function should return 1 if the computation x-y does not overﬂow.
2.75 ◆◆◆
Suppose we want to compute the complete 2w-bit representation of x . y, where
both x and y are unsigned, on a machine for which data type unsigned is w bits.
The low-order w bits of the product can be computed with the expression x*y, so
we only require a procedure with prototype
unsigned unsigned_high_prod(unsigned x, unsigned y);
that computes the high-order w bits of x . y for unsigned variables.
We have access to a library function with prototype
int signed_high_prod(int x, int y);
that computes the high-order w bits of x . y for the case where x and y are in two’s-
complement form. Write code calling this procedure to implement the function
for unsigned arguments. Justify the correctness of your solution.
Hint: Look at the relationship between the signed product x . y and the un-
signed product x′ . y′ in the derivation of Equation 2.18.
2.76 ◆
The library function calloc has the following declaration:
void *calloc(size_t nmemb, size_t size);
According to the library documentation, “The calloc function allocates memory
for an array of nmemb elements of size bytes each. The memory is set to zero. If
nmemb or size is zero, then calloc returns NULL.”
Write an implementation of calloc that performs the allocation by a call to
malloc and sets the memory to zero via memset. Your code should not have any
vulnerabilities due to arithmetic overﬂow, and it should work correctly regardless
of the number of bits used to represent data of type size_t.
As a reference, functions malloc and memset have the following declarations:

void *malloc(size_t size);
void *memset(void *s, int c, size_t n);
2.77 ◆◆
Suppose we are given the task of generating code to multiply integer variable x
by various different constant factors K. To be efﬁcient, we want to use only the
operations +, -, and <<. For the following values of K, write C expressions to
perform the multiplication using at most three operations per expression.
A. K = 17
B. K = −7
C. K = 60
D. K = −112
2.78 ◆◆
Write code for a function with the following prototype:
/* Divide by power of 2. Assume 0 <= k < w-1 */
int divide_power2(int x, int k);
The function should compute x/2k with correct rounding, and it should follow
the bit-level integer coding rules (page 164).
2.79 ◆◆
Write code for a function mul3div4 that, for integer argument x, computes 3 ∗
x/4 but follows the bit-level integer coding rules (page 164). Your code should
replicate the fact that the computation 3*x can cause overﬂow.
2.80 ◆◆◆
Write code for a function threefourths that, for integer argument x, computes
the value of 3
4x, rounded toward zero. It should not overﬂow. Your function should
follow the bit-level integer coding rules (page 164).
2.81 ◆◆
Write C expressions to generate the bit patterns that follow, where ak represents
k repetitions of symbol a. Assume a w-bit data type. Your code may contain
references to parameters j and k, representing the values of j and k, but not a
parameter representing w.
A. 1w−k0k
B. 0w−k−j1k0j
2.82 ◆
We are running programs where values of type int are 32 bits. They are repre-
sented in two’s complement, and they are right shifted arithmetically. Values of
type unsigned are also 32 bits.

We generate arbitrary values x and y, and convert them to unsigned values as
follows:
/* Create some arbitrary values */
int x = random();
int y = random();
/* Convert to unsigned */
unsigned ux = (unsigned) x;
unsigned uy = (unsigned) y;
For each of the following C expressions, you are to indicate whether or
not the expression always yields 1. If it always yields 1, describe the underlying
mathematical principles. Otherwise, give an example of arguments that make it
yield 0.
A. (x<y) == (-x>-y)
B. ((x+y)<<4) + y-x == 17*y+15*x
C. ~x+~y+1 == ~(x+y)
D. (ux-uy) == -(unsigned)(y-x)
E. ((x >> 2) << 2) <= x
2.83 ◆◆
Consider numbers having a binary representation consisting of an inﬁnite string
of the form 0.y y y y y y . . . , where y is a k-bit sequence. For example, the binary
representation of 1
3 is 0.01010101 . . . (y = 01), while the representation of 1
5 is
0.001100110011 . . . (y = 0011).
A. Let Y = B2Uk(y), that is, the number having binary representation y. Give
a formula in terms of Y and k for the value represented by the inﬁnite string.
Hint: Consider the effect of shifting the binary point k positions to the right.
B. What is the numeric value of the string for the following values of y?
(a) 101
(b) 0110
(c) 010011
2.84 ◆
Fill in the return value for the following procedure, which tests whether its ﬁrst
argument is less than or equal to its second. Assume the function f2u returns an
unsigned 32-bit number having the same bit representation as its ﬂoating-point
argument. You can assume that neither argument is NaN. The two ﬂavors of zero,
+0 and −0, are considered equal.
int float_le(float x, float y) {
unsigned ux = f2u(x);
unsigned uy = f2u(y);

/* Get the sign bits */
unsigned sx = ux >> 31;
unsigned sy = uy >> 31;
/* Give an expression using only ux, uy, sx, and sy */
return
;
}
2.85 ◆
Given a ﬂoating-point format with a k-bit exponent and an n-bit fraction, write
formulas for the exponent E, the signiﬁcand M, the fraction f , and the value V
for the quantities that follow. In addition, describe the bit representation.
A. The number 7.0
B. The largest odd integer that can be represented exactly
C. The reciprocal of the smallest positive normalized value
2.86 ◆
Intel-compatible processors also support an “extended-precision” ﬂoating-point
format with an 80-bit word divided into a sign bit, k = 15 exponent bits, a single
integer bit, and n = 63 fraction bits. The integer bit is an explicit copy of the
implied bit in the IEEE ﬂoating-point representation. That is, it equals 1 for
normalized values and 0 for denormalized values. Fill in the following table giving
the approximate values of some “interesting” numbers in this format:
Extended precision
Description
Value
Decimal
Smallest positive denormalized
Smallest positive normalized
Largest normalized
This format can be used in C programs compiled for Intel-compatible ma-
chines by declaring the data to be of type long double. However, it forces the
compiler to generate code based on the legacy 8087 ﬂoating-point instructions.
The resulting program will most likely run much slower than would be the case
for data type float or double.
2.87 ◆
The 2008 version of the IEEE ﬂoating-point standard, named IEEE 754-2008,
includes a 16-bit “half-precision” ﬂoating-point format. It was originally devised
by computer graphics companies for storing data in which a higher dynamic range
is required than can be achieved with 16-bit integers. This format has 1 sign
bit, 5 exponent bits (k = 5), and 10 fraction bits (n = 10). The exponent bias is
25−1 −1 = 15.
Fill in the table that follows for each of the numbers given, with the following
instructions for each column:

Hex: The four hexadecimal digits describing the encoded form.
M: The value of the signiﬁcand. This should be a number of the form x or x
y ,
where x is an integer and y is an integral power of 2. Examples include 0,
67
64, and
1
256.
E: The integer value of the exponent.
V : The numeric value represented. Use the notation x or x × 2z, where x and
z are integers.
D: The (possibly approximate) numerical value, as is printed using the %f
formatting speciﬁcation of printf.
As an example, to represent the number 7
8, we would have s = 0, M = 7
4,
and E = −1. Our number would therefore have an exponent ﬁeld of 011102
(decimal value 15 −1 = 14) and a signiﬁcand ﬁeld of 11000000002, giving a hex
representation 3B00. The numerical value is 0.875.
You need not ﬁll in entries marked —.
Description
Hex
M
E
V
D
−0
−0
−0.0
Smallest value > 2
512
512
512.0
Largest denormalized
−∞
—
—
−∞
−∞
Number with hex
representation 3BB0
3BB0
2.88 ◆◆
Consider the following two 9-bit ﬂoating-point representations based on the IEEE
ﬂoating-point format.
1. Format A
There is 1 sign bit.
There are k = 5 exponent bits. The exponent bias is 15.
There are n = 3 fraction bits.
2. Format B
There is 1 sign bit.
There are k = 4 exponent bits. The exponent bias is 7.
There are n = 4 fraction bits.
In the following table, you are given some bit patterns in format A, and your
task is to convert them to the closest value in format B. If rounding is necessary
you should round toward +∞. In addition, give the values of numbers given by
the format A and format B bit patterns. Give these as whole numbers (e.g., 17) or
as fractions (e.g., 17/64 or 17/26).

Format A
Format B
Bits
Value
Bits
Value
1 01111 001
−9
8
1 0111 0010
−9
8
0 10110 011
1 00111 010
0 00000 111
1 11100 000
0 10111 100
2.89 ◆
We are running programs on a machine where values of type int have a 32-
bit two’s-complement representation. Values of type float use the 32-bit IEEE
format, and values of type double use the 64-bit IEEE format.
We generate arbitrary integer values x, y, and z, and convert them to values
of type double as follows:
/* Create some arbitrary values */
int x = random();
int y = random();
int z = random();
/* Convert to double */
double
dx = (double) x;
double
dy = (double) y;
double
dz = (double) z;
For each of the following C expressions, you are to indicate whether or
not the expression always yields 1. If it always yields 1, describe the underlying
mathematical principles. Otherwise, give an example of arguments that make
it yield 0. Note that you cannot use an IA32 machine running gcc to test your
answers, since it would use the 80-bit extended-precision representation for both
float and double.
A. (float) x == (float) dx
B. dx - dy == (double) (x-y)
C. (dx + dy) + dz == dx + (dy + dz)
D. (dx * dy) * dz == dx * (dy * dz)
E. dx / dx == dz / dz
2.90 ◆
You have been assigned the task of writing a C function to compute a ﬂoating-
point representation of 2x. You decide that the best way to do this is to directly
construct the IEEE single-precision representation of the result. When x is too
small, your routine will return 0.0. When x is too large, it will return +∞. Fill in the
blank portions of the code that follows to compute the correct result. Assume the

function u2f returns a ﬂoating-point value having an identical bit representation
as its unsigned argument.
float fpwr2(int x)
{
/* Result exponent and fraction */
unsigned exp, frac;
unsigned u;
if (x <
) {
/* Too small.
Return 0.0 */
exp =
;
frac =
;
} else if (x <
) {
/* Denormalized result */
exp =
;
frac =
;
} else if (x <
) {
/* Normalized result. */
exp =
;
frac =
;
} else {
/* Too big.
Return +oo */
exp =
;
frac =
;
}
/* Pack exp and frac into 32 bits */
u = exp << 23 | frac;
/* Return as float */
return u2f(u);
}
2.91 ◆
Around 250 B.C., the Greek mathematician Archimedes proved that 223
71 < π < 22
7 .
Hadhehadaccesstoa computerand thestandardlibrary <math.h>, hewouldhave
been able to determine that the single-precision ﬂoating-point approximation of
π has the hexadecimal representation 0x40490FDB. Of course, all of these are just
approximations, since π is not rational.
A. What is the fractional binary number denoted by this ﬂoating-point value?
B. What is the fractional binary representation of 22
7 ? Hint: See Problem 2.83.
C. At what bit position (relative to the binary point) do these two approxima-
tions to π diverge?

Bit-Level Floating-Point Coding Rules
In the following problems, you will write code to implement ﬂoating-point func-
tions, operating directly on bit-level representations of ﬂoating-point numbers.
Your code should exactly replicate the conventions for IEEE ﬂoating-point oper-
ations, including using round-to-even mode when rounding is required.
To this end, we deﬁne data type float_bits to be equivalent to unsigned:
/* Access bit-level representation floating-point number */
typedef unsigned float_bits;
Rather than using data type float in your code, you will use float_bits.
You may use both int and unsigned data types, including unsigned and integer
constants and operations. You may not use any unions, structs, or arrays. Most
signiﬁcantly, you may not use any ﬂoating-point data types, operations, or con-
stants. Instead, your code should perform the bit manipulations that implement
the speciﬁed ﬂoating-point operations.
The following function illustrates the use of these coding rules. For argument
f , it returns ±0 if f is denormalized (preserving the sign of f ), and returns f
otherwise.
/* If f is denorm, return 0.
Otherwise, return f */
float_bits float_denorm_zero(float_bits f) {
/* Decompose bit representation into parts */
unsigned sign = f>>31;
unsigned exp =
f>>23 & 0xFF;
unsigned frac = f
& 0x7FFFFF;
if (exp == 0) {
/* Denormalized.
Set fraction to 0 */
frac = 0;
}
/* Reassemble bits */
return (sign << 31) | (exp << 23) | frac;
}
2.92 ◆◆
Following the bit-level ﬂoating-point coding rules, implement the function with
the following prototype:
/* Compute -f.
If f is NaN, then return f. */
float_bits float_negate(float_bits f);
For ﬂoating-point number f , this function computes −f . If f is NaN, your
function should simply return f .
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine’s ﬂoating-point
operations.

2.93 ◆◆
Following the bit-level ﬂoating-point coding rules, implement the function with
the following prototype:
/* Compute |f|.
If f is NaN, then return f. */
float_bits float_absval(float_bits f);
For ﬂoating-point number f , this function computes |f |. If f is NaN, your
function should simply return f .
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine’s ﬂoating-point
operations.
2.94 ◆◆◆
Following the bit-level ﬂoating-point coding rules, implement the function with
the following prototype:
/* Compute 2*f.
If f is NaN, then return f. */
float_bits float_twice(float_bits f);
For ﬂoating-point number f , this function computes 2.0 . f . If f is NaN, your
function should simply return f .
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine’s ﬂoating-point
operations.
2.95 ◆◆◆
Following the bit-level ﬂoating-point coding rules, implement the function with
the following prototype:
/* Compute 0.5*f.
If f is NaN, then return f. */
float_bits float_half(float_bits f);
For ﬂoating-point number f , this function computes 0.5 . f . If f is NaN, your
function should simply return f .
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine’s ﬂoating-point
operations.
2.96 ◆◆◆◆
Following the bit-level ﬂoating-point coding rules, implement the function with
the following prototype:
/*
* Compute (int) f.
* If conversion causes overflow or f is NaN, return 0x80000000
*/
int float_f2i(float_bits f);

For ﬂoating-point number f , this function computes (int) f . Your function
should round toward zero. If f cannot be represented as an integer (e.g., it is out
of range, or it is NaN), then the function should return 0x80000000.
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine’s ﬂoating-point
operations.
2.97 ◆◆◆◆
Following the bit-level ﬂoating-point coding rules, implement the function with
the following prototype:
/* Compute (float) i */
float_bits float_i2f(int i);
For argument i, this function computes the bit-level representation of
(float) i.
Test your function by evaluating it for all 232 values of argument f and com-
paring the result to what would be obtained using your machine’s ﬂoating-point
operations.
Solutions to Practice Problems
Solution to Problem 2.1 (page 73)
Understanding the relation between hexadecimal and binary formats will be im-
portant once we start looking at machine-level programs. The method for doing
these conversions is in the text, but it takes a little practice to become familiar.
A. 0x25B9D2 to binary:
Hexadecimal
2
5
B
9
D
2
Binary
0010
0101
1101
1001
1101
0010
B. Binary 1100 1001 0111 1011 to hexadecimal:
Binary
1100
1001
0111
1011
Hexadecimal
C
9
7
B
C. 0xA8B3D to binary:
Hexadecimal
A
8
B
3
D
Binary
1010
1000
1011
0011
1101
D. Binary 11 0010 0010 1101 1001 0110 to hexadecimal:
Binary
11
0010
0010
1101
1001
0110
Hexadecimal
3
2
2
D
9
6
Solution to Problem 2.2 (page 73)
This problem gives you a chance to think about powers of 2 and their hexadecimal
representations.

n
2n (decimal)
2n (hexadecimal)
5
32
0x20
23
8,388,608
0x800000
15
32,768
0x8000
13
8,192
0x2000
12
4,096
0x1000
6
64
0x40
8
256
0x100
Solution to Problem 2.3 (page 74)
This problem gives you a chance to try out conversions between hexadecimal and
decimal representations for some smaller numbers. For larger ones, it becomes
much more convenient and reliable to use a calculator or conversion program.
Decimal
Binary
Hexadecimal
0
0000 0000
0x00
158 = 16 . 9 + 14
1001 1110
0x9E
76 = 16 . 4 + 12
0100 1100
0x4C
145 = 16 . 9 + 1
1001 0001
0x91
16 . 10 + 14 = 174
1010 1110
0xAE
16 . 3 + 12 = 60
0011 1100
0x3C
16 . 15 + 1 = 241
1111 0001
0xF1
16 . 7 + 5 = 117
0111 0101
0x75
16 . 11 + 13 = 189
1011 1101
0xBD
16 . 15 + 5 = 245
1111 0101
0xF5
Solution to Problem 2.4 (page 75)
When you begin debugging machine-level programs, you will ﬁnd many cases
where some simple hexadecimal arithmetic would be useful. You can always
convert numbers to decimal, perform the arithmetic, and convert them back, but
being able to work directly in hexadecimal is more efﬁcient and informative.
A. 0x605C + 0x5 = 0x6061. Adding 5 to hex C gives 1 with a carry of 1.
B. 0x605C −0x20 = 0x603C. Subtracting 2 from 5 in the second digit position
requires no borrow from the third. This gives 3.
C. 0x605C + 32 = 0x607C. Decimal 32 (25) equals hexadecimal 0x20.
D. 0x60FA −0x605C = 0x9E. To subtract hex C (decimal 12) from hex A (decimal
10), we borrow 16 from the second digit, giving hex F (decimal 15). In the
second digit, we now subtract 5 from hex E (decimal 14), giving decimal 9.
Solution to Problem 2.5 (page 84)
This problem tests your understanding of the byte representation of data and the
two different byte orderings.
A.
Little endian: 78
Big endian: 12
B.
Little endian: 78 56
Big endian: 12 34

C.
Little endian: 78 56 34
Big endian: 12 34 56
Recall that show_bytes enumerates a series of bytes starting from the one with
lowest address and working toward the one with highest address. On a little-
endian machine, it will list the bytes from least signiﬁcant to most. On a big-endian
machine, it will list bytes from the most signiﬁcant byte to the least.
Solution to Problem 2.6 (page 85)
This problem is another chance to practice hexadecimal to binary conversion. It
also gets you thinking about integer and ﬂoating-point representations. We will
explore these representations in more detail later in this chapter.
A. Using the notation of the example in the text, we write the two strings as
follows:
0
0
2
7
C
8
F
8
00000000001001111100100011111000
**********************
4
A
1
F
2
3
E
0
01001010000111110010001111100000
B. With the second word shifted two positions to the right relative to the ﬁrst,
we ﬁnd a sequence with 21 matching bits.
C. We ﬁnd all bits of the integer embedded in the ﬂoating-point number, except
for the most signiﬁcant bit having value 0. Such is the case for the example
in the text as well. In addition, the ﬂoating-point number has some nonzero
high-order bits that do not match those of the integer.
Solution to Problem 2.7 (page 85)
It prints 6D 6E 6F 70 71 72. Recall also that the library routine strlen does not
count the terminating null character, and so show_bytes printed only through the
character ‘r’.
Solution to Problem 2.8 (page 87)
This problem is a drill to help you become more familiar with Boolean operations.
Operation
Result
a
[01001110]
b
[11100001]
~a
[10110001]
~b
[00011110]
a & b
[01000000]
a | b
[11101111]
a ^ b
[10101111]

Solution to Problem 2.9 (page 89)
This problem illustrates how Boolean algebra can be used to describe and reason
about real-world systems. We can see that this color algebra is identical to the
Boolean algebra over bit vectors of length 3.
A. Colors are complemented by complementing the values of R, G, and B.
From this, we can see that white is the complement of black, yellow is the
complement of blue, magenta is the complement of green, and cyan is the
complement of red.
B. We perform Boolean operations based on a bit-vector representation of the
colors. From this we get the following:
Blue (001)
|
Green (010)
=
Cyan (011)
Yellow (110)
&
Cyan (011)
=
Green (010)
Red (100)
^
Magenta (101)
=
Blue (001)
Solution to Problem 2.10 (page 90)
This procedure relies on the fact that exclusive-or is commutative and associative,
and that a ^ a = 0 for any a.
Step
*x
*y
Initially
a
b
Step 1
a
a ^ b
Step 2
a ^ (a ^ b) = (a ^ a) ^ b = b
a ^ b
Step 3
b
b ^ (a ^ b) = (b ^ b) ^ a = a
See Problem 2.11 for a case where this function will fail.
Solution to Problem 2.11 (page 91)
This problem illustrates a subtle and interesting feature of our inplace swap
routine.
A. Both first and last have value k, so we are attempting to swap the middle
element with itself.
B. In this case, arguments x and y to inplace_swap both point to the same
location. When we compute *x ^ *y, we get 0. We then store 0 as the middle
element of the array, and the subsequent steps keep setting this element to
0. We can see that our reasoning in Problem 2.10 implicitly assumed that x
and y denote different locations.
C. Simply replace the test in line 4 of reverse_array to be first < last, since
there is no need to swap the middle element with itself.
Solution to Problem 2.12 (page 91)
Here are the expressions:

A. x & 0xFF
B. x ^ ~0xFF
C. x | 0xFF
These expressions are typical of the kind commonly found in performing low-level
bit operations. The expression ~0xFF creates a mask where the 8 least-signiﬁcant
bits equal 0 and the rest equal 1. Observe that such a mask will be generated
regardless of the word size. By contrast, the expression 0xFFFFFF00 would only
work when data type int is 32 bits.
Solution to Problem 2.13 (page 92)
These problems help you think about the relation between Boolean operations
and typical ways that programmers apply masking operations. Here is the code:
/* Declarations of functions implementing operations bis and bic */
int bis(int x, int m);
int bic(int x, int m);
/* Compute x|y using only calls to functions bis and bic */
int bool_or(int x, int y) {
int result = bis(x,y);
return result;
}
/* Compute x^y using only calls to functions bis and bic */
int bool_xor(int x, int y) {
int result = bis(bic(x,y), bic(y,x));
return result;
}
The bis operation is equivalent to Boolean or—a bit is set in z if either this
bit is set in x or it is set in m. On the other hand, bic(x, m) is equivalent to x & ~m;
we want the result to equal 1 only when the corresponding bit of x is 1 and of m is
0.
Given that, we can implement | with a single call to bis. To implement ^, we
take advantage of the property
x ^ y = (x & ~y) | (~x & y)
Solution to Problem 2.14 (page 93)
This problem highlights the relation between bit-level Boolean operations and
logical operations in C. A common programming error is to use a bit-level oper-
ation when a logical one is intended, or vice versa.

Expression
Value
Expression
Value
a & b
0x44
a && b
0x01
a | b
0x57
a || b
0x01
~a | ~b
0xBB
!a || !b
0x00
a & !b
0x00
a && ~b
0x01
Solution to Problem 2.15 (page 93)
The expression is !(x ^ y).
That is, x^y will be zero if and only if every bit of x matches the corresponding
bit of y. We then exploit the ability of ! to determine whether a word contains any
nonzero bit.
There is no real reason to use this expression rather than simply writing x ==
y, but it demonstrates some of the nuances of bit-level and logical operations.
Solution to Problem 2.16 (page 94)
This problem is a drill to help you understand the different shift operations.
Logical
Arithmetic
x
a << 2
a >> 3
a >> 3
Hex
Binary
Binary
Hex
Binary
Hex
Binary
Hex
0xD4
[11010100]
[01010000]
0x50
[00011010]
0x1A
[11111010]
0xFA
0x64
[01100100]
[10010000]
0x90
[00001100]
0x0C
[11101100]
0xEC
0x72
[01110010]
[11001000]
0xC8
[00001110]
0x0E
[00001110]
0x0E
0x44
[01000100]
[00010000]
0x10
[00001000]
0x08
[11101000]
0xE9
Solution to Problem 2.17 (page 101)
In general, working through examples for very small word sizes is a very good way
to understand computer arithmetic.
The unsigned values correspond to those in Figure 2.2. For the two’s-
complement values, hex digits 0 through 7 have a most signiﬁcant bit of 0, yielding
nonnegative values, while hex digits 8 through F have a most signiﬁcant bit of 1,
yielding a negative value.
Hexadecimal
Binary
⃗x
B2U4(⃗x)
B2T4(⃗x)
0xA
[1010]
23 + 21 = 10
−23 + 22 = −6
0x1
[0001]
20 = 1
20 = 1
0xB
[1011]
23 + 21 + 20 = 11
−23 + 21 + 20 = −5
0x2
[0010]
21 = 2
21 = 2
0x7
[0111]
22 + 21 + 20 = 7
22 + 21 + 20 = 7
0xC
[1100]
23 + 22 = 12
−23 + 22 = −4

Solution to Problem 2.18 (page 105)
For a 32-bit word, any value consisting of 8 hexadecimal digits beginning with one
of the digits 8 through f represents a negative number. It is quite common to see
numbers beginning with a string of f’s, since the leading bits of a negative number
are all ones. You must look carefully, though. For example, the number 0x8048337
has only 7 digits. Filling this out with a leading zero gives 0x08048337, a positive
number.
4004d0:
48 81 ec e0 02 00 00
sub
$0x2e0,%rsp
A. 736
4004d7:
48 8b 44 24 a8
mov
-0x58(%rsp),%rax
B. -88
4004dc:
48 03 47 28
add
0x28(%rdi),%rax
C.
40
4004e0:
48 89 44 24 d0
mov
%rax,-0x30(%rsp)
D. -48
4004e5:
48 8b 44 24 78
mov
0x78(%rsp),%rax
E. 120
4004ea:
48 89 87 88 00 00 00
mov
%rax,0x88(%rdi)
F. 136
4004f1:
48 8b 84 24 f8 01 00
mov
0x1f8(%rsp),%rax
G. 504
4004f8:
00
4004f9:
48 03 44 24 08
add
0x8(%rsp),%rax
4004fe:
48 89 84 24 c0 00 00
mov
%rax,0xc0(%rsp)
H. 192
400505:
00
400506:
48 8b 44 d4 b8
mov
-0x48(%rsp,%rdx,8),%rax
I. -72
Solution to Problem 2.19 (page 107)
The functions T2U and U2T are very peculiar from a mathematical perspective.
It is important to understand how they behave.
We solve this problem by reordering the rows in the solution of Problem 2.17
according to the two’s-complement value and then listing the unsigned value as
the result of the function application. We show the hexadecimal values to make
this process more concrete.
⃗x (hex)
x
T2U4(vecx)
0xF
−1
15
0xB
−5
11
0xA
−6
10
0xC
−4
12
0x1
1
1
0x8
8
8
Solution to Problem 2.20 (page 109)
This exercise tests your understanding of Equation 2.5.
For the ﬁrst four entries, the values of x are negative and T2U4(x) = x + 24.
For the remaining two entries, the values of x are nonnegative and T2U4(x) = x.
Solution to Problem 2.21 (page 112)
This problem reinforces your understanding of the relation between two’s-
complement and unsigned representations, as well as the effects of the C promo-
tion rules. Recall that TMin32 is −2,147,483,648, and that when cast to unsigned it

becomes 2,147,483,648. In addition, if either operand is unsigned, then the other
operand will be cast to unsigned before comparing.
Expression
Type
Evaluation
-2147483647-1 == 2147483648U
Unsigned
1
-2147483647-1 < 2147483647
Signed
1
-2147483647-1U < 2147483647
Unsigned
0
-2147483647-1 < -2147483647
Signed
1
-2147483647-1U < -2147483647
Unsigned
1
Solution to Problem 2.22 (page 115)
This exercise provides a concrete demonstration of how sign extension preserves
the numeric value of a two’s-complement representation.
A.
[1100]
−23 + 22
=
−8 + 4
=
−4
B.
[11100]
−24 + 23 + 22
=
−16 + 8 + 4
=
−4
C.
[111100]
−25 + 24 + 23 + 22
=
−32 + 16 + 8 + 4
=
−4
Solution to Problem 2.23 (page 116)
The expressions in these functions are common program “idioms” for extracting
values from a word in which multiple bit ﬁelds have been packed. They exploit
the zero-ﬁlling and sign-extending properties of the different shift operations.
Note carefully the ordering of the cast and shift operations. In fun1, the shifts
are performed on unsigned variable word and hence are logical. In fun2, shifts
are performed after casting word to int and hence are arithmetic.
A.
w
fun1(w)
fun2(w)
0x00000076
0x00000076
0x00000076
0x87654321
0x00000021
0x00000021
0x000000C9
0x000000C9
0xFFFFFFC9
0xEDCBA987
0x00000087
0xFFFFFF87
B. Function fun1 extracts a value from the low-order 8 bits of the argument,
giving an integer ranging between 0 and 255. Function fun2 extracts a value
from the low-order 8 bits of the argument, but it also performs sign extension.
The result will be a number between −128 and 127.
Solution to Problem 2.24 (page 118)
The effect of truncation is fairly intuitive for unsigned numbers, but not for two’s-
complement numbers. This exercise lets you explore its properties using very small
word sizes.

Hex
Unsigned
Two’s complement
Original
Truncated
Original
Truncated
Original
Truncated
1
1
1
1
1
1
3
3
3
3
3
3
5
5
5
5
5
5
C
4
12
4
−4
4
E
6
14
6
−2
6
As Equation 2.9 states, the effect of this truncation on unsigned values is to
simply ﬁnd their residue, modulo 8. The effect of the truncation on signed values
is a bit more complex. According to Equation 2.10, we ﬁrst compute the modulo 8
residue of the argument. This will give values 0 through 7 for arguments 0 through
7, and also for arguments −8 through −1. Then we apply function U2T3 to these
residues, giving two repetitions of the sequences 0 through 3 and −4 through −1.
Solution to Problem 2.25 (page 119)
This problem is designed to demonstrate how easily bugs can arise due to the
implicit casting from signed to unsigned. It seems quite natural to pass parameter
length as an unsigned, since one would never want to use a negative length. The
stopping criterion i <= length-1 also seems quite natural. But combining these
two yields an unexpected outcome!
Since parameter length is unsigned, the computation 0 −1is performed using
unsigned arithmetic, which is equivalent to modular addition. The result is then
UMax. The ≤comparison is also performed using an unsigned comparison, and
since any number is less than or equal to UMax, the comparison always holds!
Thus, the code attempts to access invalid elements of array a.
The code can be ﬁxed either by declaring length to be an int or by changing
the test of the for loop to be i < length.
Solution to Problem 2.26 (page 119)
This example demonstrates a subtle feature of unsigned arithmetic, and also the
property that we sometimes perform unsigned arithmetic without realizing it. This
can lead to very tricky bugs.
A. For what cases will this function produce an incorrect result? The function
will incorrectly return 1 when s is shorter than t.
B. Explain how this incorrect result comes about. Since strlen is deﬁned to
yield an unsigned result, the difference and the comparison are both com-
puted using unsigned arithmetic. When s is shorter than t, the difference
strlen(s) - strlen(t) should be negative, but instead becomes a large,
unsigned number, which is greater than 0.
C. Show how to ﬁx the code so that it will work reliably. Replace the test with
the following:
return strlen(s) > strlen(t);

Solution to Problem 2.27 (page 125)
This function is a direct implementation of the rules given to determine whether
or not an unsigned addition overﬂows.
/* Determine whether arguments can be added without overflow */
int uadd_ok(unsigned x, unsigned y) {
unsigned sum = x+y;
return sum >= x;
}
Solution to Problem 2.28 (page 125)
This problem is a simple demonstration of arithmetic modulo 16. The easiest way
to solve it is to convert the hex pattern into its unsigned decimal value. For nonzero
values of x, we must have (-u
4 x) + x = 16. Then we convert the complemented
value back to hex.
x
-u
4 x
Hex
Decimal
Decimal
Hex
1
1
15
F
4
4
12
C
7
7
9
9
A
10
6
6
E
14
2
2
Solution to Problem 2.29 (page 129)
This problem is an exercise to make sure you understand two’s-complement
addition.
x
y
x + y
x +t
5 y
Case
−12
−15
−27
5
1
[10100]
[10001]
[100101]
[00101]
−8
−8
−16
−16
2
[11000]
[11000]
[110000]
[10000]
−9
8
−1
−1
2
[10111]
[01000]
[111111]
[11111]
2
5
7
7
3
[00010]
[00101]
[000111]
[00111]
12
4
16
−16
4
[01100]
[00100]
[010000]
[10000]

Solution to Problem 2.30 (page 130)
This function is a direct implementation of the rules given to determine whether
or not a two’s-complement addition overﬂows.
/* Determine whether arguments can be added without overflow */
int tadd_ok(int x, int y) {
int sum = x+y;
int neg_over = x <
0 && y <
0 && sum >= 0;
int pos_over = x >= 0 && y >= 0 && sum <
0;
return !neg_over && !pos_over;
}
Solution to Problem 2.31 (page 130)
Your coworker could have learned, by studying Section 2.3.2, that two’s-
complement addition forms an abelian group, and so the expression (x+y)-x
will evaluate to y regardless of whether or not the addition overﬂows, and that
(x+y)-y will always evaluate to x.
Solution to Problem 2.32 (page 130)
This function will give correct values, except when y is TMin. In this case, we
will have -y also equal to TMin, and so the call to function tadd_ok will indicate
overﬂow when x is negative and no overﬂow when x is nonnegative. In fact, the
opposite is true: tsub_ok(x, TMin) should yield 0 when x is negative and 1 when
it is nonnegative.
One lesson to be learned from this exercise is that TMin should be included
as one of the cases in any test procedure for a function.
Solution to Problem 2.33 (page 131)
This problem helps you understand two’s-complement negation using a very small
word size.
For w = 4, we have TMin4 = −8. So −8 is its own additive inverse, while other
values are negated by integer negation.
x
-t
4 x
Hex
Decimal
Decimal
Hex
2
2
−2
E
3
3
−3
D
9
−9
−9
7
B
−5
5
5
C
−4
4
4
The bit patterns are the same as for unsigned negation.
Solution to Problem 2.34 (page 134)
This problem is an exercise to make sure you understand two’s-complement
multiplication.

Mode
x
y
x . y
Truncated x . y
Unsigned
4
[100]
5
[101]
20
[010100]
4
[100]
Two’s complement
−4
[100]
−3
[101]
12
[001100]
−4
[100]
Unsigned
2
[010]
7
[111]
14
[001110]
6
[110]
Two’s complement
2
[010]
−1
[111]
−2
[111110]
−2
[110]
Unsigned
6
[110]
6
[110]
36
[100100]
4
[100]
Two’s complement
−2
[110]
−2
[110]
4
[000100]
−4
[100]
Solution to Problem 2.35 (page 135)
It is not realistic to test this function for all possible values of x and y. Even if
you could run 10 billion tests per second, it would require over 58 years to test all
combinations when data type int is 32 bits. On the other hand, it is feasible to test
your code by writing the function with data type short or char and then testing
it exhaustively.
Here’s a more principled approach, following the proposed set of arguments:
1. We know that x . y can be written as a 2w-bit two’s-complement number. Let
u denote the unsigned number represented by the lower w bits, and v denote
the two’s-complement number represented by the upper w bits. Then, based
on Equation 2.3, we can see that x . y = v2w + u.
We also know that u = T2Uw(p), since they are unsigned and two’s-
complement numbers arising from the same bit pattern, and so by Equation
2.6, we can write u = p + pw−12w, where pw−1 is the most signiﬁcant bit of p.
Letting t = v + pw−1, we have x . y = p + t2w.
When t = 0, we have x . y = p; the multiplication does not overﬂow. When
t ̸= 0, we have x . y ̸= p; the multiplication does overﬂow.
2. By deﬁnition of integer division, dividing p by nonzero x gives a quotient
q and a remainder r such that p = x . q + r, and |r| < |x|. (We use absolute
values here, because the signs of x and r may differ. For example, dividing −7
by 2 gives quotient −3 and remainder −1.)
3. Suppose q = y. Then we have x . y = x . y + r + t2w. From this, we can see
that r + t2w = 0. But |r| < |x| ≤2w, and so this identity can hold only if t = 0,
in which case r = 0.
Suppose r = t = 0. Then we will have x . y = x . q, implying that y = q.
When x equals 0, multiplication does not overﬂow, and so we see that our code
provides a reliable way to test whether or not two’s-complement multiplication
causes overﬂow.
Solution to Problem 2.36 (page 135)
With 64 bits, we can perform the multiplication without overﬂowing. We then test
whether casting the product to 32 bits changes the value:

1
/* Determine whether the arguments can be multiplied
2
without overflow */
3
int tmult_ok(int x, int y) {
4
/* Compute product without overflow */
5
int64_t pll = (int64_t) x*y;
6
/* See if casting to int preserves value */
7
return pll == (int) pll;
8
}
Note that the casting on the right-hand side of line 5 is critical. If we instead
wrote the line as
int64_t pll = x*y;
the product would be computed as a 32-bit value (possibly overﬂowing) and then
sign extended to 64 bits.
Solution to Problem 2.37 (page 135)
A. This change does not help at all. Even though the computation of asize will
be accurate, the call to malloc will cause this value to be converted to a 32-bit
unsigned number, and so the same overﬂow conditions will occur.
B. With malloc having a 32-bit unsigned number as its argument, it cannot
possibly allocate a block of more than 232 bytes, and so there is no point
attempting to allocate or copy this much memory. Instead, the function
should abort and return NULL, as illustrated by the following replacement
to the original call to malloc (line 9):
uint64_t required_size = ele_cnt * (uint64_t) ele_size;
size_t request_size = (size_t) required_size;
if (required_size != request_size)
/* Overflow must have occurred. Abort operation */
return NULL;
void *result = malloc(request_size);
if (result == NULL)
/* malloc failed */
return NULL;
Solution to Problem 2.38 (page 138)
In Chapter 3, we will see many examples of the lea instruction in action. The
instruction is provided to support pointer arithmetic, but the C compiler often
uses it as a way to perform multiplication by small constants.
For each value of k, we can compute two multiples: 2k (when b is 0) and 2k + 1
(when b is a). Thus, we can compute multiples 1, 2, 3, 4, 5, 8, and 9.

Solution to Problem 2.39 (page 139)
The expression simply becomes -(x<<m). To see this, let the word size be w so that
n = w −1. Form B states that we should compute (x<<w) - (x<<m), but shifting
x to the left by w will yield the value 0.
Solution to Problem 2.40 (page 139)
This problem requires you to try out the optimizations already described and also
to supply a bit of your own ingenuity.
K
Shifts
Add/Subs
Expression
7
1
1
(x<<3) - x
30
4
3
(x<<4) + (x<<3) + (x<<2) + (x<<1)
28
2
1
(x<<5) - (x<<2)
55
2
2
(x<<6) - (x<<3) - x
Observe that the fourth case uses a modiﬁed version of form B. We can view
the bit pattern [11011] as having a run of 6 ones with a zero in the middle, and so
we apply the rule for form B, but then we subtract the term corresponding to the
middle zero bit.
Solution to Problem 2.41 (page 139)
Assuming that addition and subtraction have the same performance, the rule is
to choose form A when n = m, either form when n = m + 1, and form B when
n > m + 1.
The justiﬁcation for this rule is as follows. Assume ﬁrst that m > 0. When
n = m, form A requires only a single shift, while form B requires two shifts
and a subtraction. When n = m + 1, both forms require two shifts and either an
addition or a subtraction. When n > m + 1, form B requires only two shifts and one
subtraction, while form A requires n −m + 1 > 2 shifts and n −m > 1 additions.
For the case of m = 0, we get one fewer shift for both forms A and B, and so the
same rules apply for choosing between the two.
Solution to Problem 2.42 (page 143)
The only challenge here is to compute the bias without any testing or conditional
operations. We use the trick that the expression x >> 31 generates a word with all
ones if x is negative, and all zeros otherwise. By masking off the appropriate bits,
we get the desired bias value.
int div16(int x) {
/* Compute bias to be either 0 (x >= 0) or 15 (x < 0) */
int bias = (x >> 31) & 0xF;
return (x + bias) >> 4;
}

Solution to Problem 2.43 (page 143)
We have found that people have difﬁculty with this exercise when working di-
rectly with assembly code. It becomes more clear when put in the form shown in
optarith.
We can see that M is 31; x*M is computed as (x<<5)-x.
We can see that N is 8; a bias value of 7 is added when y is negative, and the
right shift is by 3.
Solution to Problem 2.44 (page 144)
These “C puzzle” problems provide a clear demonstration that programmers must
understand the properties of computer arithmetic:
A. (x > 0) || (x-1 < 0)
False. Let x be −2,147,483,648 (TMin32). We will then have x-1 equal to
2,147,483,647 (TMax32).
B. (x & 7) != 7 || (x<<29 < 0)
True. If (x & 7) != 7 evaluates to 0, then we must have bit x2 equal to 1.
When shifted left by 29, this will become the sign bit.
C. (x * x) >= 0
False. When x is 65,535 (0xFFFF), x*x is −131,071 (0xFFFE0001).
D. x < 0 || -x <= 0
True. If x is nonnegative, then -x is nonpositive.
E. x > 0 || -x >= 0
False. Let x be −2,147,483,648 (TMin32). Then both x and -x are negative.
F.
x+y == uy+ux
True. Two’s-complement and unsigned addition have the same bit-level be-
havior, and they are commutative.
G. x*~y + uy*ux == -x
True. ~y equals -y-1. uy*ux equals x*y. Thus, the left-hand side is equivalent
to x*-y-x+x*y.
Solution to Problem 2.45 (page 147)
Understanding fractional binary representations is an important step to under-
standing ﬂoating-point encodings. This exercise lets you try out some simple ex-
amples.
1
8
0.001
0.125
3
4
0.11
0.75
25
16
1.1001
1.5625
43
16
10.1011
2.6875
9
8
1.001
1.125
47
8
101.111
5.875
51
16
11.0011
3.1875

One simple way to think about fractional binary representations is to repre-
sent a number as a fraction of the form x
2k . We can write this in binary using the
binary representation of x, with the binary point inserted k positions from the
right. As an example, for 25
16, we have 2510 = 110012. We then put the binary point
four positions from the right to get 1.10012.
Solution to Problem 2.46 (page 147)
In most cases, the limited precision of ﬂoating-point numbers is not a major
problem, because the relative error of the computation is still fairly low. In this
example, however, the system was sensitive to the absolute error.
A. We can see that 0.1 −x has the binary representation
0.000000000000000000000001100[1100] . . . 2
B. Comparing this to the binary representation of 1
10, we can see that it is simply
2−20 × 1
10, which is around 9.54 × 10−8.
C. 9.54 × 10−8 × 100 × 60 × 60 × 10 ≈0.343 seconds.
D. 0.343 × 2,000 ≈687 meters.
Solution to Problem 2.47 (page 153)
Working through ﬂoating-point representations for very small word sizes helps
clarify how IEEE ﬂoating point works. Note especially the transition between
denormalized and normalized values.
Bits
e
E
2E
f
M
2E × M
V
Decimal
0 00 00
0
0
1
0
4
0
4
0
4
0
0.0
0 00 01
0
0
1
1
4
1
4
1
4
1
4
0.25
0 00 10
0
0
1
2
4
2
4
2
4
1
2
0.5
0 00 11
0
0
1
3
4
3
4
3
4
3
4
0.75
0 01 00
1
0
1
0
4
4
4
4
4
1
1.0
0 01 01
1
0
1
1
4
5
4
5
4
5
4
1.25
0 01 10
1
0
1
2
4
6
4
6
4
3
2
1.5
0 01 11
1
0
1
3
4
7
4
7
4
7
4
1.75
0 10 00
2
1
2
0
4
4
4
8
4
2
2.0
0 10 01
2
1
2
1
4
5
4
10
4
5
2
2.5
0 10 10
2
1
2
2
4
6
4
12
4
3
3.0
0 10 11
2
1
2
3
4
7
4
14
4
7
2
3.5
0 11 00
—
—
—
—
—
—
∞
—
0 11 01
—
—
—
—
—
—
NaN
—
0 11 10
—
—
—
—
—
—
NaN
—
0 11 11
—
—
—
—
—
—
NaN
—

Solution to Problem 2.48 (page 155)
Hexadecimal 0x359141 is equivalent to binary [1101011001000101000001]. Shift-
ing this right 21 places gives 1.1010110010001010000012 × 221. We form the frac-
tion ﬁeld by dropping the leading 1 and adding two zeros, giving
[10101100100010100000100]
The exponent is formed by adding bias 127 to 21, giving 148 (binary [10010100]).
We combine this with a sign ﬁeld of 0 to give a binary representation
[01001010010101100100010100000100]
We see that the matching bits in the two representations correspond to the low-
order bits of the integer, up to the most signiﬁcant bit equal to 1 matching the
high-order 21 bits of the fraction:
0
0
3
5
9
1
4
1
00000000001101011001000101000001
*********************
4
A
5
6
4
5
0
4
01001010010101100100010100000100
Solution to Problem 2.49 (page 156)
This exercise helps you think about what numbers cannot be represented exactly
in ﬂoating point.
A. The number has binary representation 1, followed by n zeros, followed by 1,
giving value 2n+1 + 1.
B. When n = 23, the value is 224 + 1 = 16,777,217.
Solution to Problem 2.50 (page 157)
Performing rounding by hand helps reinforce the idea of round-to-even with
binary numbers.
Original
Rounded
10.1112
2 7
8
11.0
3
11.0102
3 1
4
11.0
3
11.0002
3
11.0
3
10.1102
2 3
4
11.0
3
Solution to Problem 2.51 (page 158)
A. Looking at the nonterminating sequence for 1
10, we see that the 2 bits to the
right of the rounding position are 1, so a better approximation to 1
10 would be
obtained by incrementing x to get x′ = 0.000110011001100110011012, which
is larger than 0.1.
B. We can see that x′ −0.1 has binary representation
0.0000000000000000000000000[1100]

Comparing this to the binary representation of
1
10, we can see that it is
2−22 × 1
10, which is around 2.38 × 10−8.
C. 2.38 × 10−8 × 100 × 60 × 60 × 10 ≈0.086 seconds, a factor of 4 less than the
error in the Patriot system.
D. 0.086 × 2,000 ≈171 meters.
Solution to Problem 2.52 (page 158)
This problem tests a lot of concepts about ﬂoating-point representations, including
the encoding of normalized and denormalized values, as well as rounding.
Format A
Format B
Bits
Value
Bits
Value
Comments
011 0000
1
0111 000
1
101 1110
15
2
1001 111
15
2
010 1001
25
32
0110 100
3
4
Round down
110 1111
31
2
1011 000
16
Round up
000 0001
1
64
0001 000
1
64
Denorm →norm
Solution to Problem 2.53 (page 161)
In general, it is better to use a library macro rather than inventing your own code.
This code seems to work on a variety of machines, however.
We assume that the value 1e400 overﬂows to inﬁnity.
#define POS_INFINITY 1e400
#define NEG_INFINITY (-POS_INFINITY)
#define NEG_ZERO (-1.0/POS_INFINITY)
Solution to Problem 2.54 (page 161)
Exercises such as this one help you develop your ability to reason about ﬂoating-
point operations from a programmer’s perspective. Make sure you understand
each of the answers.
A. x == (int)(double) x
Yes, since double has greater precision and range than int.
B. x == (int)(float) x
No. For example, when x is TMax.
C. d == (double)(float) d
No. For example, when d is 1e40, we will get +∞on the right.
D. f == (float)(double) f
Yes, since double has greater precision and range than float.
E. f == -(-f)
Yes, since a ﬂoating-point number is negated by simply inverting its sign bit.

F.
1.0/2 == 1/2.0
Yes, the numerators and denominators will both be converted to ﬂoating-
point representations before the division is performed.
G. d*d >= 0.0
Yes, although it may overﬂow to +∞.
H. (f+d)-f == d
No. For example, when f is 1.0e20 and d is 1.0, the expression f+d will be
rounded to 1.0e20, and so the expression on the left-hand side will evaluate
to 0.0, while the right-hand side will be 1.0.


---

## 本章小结

<!-- TODO: 添加本章要点总结 -->

- 要点 1
- 要点 2
- 要点 3